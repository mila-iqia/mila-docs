

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>User’s guide &mdash; Mila Technical Documentation latest documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="_static/custom.css?v=8da7d091" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=c6e86fd7"></script>
      <script src="_static/doctools.js?v=888ff710"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="_static/copybutton.js?v=f281be69"></script>
      <script src="_static/documentation_options.js?v=c6e86fd7"></script>
      <script src="_static/documentation_options_fix.js?v=1c8886ec"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="AI tooling and methodology handbook" href="Handbook.html" />
    <link rel="prev" title="Purpose of this documentation" href="Purpose.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html">
            
              <img src="_static/image.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Purpose.html">Purpose of this documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Purpose.html#contributing">Contributing</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How-tos and Guides</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">User’s guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quick-start">Quick Start</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mila-code">mila code</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#logging-in-to-the-cluster">Logging in to the cluster</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ssh-secure-shell">SSH (Secure Shell)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#logging-in-with-ssh">Logging in with SSH</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ssh-private-keys">SSH Private Keys</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ssh-public-keys">SSH Public Keys</a></li>
<li class="toctree-l4"><a class="reference internal" href="#checking-if-you-already-have-ssh-private-keys">Checking If You Already Have SSH (Private) Keys</a></li>
<li class="toctree-l4"><a class="reference internal" href="#generating-an-ssh-private-key">Generating an SSH Private Key</a></li>
<li class="toctree-l4"><a class="reference internal" href="#generating-an-ssh-public-key-from-a-private-key">Generating an SSH Public Key from a Private Key</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#configuring-ssh">Configuring SSH</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#manual-ssh-configuration">Manual SSH configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mila-init">mila init</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#connecting-to-compute-nodes">Connecting to compute nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#auto-allocation-with-mila-cpu">Auto-allocation with mila-cpu</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-a-non-bash-unix-shell">Using a non-Bash Unix shell</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#running-your-code">Running your code</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#slurm-commands-guide">SLURM commands guide</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#basic-usage">Basic Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#submitting-jobs">Submitting jobs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#job-submission-arguments">Job submission arguments</a></li>
<li class="toctree-l4"><a class="reference internal" href="#checking-job-status">Checking job status</a></li>
<li class="toctree-l4"><a class="reference internal" href="#removing-a-job">Removing a job</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#partitioning">Partitioning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#information-on-partitions-nodes">Information on partitions/nodes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#useful-commands">Useful Commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="#special-gpu-requirements">Special GPU requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="#example-script">Example script</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#portability-concerns-and-solutions">Portability concerns and solutions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#managing-your-environments">Managing your environments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#virtual-environments">Virtual environments</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pip-virtualenv">Pip/Virtualenv</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-uv">UV</a></li>
<li class="toctree-l3"><a class="reference internal" href="#conda">Conda</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mamba">Mamba</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#using-modules">Using Modules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-module-command">The module command</a></li>
<li class="toctree-l4"><a class="reference internal" href="#available-software">Available Software</a></li>
<li class="toctree-l4"><a class="reference internal" href="#default-package-location">Default package location</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#on-using-containers">On using containers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#using-containers">Using containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#using-in-slurm">Using in SLURM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpu">GPU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id8">Singularity</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#what-is-singularity">What is Singularity?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#links-to-official-documentation">Links to official documentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#overview-of-the-steps-used-in-practice">Overview of the steps used in practice</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nope-not-on-macos">Nope, not on MacOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="#where-to-build-images">Where to build images</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#building-the-containers">Building the containers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#first-way-build-and-use-a-sandbox">First way: Build and use a sandbox</a></li>
<li class="toctree-l4"><a class="reference internal" href="#second-way-use-recipes">Second way: Use recipes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#using-containers-on-clusters">Using containers on clusters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-to-use-containers-on-clusters">How to use containers on clusters</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#sharing-data-with-acls">Sharing Data with ACLs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#viewing-and-verifying-acls">Viewing and Verifying ACLs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#contributing-datasets">Contributing datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#publicly-share-a-mila-dataset">Publicly share a Mila dataset</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id12">Academic Torrent</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id14">Google Drive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#digital-object-identifier-doi">Digital Object Identifier (DOI)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-transmission-using-globus-connect-personal">Data Transmission using Globus Connect Personal</a></li>
<li class="toctree-l2"><a class="reference internal" href="#jupyterhub">JupyterHub</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#access-mila-storage-in-jupyterlab">Access Mila Storage in JupyterLab</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#advanced-slurm-usage-and-multiple-gpu-jobs">Advanced SLURM usage and Multiple GPU jobs</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#handling-preemption">Handling preemption</a></li>
<li class="toctree-l3"><a class="reference internal" href="#packing-jobs">Packing jobs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#sharing-a-gpu-between-processes">Sharing a GPU between processes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sharing-a-node-with-multiple-gpu-1process-gpu">Sharing a node with multiple GPU 1process/GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sharing-a-node-with-multiple-gpu-multiple-processes-gpu">Sharing a node with multiple GPU &amp; multiple processes/GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#multiple-nodes">Multiple Nodes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-parallel">Data Parallel</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#weight-and-biases-wandb">Weight and Biases (WandB)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#logging-in-for-the-first-time">Logging in for the first time</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#for-those-who-already-have-a-wandb-account">For those who already have a WandB account</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#comet">Comet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id17">Logging in for the first time</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#frequently-asked-questions-faqs">Frequently asked questions (FAQs)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#connection-ssh-issues">Connection/SSH issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node">I’m getting <code class="docutils literal notranslate"><span class="pre">connection</span> <span class="pre">refused</span></code> while trying to connect to a login node</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#shell-issues">Shell issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-do-i-change-my-shell">How do I change my shell ?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#slurm-issues">SLURM issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-get-an-interactive-shell-on-the-cluster">How can I get an interactive shell on the cluster ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-reset-my-cluster-password">How can I reset my cluster password ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#srun-error-mem-and-mem-per-cpu-are-mutually-exclusive">srun: error: –mem and –mem-per-cpu are mutually exclusive</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-see-where-and-if-my-jobs-are-running">How can I see where and if my jobs are running ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified">Unable to allocate resources: Invalid account or account/partition combination specified</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-do-i-cancel-a-job">How do I cancel a job?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-can-i-access-a-node-on-which-one-of-my-jobs-is-running">How can I access a node on which one of my jobs is running ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node">I’m getting <code class="docutils literal notranslate"><span class="pre">Permission</span> <span class="pre">denied</span> <span class="pre">(publickey)</span></code> while trying to connect to a node</a></li>
<li class="toctree-l4"><a class="reference internal" href="#where-do-i-put-my-data-during-a-job">Where do I put my data during a job ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup">slurmstepd: error: Detected 1 oom-kill event(s) in step #####.batch cgroup</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fork-retry-resource-temporarily-unavailable">fork: retry: Resource temporarily unavailable</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#pytorch-issues">PyTorch issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263">I randomly get <code class="docutils literal notranslate"><span class="pre">INTERNAL</span> <span class="pre">ASSERT</span> <span class="pre">FAILED</span> <span class="pre">at</span> <span class="pre">&quot;../aten/src/ATen/MapAllocator.cpp&quot;:263</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#on-mig-gpus-i-get-torch-cuda-device-count-0-despite-torch-cuda-is-available">On MIG GPUs, I get <code class="docutils literal notranslate"><span class="pre">torch.cuda.device_count()</span> <span class="pre">==</span> <span class="pre">0</span></code> despite <code class="docutils literal notranslate"><span class="pre">torch.cuda.is_available()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#i-am-told-my-pytorch-job-abuses-the-filesystem-with-extreme-amounts-of-iops">I am told my PyTorch job abuses the filesystem with extreme amounts of IOPS</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conda-refuses-to-create-an-environment-with-your-installed-cuda-driver-is-not-available">Conda refuses to create an environment with <code class="docutils literal notranslate"><span class="pre">Your</span> <span class="pre">installed</span> <span class="pre">CUDA</span> <span class="pre">driver</span> <span class="pre">is:</span> <span class="pre">not</span> <span class="pre">available</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Handbook.html">AI tooling and methodology handbook</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Systems and services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Information.html">Computing infrastructure and policies</a></li>
<li class="toctree-l1"><a class="reference internal" href="Extra_compute.html">Computational resources outside of Mila</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">General theory</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html">What is a computer cluster?</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#parts-of-a-computing-cluster">Parts of a computing cluster</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#unix">UNIX</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#the-workload-manager">The workload manager</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#processing-data">Processing data</a></li>
<li class="toctree-l1"><a class="reference internal" href="Theory_cluster.html#software-on-the-cluster">Software on the cluster</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Minimal Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/frameworks/index.html">Software Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/distributed/index.html">Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/good_practices/index.html">Good practices</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/mila-iqia/ResearchTemplate">🔗 Research Project Template</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Extras</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Acknowledgement.html">Acknowledging Mila</a></li>
<li class="toctree-l1"><a class="reference external" href="https://datasets.server.mila.quebec/">Mila Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="Audio_video.html">Audio and video resources at Mila</a></li>
<li class="toctree-l1"><a class="reference internal" href="VSCode.html">Visual Studio Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="IDT.html">Who, what, where is IDT</a></li>
<li class="toctree-l1"><a class="reference internal" href="Cheatsheet.html">Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="Environmental_impact.html">Environmental Impact</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Mila Technical Documentation</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">User’s guide</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/mila-iqia/mila-docs/blob/master/docs/Userguide.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="user-s-guide">
<span id="userguide"></span><h1>User’s guide<a class="headerlink" href="#user-s-guide" title="Link to this heading"></a></h1>
<p>…or <em>IDT’s list of opinionated howtos</em></p>
<p>This section seeks to provide users of the Mila infrastructure with practical
knowledge, tips and tricks and example commands.</p>
<section id="quick-start">
<span id="id1"></span><h2>Quick Start<a class="headerlink" href="#quick-start" title="Link to this heading"></a></h2>
<p>Users first need <a class="reference internal" href="#logging-in"><span class="std std-ref">login access to the cluster</span></a>. It is
recommended to install <a class="reference external" href="https://github.com/mila-iqia/milatools">milatools</a> which will help in the <a class="reference internal" href="#mila-init"><span class="std std-ref">set up of the
ssh configuration</span></a> needed to securely and easily connect to the
cluster.</p>
<section id="mila-code">
<span id="id2"></span><h3>mila code<a class="headerlink" href="#mila-code" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://github.com/mila-iqia/milatools">milatools</a> also makes it easy to run and debug code on the Mila cluster.</p>
<p>First you need to setup your ssh configuration using <code class="docutils literal notranslate"><span class="pre">mila</span> <span class="pre">init</span></code>. The
initialisation of the ssh configuration is explained
<a class="reference internal" href="#mila-init"><span class="std std-ref">here</span></a> and in the <a class="reference external" href="https://github.com/mila-iqia/milatools#mila-init">mila init section of github page</a>.</p>
<p>Once that is done, you may run <a class="reference external" href="https://code.visualstudio.com/">VSCode</a>
on the cluster simply by <a class="reference external" href="https://code.visualstudio.com/docs/remote/ssh#_connect-to-a-remote-host">using the Remote-SSH extension</a>
and selecting <code class="docutils literal notranslate"><span class="pre">mila-cpu</span></code> as the host (in step 2).</p>
<p><code class="docutils literal notranslate"><span class="pre">mila-cpu</span></code> allocates a single CPU and 8 GB of RAM. If you need more
resources from within VSCode (e.g. to run a ML model in a notebook), then
you can use <code class="docutils literal notranslate"><span class="pre">mila</span> <span class="pre">code</span></code>. For example, if you want a GPU, 32G of RAM and 4 cores,
run this command in the terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mila<span class="w"> </span>code<span class="w"> </span>path/on/cluster<span class="w"> </span>--alloc<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>--mem<span class="o">=</span>32G<span class="w"> </span>-c<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
<p>The details of the command can be found in the <a class="reference external" href="https://github.com/mila-iqia/milatools#mila-code">mila code section of github page</a>. Remember that you need to
first setup your ssh configuration using <code class="docutils literal notranslate"><span class="pre">mila</span> <span class="pre">init</span></code> before the <code class="docutils literal notranslate"><span class="pre">mila</span> <span class="pre">code</span></code>
command can be used.</p>
</section>
</section>
<section id="logging-in-to-the-cluster">
<span id="logging-in"></span><h2>Logging in to the cluster<a class="headerlink" href="#logging-in-to-the-cluster" title="Link to this heading"></a></h2>
<p>To access the Mila Cluster clusters, you will need a Mila account. Please contact
Mila systems administrators if you don’t have it already. Our IT support service
is available here: <a class="reference external" href="https://it-support.mila.quebec/">https://it-support.mila.quebec/</a></p>
<p>You will also need to complete and return an IT Onboarding Training to get
access to the cluster.  Please refer to the Mila Intranet for more
informations:
<a class="reference external" href="https://sites.google.com/mila.quebec/mila-intranet/it-infrastructure/it-onboarding-training">https://sites.google.com/mila.quebec/mila-intranet/it-infrastructure/it-onboarding-training</a></p>
<p><strong>IMPORTANT</strong> : Your access to the Cluster is granted based on your status at
Mila (for students, your status is the same as your main supervisor’ status),
and on the duration of your stay, set during the creation of your account. The
following have access to the cluster : <strong>Current Students of Core Professors -
Core Professors - Staff</strong></p>
<section id="ssh-secure-shell">
<span id="ssh"></span><h3>SSH (Secure Shell)<a class="headerlink" href="#ssh-secure-shell" title="Link to this heading"></a></h3>
<p><strong>All access to the Mila cluster is via SSH using public-key authentication.</strong>
As of <strong>March 31, 2025</strong>, this will become the <strong>only</strong> means of authentication,
and <strong>password-based authentication will no longer work</strong>.</p>
<p>SSH key authentication is a technique using pairs of closely-linked keys: A
private key, and a corresponding public key. The public key should be
distributed to everyone, while the private key is known to only one person. The
public key can be used by anyone to challenge a person to prove their identity.
If they have the corresponding private key, that person can perform an
electronic signature that everyone can validate but that no one else could have
done themselves. The challenge is thus answered by demonstrating possession of
the private key (and therefore their identity), without ever revealing the
private key itself.</p>
<p><em>Mila asks you to generate a pair of SSH keys, to provide Mila only with your
public key,</em> which has no confidentiality implications, and to keep the private
key for yourself. <em>The private key must remain secret and solely known to you,</em>
because anyone who possesses it is capable of impersonating you by performing
your electronic signature.</p>
<p>During the IT Onboarding Training, you will be asked to submit that SSH
public key.</p>
<ul class="simple">
<li><p>If you do not know what SSH keys are, or are not familiar with them, you can
read the informative material <a class="reference internal" href="#ssh-private-keys"><span class="std std-ref">below</span></a>, then proceed to
generate them.</p></li>
<li><p>If you do not already have SSH keys, or are not sure if you have them, skip
to the instructions on how to generate them <a class="reference internal" href="#checking-for-ssh-keys"><span class="std std-ref">here</span></a>.</p></li>
<li><p>If you do have SSH keys, you can skip to <a class="reference internal" href="#configuring-ssh"><span class="std std-ref">configuring SSH for access to Mila</span></a>.</p></li>
</ul>
<section id="logging-in-with-ssh">
<h4>Logging in with SSH<a class="headerlink" href="#logging-in-with-ssh" title="Link to this heading"></a></h4>
<p>Login to the Mila cluster is with <code class="docutils literal notranslate"><span class="pre">ssh</span></code> through four Internet-facing
<em>login nodes</em> and a load-balancer. At each connection through the load-balancer,
you will be redirected to the least loaded login node.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "$ ";
}
</style><span class="prompt1"><span class="c1"># Generic login, will send you to one of the 4 login nodes to spread the load</span></span>
<span class="prompt1">ssh<span class="w"> </span>-p<span class="w"> </span><span class="m">2222</span><span class="w"> </span>&lt;user&gt;@login.server.mila.quebec</span>
<span class="prompt1"></span>
<span class="prompt1"><span class="c1"># To connect to a specific login node, X in [1, 2, 3, 4]</span></span>
<span class="prompt1">ssh<span class="w"> </span>-p<span class="w"> </span><span class="m">2222</span><span class="w"> </span>&lt;user&gt;@login-X.login.server.mila.quebec</span>
</pre></div></div><p>This is a significant amount of typing. You are <strong>strongly</strong> encouraged to add
a <code class="docutils literal notranslate"><span class="pre">mila</span></code> “alias” to your SSH configuration file (see <a class="reference internal" href="#configuring-ssh"><span class="std std-ref">below</span></a>
for how). With a correctly-configured SSH you can now simply run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="c1"># Login with SSH configuration in place</span></span>
<span class="prompt1">ssh<span class="w"> </span>mila</span>
<span class="prompt1"></span>
<span class="prompt1"><span class="c1"># Can also scp...        vvvv</span></span>
<span class="prompt1">scp<span class="w">  </span>file-to-upload.zip<span class="w">  </span>mila:scratch/uploaded.zip</span>
<span class="prompt1"></span>
<span class="prompt1"><span class="c1">#          vvvv  ... and rsync!</span></span>
<span class="prompt1">rsync<span class="w"> </span>-avz<span class="w"> </span>mila:my/remote/sourcecode/<span class="w">  </span>downloaded-source/</span>
</pre></div></div><p>to connect to a login node. The <code class="docutils literal notranslate"><span class="pre">mila</span></code> alias will be available to <code class="docutils literal notranslate"><span class="pre">ssh</span></code>,
<code class="docutils literal notranslate"><span class="pre">scp</span></code>, <code class="docutils literal notranslate"><span class="pre">rsync</span></code> and all other programs that consult the SSH configuration file.</p>
<p>Upon first login, you may be asked to enter your SSH key passphrase. Use the
passphrase you used to create your SSH key <a class="reference internal" href="#generating-ssh-keys"><span class="std std-ref">below</span></a>.</p>
<p>Upon first login, you may also be asked whether you trust the <strong>*Mila*</strong> login
servers’ <em>own</em> SSH keys.
The ECDSA, RSA and ED25519 fingerprints for Mila’s login nodes are:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>SHA256:baEGIa311fhnxBWsIZJ/zYhq2WfCttwyHRKzAb8zlp8 (ECDSA)
SHA256:Xr0/JqV/+5DNguPfiN5hb8rSG+nBAcfVCJoSyrR0W0o (RSA)
SHA256:gfXZzaPiaYHcrPqzHvBi6v+BWRS/lXOS/zAjOKeoBJg (ED25519)
</pre></div>
</div>
<p>If the fingerprints presented to you do not match one of the above, <strong>do not</strong>
trust them!</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can run commands on the login node with <code class="docutils literal notranslate"><span class="pre">ssh</span></code> directly, for example
<code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">mila</span> <span class="pre">squeue</span> <span class="pre">-u</span> <span class="pre">'$USER'</span></code> (remember to put single quotes around any
<code class="docutils literal notranslate"><span class="pre">$VARIABLE</span></code> you want to evaluate on the remote side, otherwise it will be
evaluated locally before ssh is even executed).</p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Login nodes are merely <em>entry points</em> to the cluster. They give you access
to the compute nodes and to the filesystem, but they are not meant to run
anything heavy. Do <strong>not</strong> run compute-heavy programs on these nodes,
because in doing so you could bring them down, impeding cluster access for
everyone.</p>
<p>This means no training scripts or experiments and no compilation of software
unless it is small or ends quickly. Do not run anything that demands a
sustained large amount of computation or a large amount of memory.</p>
<p><strong>Rule of thumb:</strong> Never run a program that takes more than a few seconds on
a login node, unless it mostly sleeps or mostly moves data.</p>
<p><strong>Examples:</strong> A non-exhaustive list of use-cases, to give a sense of what is
and is not allowed on the login nodes:</p>
<ul class="simple">
<li><p>A Python training script is unacceptable on the login nodes.
<em>(Too computationally- and memory-intensive)</em></p></li>
<li><p>A Python or shell script that downloads a dataset and exits immediately
after may be acceptable on the login nodes.
<em>(Mostly moves data)</em></p></li>
<li><p>A Python hyperparameter search script that uses <code class="docutils literal notranslate"><span class="pre">submitit</span></code> to launch
jobs and only sleeps waiting for them to end and run other jobs is
acceptable on the login nodes.
<em>(Mostly sleeps; The actual jobs run on the compute nodes)</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> of <code class="docutils literal notranslate"><span class="pre">vllm</span></code> or <code class="docutils literal notranslate"><span class="pre">flash-attn</span></code> from source code on the
login nodes is unacceptable (and is likely to fail anyways).
<em>(Takes far too much RAM to compile the CUDA kernels)</em></p></li>
<li><p>Editing code with <code class="docutils literal notranslate"><span class="pre">nano</span></code>, <code class="docutils literal notranslate"><span class="pre">vim</span></code> or <code class="docutils literal notranslate"><span class="pre">emacs</span></code> is acceptable.
<em>(Editors mostly sleep awaiting user keystrokes)</em></p></li>
<li><p>Copying/moving files with <code class="docutils literal notranslate"><span class="pre">cp</span></code>, <code class="docutils literal notranslate"><span class="pre">mv</span></code>, … is acceptable.
<em>(Mostly moves data)</em></p></li>
<li><p>Connecting to compute nodes with <code class="docutils literal notranslate"><span class="pre">ssh</span></code> is acceptable.
<em>(Mostly sleeps, forwarding keystrokes and ports to/from the node)</em></p></li>
<li><p>Using <code class="docutils literal notranslate"><span class="pre">tmux</span></code> is acceptable.
<em>(Mostly sleeps, managing the processes under its control)</em></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In a similar vein, you should not run VSCode remote SSH instances directly
on login nodes, because even though they are typically not very
computationally expensive, when many people do it, they add up! See
<a class="reference internal" href="VSCode.html#visual-studio-code"><span class="std std-ref">Visual Studio Code</span></a> for specific instructions.</p>
</div>
</div>
</section>
<section id="ssh-private-keys">
<h4>SSH Private Keys<a class="headerlink" href="#ssh-private-keys" title="Link to this heading"></a></h4>
<p>A private SSH key commonly takes the form of an obscure text file. It encodes
the digital secret of how to make an electronic signature — specifically, yours.
The content of a private SSH key might resemble</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>-----BEGIN OPENSSH PRIVATE KEY-----
b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn
NhAAAAAwEAAQAAAYEAl5dD/UU2CvauaVS2/4/iWoUyO1Hey+m8KojCFMvIywL6PPdYRqVa
FOidmOw/E9V2HVzHz/z/2Dj6TO5xNX1qJFk7A/ACGGc1+KguIDQWdjR6AZb5Tat+aAMYro
…
aSeJOS59knbQJeBwPm0g5G+iFz6R17446dXk5jn3/29AutF5MPnKwqE0mjywxCLYxVX3He
YSOCZfE80P/z4sImW82BYxAzKtI8kKagLmHS4gXJEmE13Dfyq0xcB3q5OMuQ2fZwvukTx3
xdWgyqFrMyC4wHAAAAAAEC
-----END OPENSSH PRIVATE KEY-----
</pre></div>
</div>
<p>In the real world, a handwritten signature is useless for authenticating you if
it can be easily reproduced by others. In the virtual world, the same is true.
Anyone who has your private key is capable of reproducing your electronic signature.
It is therefore essential that only one person — you — holds this private key.
The secrecy of the private key is the guarantor of your online identity.</p>
<p>Mila will <strong>*never*</strong> ask you for your private SSH key, and any pretense of
request for a private key constitutes an attempt at phishing and identity theft.
Keep your private keys safe and do not share them with anyone. Do not put them
in the cloud, your emails, Slack messages, or Git repos. Protect them with a
passphrase.</p>
</section>
<section id="ssh-public-keys">
<h4>SSH Public Keys<a class="headerlink" href="#ssh-public-keys" title="Link to this heading"></a></h4>
<p>A public SSH key is a simple line of text, albeit sometimes very long, commonly
found in a file with the <code class="docutils literal notranslate"><span class="pre">.pub</span></code> extension. It encodes the digital knowledge
required to recognize and validate your electronic signature, without however
making it possible to reproduce it elsewhere. Here are three examples of public
SSH keys:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDMYpSndal/…mPL+NXs=
ssh-ed25519 AAAA…d/ca2h  user@server
ecdsa-sha2-nistp256 AAAA…hWQcQg8=  mylaptop
</pre></div>
</div>
<p>You are requested to submit just such a public SSH key to Mila, which will
allow Mila to recognize you when you connect to the Mila cluster, but without
revealing the secret of how to perform your signature.</p>
</section>
<section id="checking-if-you-already-have-ssh-private-keys">
<span id="checking-for-ssh-keys"></span><h4>Checking If You Already Have SSH (Private) Keys<a class="headerlink" href="#checking-if-you-already-have-ssh-private-keys" title="Link to this heading"></a></h4>
<p>Usually, a private SSH key is found in the hidden directory <code class="docutils literal notranslate"><span class="pre">~/.ssh/</span></code> and is
named <code class="docutils literal notranslate"><span class="pre">id_rsa</span></code>, <code class="docutils literal notranslate"><span class="pre">id_ed25519</span></code>, or <code class="docutils literal notranslate"><span class="pre">id_ecdsa</span></code>. Its corresponding public SSH
key is usually in the same directory and shares the same filename, except with
a <code class="docutils literal notranslate"><span class="pre">.pub</span></code> suffix.</p>
</section>
<section id="generating-an-ssh-private-key">
<span id="generating-ssh-keys"></span><h4>Generating an SSH Private Key<a class="headerlink" href="#generating-an-ssh-private-key" title="Link to this heading"></a></h4>
<p>If no private SSH key already exists, you can create one with the <code class="docutils literal notranslate"><span class="pre">ssh-keygen</span></code> utility:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>RSA</p></th>
<th class="head"><p>Ed25519</p></th>
</tr>
<tr class="row-even"><th class="head"><p><em>Integer factorization</em></p></th>
<th class="head"><p><em>Elliptic curve</em></p></th>
</tr>
</thead>
<tbody>
<tr class="row-odd"><td><ul class="simple">
<li><p>Classic</p></li>
<li><p>Ultra-compatible, standardized,
the reference</p></li>
<li><p>Large key size, but configurable</p></li>
<li><p>Slow or even very slow</p></li>
</ul>
</td>
<td><ul class="simple">
<li><p>New</p></li>
<li><p>Less compatible</p></li>
<li><p>Fixed, small key size</p></li>
<li><p>Fast</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">ssh-keygen</span> <span class="pre">-t</span> <span class="pre">rsa</span> <span class="pre">-b</span> <span class="pre">3072</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">(enter</span> <span class="pre">passphrase)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">(re-enter</span> <span class="pre">passphrase)</span></code></p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">ssh-keygen</span> <span class="pre">-t</span> <span class="pre">ed25519</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">(enter</span> <span class="pre">passphrase)</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">(re-enter</span> <span class="pre">passphrase)</span></code></p>
</td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The pass-<strong>phrase</strong> protects the SSH private key <strong>on-disk</strong>. The
passphrase is <strong>not</strong> the same thing as the pass-<strong>word</strong> used to <em>log into
your personal computer account</em>. However, choosing them to be equal may
allow for automatic unlocking of encrypted SSH private keys at login, in
combination with software such as <code class="docutils literal notranslate"><span class="pre">pam_ssh(8)</span></code> (Linux) or Keychain
(Mac OS X/macOS). This makes the good practice of using encrypted keys
convenient as well.</p>
</div>
</section>
<section id="generating-an-ssh-public-key-from-a-private-key">
<h4>Generating an SSH Public Key from a Private Key<a class="headerlink" href="#generating-an-ssh-public-key-from-a-private-key" title="Link to this heading"></a></h4>
<p>If a private SSH key exists, but not its corresponding SSH public key, it can
be recalculated with the <code class="docutils literal notranslate"><span class="pre">ssh-keygen</span></code> utility as well:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>RSA</p></th>
<th class="head"><p>Ed25519</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>SSH public key:</strong></p>
<p>&gt;380 bytes &#64; 2048 bits (not rec.)</p>
<p>&gt;550 bytes &#64; 3072 bits (recommended)</p>
<p>&gt;725 bytes &#64; 4096 bits (slower)</p>
<p>&gt;1400 bytes &#64; 8192 bits (much slower)</p>
</td>
<td><p><strong>SSH public key:</strong></p>
<p>~82 bytes</p>
</td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">ssh-keygen</span> <span class="pre">-y</span> <span class="pre">-f</span> <span class="pre">~/.ssh/id_rsa</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">(enter</span> <span class="pre">passphrase)</span></code></p>
</td>
<td><p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">ssh-keygen</span> <span class="pre">-y</span> <span class="pre">-f</span> <span class="pre">~/.ssh/id_ed25519</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">(enter</span> <span class="pre">passphrase)</span></code></p>
</td>
</tr>
</tbody>
</table>
<p>It is this SSH public key that you should submit in the IT Onboarding Training form.</p>
</section>
</section>
<section id="configuring-ssh">
<h3>Configuring SSH<a class="headerlink" href="#configuring-ssh" title="Link to this heading"></a></h3>
<p>SSH uses a configuration file <code class="docutils literal notranslate"><span class="pre">~/.ssh/config</span></code> (right next to the SSH keys)
to indicate which connection settings to use for each SSH server one can
connect to.</p>
<p>The Mila <strong>login</strong> nodes require:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Hostname</span></code>: <code class="docutils literal notranslate"><span class="pre">login.server.mila.quebec</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Port</span></code>: <code class="docutils literal notranslate"><span class="pre">2222</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">User</span></code>: <em>Your Mila account username</em></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PreferredAuthentications</span></code>: <code class="docutils literal notranslate"><span class="pre">publickey,keyboard-interactive</span></code></p></li>
</ul>
<p>Password authentication will be withdrawn on <a class="reference internal" href="#ssh"><span class="std std-ref">March 31, 2025</span></a>.</p>
<p>A simple SSH configuration is automatically created and added for you to
<code class="docutils literal notranslate"><span class="pre">~/.ssh/config</span></code> by <a class="reference internal" href="#mila-init"><span class="std std-ref">mila init</span></a>.</p>
<p>Alternatively, more advanced users can edit the SSH <code class="docutils literal notranslate"><span class="pre">.config</span></code> file
<a class="reference internal" href="#manual-ssh-config"><span class="std std-ref">manually</span></a>.</p>
<section id="manual-ssh-configuration">
<span id="manual-ssh-config"></span><h4>Manual SSH configuration<a class="headerlink" href="#manual-ssh-configuration" title="Link to this heading"></a></h4>
<p>If you would like to set entries in your <code class="docutils literal notranslate"><span class="pre">~/.ssh/config</span></code> file manually for
advanced use-cases, you may use the following as inspiration:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>#   Mila
Host mila             login.server.mila.quebec
    Hostname          login.server.mila.quebec
Host mila1    login-1.login.server.mila.quebec
    Hostname  login-1.login.server.mila.quebec
Host mila2    login-2.login.server.mila.quebec
    Hostname  login-2.login.server.mila.quebec
Host mila3    login-3.login.server.mila.quebec
    Hostname  login-3.login.server.mila.quebec
Host mila4    login-4.login.server.mila.quebec
    Hostname  login-4.login.server.mila.quebec
Host mila5    login-5.login.server.mila.quebec
    Hostname  login-5.login.server.mila.quebec
Host cn-????
    Hostname             %h.server.mila.quebec
Match host !*login.server.mila.quebec,*.server.mila.quebec
    Hostname                 %h
    ProxyJump                mila
Match host           *login.server.mila.quebec
    Port                     2222
    ServerAliveInterval      120
    ServerAliveCountMax      5
Match host *.server.mila.quebec
    PreferredAuthentications publickey,keyboard-interactive
    AddKeysToAgent           yes
    ## Consider uncommenting:
    # ForwardAgent             yes
    ## Delete if on Linux, uncomment if on Mac:
    # UseKeychain              yes
    User                     CHANGEME_YOUR_MILA_USERNAME
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Please make the required edits to the template above, especially regarding
<code class="docutils literal notranslate"><span class="pre">CHANGEME_YOUR_MILA_USERNAME</span></code>!</p>
</div>
</section>
<section id="mila-init">
<span id="id3"></span><h4>mila init<a class="headerlink" href="#mila-init" title="Link to this heading"></a></h4>
<p>To make it easier to set up a productive environment, Mila publishes the
<a class="reference external" href="https://github.com/mila-iqia/milatools">milatools</a> package, which defines a <code class="docutils literal notranslate"><span class="pre">mila</span> <span class="pre">init</span></code> command which will
automatically perform some of the below steps for you. You can install it with
<code class="docutils literal notranslate"><span class="pre">pip</span></code> and use it, provided your Python version is at least 3.9:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">pip<span class="w"> </span>install<span class="w"> </span>milatools</span>
<span class="prompt1">mila<span class="w"> </span>init</span>
</pre></div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>This guide is current for <code class="docutils literal notranslate"><span class="pre">milatools</span> <span class="pre">&gt;=</span> <span class="pre">0.0.17</span></code>. If you have installed an older
version previously, run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-U</span> <span class="pre">milatools</span></code> to upgrade and re-run
<code class="docutils literal notranslate"><span class="pre">mila</span> <span class="pre">init</span></code> in order to apply new features or bug fixes.</p>
</div>
</section>
</section>
<section id="connecting-to-compute-nodes">
<h3>Connecting to compute nodes<a class="headerlink" href="#connecting-to-compute-nodes" title="Link to this heading"></a></h3>
<p>If (and only if) you have a job running on compute node <code class="docutils literal notranslate"><span class="pre">cnode</span></code>, you are
allowed to SSH to it, if for some reason you need a second terminal.
That session will be automatically ended when your job ends.</p>
<p>First, however, you need to add your public key (the one you provided to IT-support)
to the ~/.ssh/authorized_keys file on the cluster, or configure an
<code class="docutils literal notranslate"><span class="pre">ssh-agent</span></code> that will forward that key when connecting to the compute node.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="c1"># ON A LOGIN NODE</span></span>
<span class="prompt1">mkdir<span class="w"> </span>-p<span class="w"> </span>~/.ssh</span>
<span class="prompt1"><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;THE SSH PUBLIC KEY THAT YOU GAVE TO IT-SUPPORT&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>~/.ssh/authorized_keys</span>
<span class="prompt1">chmod<span class="w"> </span><span class="m">600</span><span class="w"> </span>~/.ssh/authorized_keys</span>
<span class="prompt1">chmod<span class="w"> </span><span class="m">700</span><span class="w"> </span>~/.ssh</span>
<span class="prompt1">chmod<span class="w"> </span>go-w<span class="w"> </span>~<span class="w">   </span><span class="c1"># in case you accidentally gave too many permissions for $HOME in the past.</span></span>
</pre></div></div><p>Then from the login node you can write <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">cnode</span></code>. From your local
machine, you can use <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-J</span> <span class="pre">mila</span> <span class="pre">USERNAME&#64;cnode</span></code> (<code class="docutils literal notranslate"><span class="pre">-J</span></code> represents a “jump”
through the login node, necessary because the compute nodes are behind a
firewall).</p>
<p>If you wish, you may also add the following wildcard rule in your <code class="docutils literal notranslate"><span class="pre">.ssh/config</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Host *.server.mila.quebec !*login.server.mila.quebec
    HostName %h
    User YOUR-USERNAME
    ProxyJump mila
</pre></div>
</div>
<p>This will let you connect to a compute node with <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">&lt;node&gt;.server.mila.quebec</span></code>.</p>
</section>
<section id="auto-allocation-with-mila-cpu">
<h3>Auto-allocation with mila-cpu<a class="headerlink" href="#auto-allocation-with-mila-cpu" title="Link to this heading"></a></h3>
<p>If you install <a class="reference external" href="https://github.com/mila-iqia/milatools">milatools</a> and run <code class="docutils literal notranslate"><span class="pre">mila</span> <span class="pre">init</span></code>, then you can automatically allocate
a CPU on a compute node and connect to it by running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ssh<span class="w"> </span>mila-cpu</span>
</pre></div></div><p>And that’s it! Multiple connections to <code class="docutils literal notranslate"><span class="pre">mila-cpu</span></code> will all reuse the same job, so
you can use it liberally. It also works transparently with VSCode’s Remote SSH feature.</p>
<p>We recommend using this for light work that is too heavy for a login node but does not
require a lot of resources: editing via VSCode, building conda environments, tests, etc.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">mila-cpu</span></code> entry should be in your <code class="docutils literal notranslate"><span class="pre">.ssh/config</span></code>. Changes are at your own risk.</p>
</section>
<section id="using-a-non-bash-unix-shell">
<h3>Using a non-Bash Unix shell<a class="headerlink" href="#using-a-non-bash-unix-shell" title="Link to this heading"></a></h3>
<p>While Mila does not provide support in debugging your shell setup, Bash is the
standard shell to be used on the cluster and the cluster is designed to support
both Bash and Zsh shells. If you think things should work with Zsh and they
don’t, please contact <a class="reference external" href="https://it-support.mila.quebec">Mila’s IT support</a>.</p>
</section>
</section>
<section id="running-your-code">
<h2>Running your code<a class="headerlink" href="#running-your-code" title="Link to this heading"></a></h2>
<section id="slurm-commands-guide">
<h3>SLURM commands guide<a class="headerlink" href="#slurm-commands-guide" title="Link to this heading"></a></h3>
<section id="basic-usage">
<h4>Basic Usage<a class="headerlink" href="#basic-usage" title="Link to this heading"></a></h4>
<p>The SLURM <a class="reference external" href="https://slurm.schedmd.com/documentation.html">documentation</a>
provides extensive information on the available commands to query the cluster
status or submit jobs.</p>
<p>Below are some basic examples of how to use SLURM.</p>
</section>
<section id="submitting-jobs">
<h4>Submitting jobs<a class="headerlink" href="#submitting-jobs" title="Link to this heading"></a></h4>
<section id="batch-job">
<h5>Batch job<a class="headerlink" href="#batch-job" title="Link to this heading"></a></h5>
<p>In order to submit a batch job, you have to create a script containing the main
command(s) you would like to execute on the allocated resources/nodes.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH --job-name=test</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --output=job_output.txt</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --error=job_error.txt</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --ntasks=1</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --time=10:00</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH --mem=100Gb</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span>module<span class="w"> </span>load<span class="w"> </span>python/3.5
<span class="linenos">10</span>python<span class="w"> </span>my_script.py
</pre></div>
</div>
<p>Your job script is then submitted to SLURM with <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> (<a class="reference external" href="https://slurm.schedmd.com/sbatch.html">ref.</a>)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt2:before {
  content: " ";
}
</style><span class="prompt1">sbatch<span class="w"> </span>job_script</span>
<span class="prompt2">sbatch:<span class="w"> </span>Submitted<span class="w"> </span>batch<span class="w"> </span>job<span class="w"> </span><span class="m">4323674</span></span>
</pre></div></div><p>The <em>working directory</em> of the job will be the one where your executed <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Slurm directives can be specified on the command line alongside <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> or
inside the job script with a line starting with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>.</p>
</div>
</section>
<section id="interactive-job">
<h5>Interactive job<a class="headerlink" href="#interactive-job" title="Link to this heading"></a></h5>
<p>Workload managers usually run batch jobs to avoid having to watch its
progression and let the scheduler run it as soon as resources are available. If
you want to get access to a shell while leveraging cluster resources, you can
submit an interactive jobs where the main executable is a shell with the
<code class="docutils literal notranslate"><span class="pre">srun/salloc</span></code> (<a class="reference external" href="https://slurm.schedmd.com/srun.html">srun</a>/<a class="reference external" href="https://slurm.schedmd.com/salloc.html">salloc</a>) commands</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">salloc</span>
</pre></div></div><p>Will start an interactive job on the first node available with the default
resources set in SLURM (1 task/1 CPU).  <code class="docutils literal notranslate"><span class="pre">srun</span></code> accepts the same arguments as
<code class="docutils literal notranslate"><span class="pre">sbatch</span></code> with the exception that the environment is not passed.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To pass your current environment to an interactive job, add
<code class="docutils literal notranslate"><span class="pre">--preserve-env</span></code> to <code class="docutils literal notranslate"><span class="pre">srun</span></code>.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">salloc</span></code> can also be used and is mostly a wrapper around <code class="docutils literal notranslate"><span class="pre">srun</span></code> if provided
without more info but it gives more flexibility if for example you want to get
an allocation on multiple nodes.</p>
</section>
</section>
<section id="job-submission-arguments">
<h4>Job submission arguments<a class="headerlink" href="#job-submission-arguments" title="Link to this heading"></a></h4>
<p>In order to accurately select the resources for your job, several arguments are
available. The most important ones are:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Argument</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>-n, –ntasks=&lt;number&gt;</p></td>
<td><p>The number of task in your script, usually =1</p></td>
</tr>
<tr class="row-odd"><td><p>-c, –cpus-per-task=&lt;ncpus&gt;</p></td>
<td><p>The number of cores for each task</p></td>
</tr>
<tr class="row-even"><td><p>-t, –time=&lt;time&gt;</p></td>
<td><p>Time requested for your job</p></td>
</tr>
<tr class="row-odd"><td><p>–mem=&lt;size[units]&gt;</p></td>
<td><p>Memory requested for all your tasks</p></td>
</tr>
<tr class="row-even"><td><p>–gres=&lt;list&gt;</p></td>
<td><p>Select generic resources such as GPUs for your job: <code class="docutils literal notranslate"><span class="pre">--gres=gpu:GPU_MODEL</span></code></p></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Always consider requesting the adequate amount of resources to improve the
scheduling of your job (small jobs always run first).</p>
</div>
</section>
<section id="checking-job-status">
<h4>Checking job status<a class="headerlink" href="#checking-job-status" title="Link to this heading"></a></h4>
<p>To display <em>jobs</em> currently in queue, use <code class="docutils literal notranslate"><span class="pre">squeue</span></code> and to get only your jobs type</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">squeue<span class="w"> </span>-u<span class="w"> </span><span class="nv">$USER</span></span>
<span class="prompt2">JOBID<span class="w">   </span>USER<span class="w">          </span>NAME<span class="w">    </span>ST<span class="w">  </span>START_TIME<span class="w">         </span>TIME<span class="w"> </span>NODES<span class="w"> </span>CPUS<span class="w"> </span>TRES_PER_NMIN_MEM<span class="w"> </span>NODELIST<span class="w"> </span><span class="o">(</span>REASON<span class="o">)</span><span class="w"> </span>COMMENT</span>
<span class="prompt2"><span class="m">133</span><span class="w">     </span>my_username<span class="w">   </span>myjob<span class="w">   </span>R<span class="w">   </span><span class="m">2019</span>-03-28T18:33<span class="w">   </span><span class="m">0</span>:50<span class="w">     </span><span class="m">1</span><span class="w">    </span><span class="m">2</span><span class="w">        </span>N/A<span class="w">  </span>7000M<span class="w"> </span>node1<span class="w"> </span><span class="o">(</span>None<span class="o">)</span><span class="w"> </span><span class="o">(</span>null<span class="o">)</span></span>
</pre></div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>The maximum number of jobs able to be submitted to the system per user is 1000 (MaxSubmitJobs=1000)
at any given time from the given association. If this limit is reached, new submission requests
will be denied until existing jobs in this association complete.</p>
</div>
</section>
<section id="removing-a-job">
<h4>Removing a job<a class="headerlink" href="#removing-a-job" title="Link to this heading"></a></h4>
<p>To cancel your job simply use <code class="docutils literal notranslate"><span class="pre">scancel</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">scancel<span class="w"> </span><span class="m">4323674</span></span>
</pre></div></div></section>
</section>
<section id="partitioning">
<h3>Partitioning<a class="headerlink" href="#partitioning" title="Link to this heading"></a></h3>
<p>Since we don’t have many GPUs on the cluster, resources must be shared as fairly
as possible.  The <code class="docutils literal notranslate"><span class="pre">--partition=/-p</span></code> flag of SLURM allows you to set the
priority you need for a job.  Each job assigned with a priority can preempt jobs
with a lower priority: <code class="docutils literal notranslate"><span class="pre">unkillable</span> <span class="pre">&gt;</span> <span class="pre">main</span> <span class="pre">&gt;</span> <span class="pre">long</span></code>. Once preempted, your job is
killed without notice and is automatically re-queued on the same partition until
resources are available. (To leverage a different preemption mechanism, see the
<a class="reference internal" href="#advanced-preemption"><span class="std std-ref">Handling preemption</span></a>)</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Flag</p></th>
<th class="head"><p>Max Resource Usage</p></th>
<th class="head"><p>Max Time</p></th>
<th class="head"><p>Note</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>--partition=unkillable</p></td>
<td><p>6  CPUs, mem=32G,  1 GPU</p></td>
<td><p>2 days</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>--partition=unkillable-cpu</p></td>
<td><p>2  CPUs, mem=16G</p></td>
<td><p>2 days</p></td>
<td><p>CPU-only jobs</p></td>
</tr>
<tr class="row-even"><td><p>--partition=short-unkillable</p></td>
<td><p>mem=1000G, 4 GPUs</p></td>
<td><p>3 hours (!)</p></td>
<td><p>Large but short jobs</p></td>
</tr>
<tr class="row-odd"><td><p>--partition=main</p></td>
<td><p>8  CPUs, mem=48G,  2 GPUs</p></td>
<td><p>5 days</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>--partition=main-cpu</p></td>
<td><p>8  CPUs, mem=64G</p></td>
<td><p>5 days</p></td>
<td><p>CPU-only jobs</p></td>
</tr>
<tr class="row-odd"><td><p>--partition=long</p></td>
<td><p>no limit of resources</p></td>
<td><p>7 days</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>--partition=long-cpu</p></td>
<td><p>no limit of resources</p></td>
<td><p>7 days</p></td>
<td><p>CPU-only jobs</p></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Historically, before the 2022 introduction of CPU-only nodes (e.g. the <code class="docutils literal notranslate"><span class="pre">cn-f</span></code>
series), CPU jobs ran side-by-side with the GPU jobs on GPU nodes. To prevent
them obstructing any GPU job, they were always lowest-priority and preemptible.
This was implemented by automatically assigning them to one of the now-obsolete
partitions <code class="docutils literal notranslate"><span class="pre">cpu_jobs</span></code>, <code class="docutils literal notranslate"><span class="pre">cpu_jobs_low</span></code> or <code class="docutils literal notranslate"><span class="pre">cpu_jobs_low-grace</span></code>.
<strong>Do not use these partition names anymore</strong>. Prefer the <code class="docutils literal notranslate"><span class="pre">*-cpu</span></code> partition
names defined above.</p>
<p>For backwards-compatibility purposes, the legacy partition names are translated
to their effective equivalent <code class="docutils literal notranslate"><span class="pre">long-cpu</span></code>, but they will eventually be removed
entirely.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>As a convenience</em>, should you request the <code class="docutils literal notranslate"><span class="pre">unkillable</span></code>, <code class="docutils literal notranslate"><span class="pre">main</span></code> or <code class="docutils literal notranslate"><span class="pre">long</span></code>
partition for a CPU-only job, the partition will be translated to its <code class="docutils literal notranslate"><span class="pre">-cpu</span></code>
equivalent automatically.</p>
</div>
<p>For instance, to request an unkillable job with 1 GPU, 4 CPUs, 10G of RAM and
12h of computation do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--mem<span class="o">=</span>10G<span class="w"> </span>-t<span class="w"> </span><span class="m">12</span>:00:00<span class="w"> </span>--partition<span class="o">=</span>unkillable<span class="w"> </span>&lt;job.sh&gt;</span>
</pre></div></div><p>You can also make it an interactive job using <code class="docutils literal notranslate"><span class="pre">salloc</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">salloc<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--mem<span class="o">=</span>10G<span class="w"> </span>-t<span class="w"> </span><span class="m">12</span>:00:00<span class="w"> </span>--partition<span class="o">=</span>unkillable</span>
</pre></div></div><p>The Mila cluster has many different types of nodes/GPUs. To request a specific
type of node/GPU, you can add specific feature requirements to your job
submission command.</p>
<p>To access those special nodes you need to request them explicitly by adding the
flag <code class="docutils literal notranslate"><span class="pre">--constraint=&lt;name&gt;</span></code>.  The full list of nodes in the Mila Cluster can be
accessed <a class="reference internal" href="Information.html#node-profile-description"><span class="std std-ref">Node profile description</span></a>.</p>
<p><em>Examples:</em></p>
<p>To request a machine with 2 GPUs using NVLink, you can use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch<span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--gres<span class="o">=</span>gpu:2<span class="w"> </span>--constraint<span class="o">=</span>nvlink</span>
</pre></div></div><p>To request a DGX system with 8 A100 GPUs, you can use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch<span class="w"> </span>-c<span class="w"> </span><span class="m">16</span><span class="w"> </span>--gres<span class="o">=</span>gpu:8<span class="w"> </span>--constraint<span class="o">=</span><span class="s2">&quot;dgx&amp;ampere&quot;</span></span>
</pre></div></div><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Particularities</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>12gb/32gb/40gb/48gb/80gb</p></td>
<td><p>Request a specific amount of <em>GPU</em> memory</p></td>
</tr>
<tr class="row-odd"><td><p>volta/turing/ampere</p></td>
<td><p>Request a specific <em>GPU</em> architecture</p></td>
</tr>
<tr class="row-even"><td><p>nvlink</p></td>
<td><p>Machine with GPUs using the NVLink interconnect technology</p></td>
</tr>
<tr class="row-odd"><td><p>dgx</p></td>
<td><p>NVIDIA <em>DGX</em> system with DGX OS</p></td>
</tr>
</tbody>
</table>
<section id="information-on-partitions-nodes">
<h4>Information on partitions/nodes<a class="headerlink" href="#information-on-partitions-nodes" title="Link to this heading"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">sinfo</span></code> (<a class="reference external" href="https://slurm.schedmd.com/sinfo.html">ref.</a>) provides most of the
information about available nodes and partitions/queues to submit jobs to.</p>
<p>Partitions are a group of nodes usually sharing similar features. On a
partition, some job limits can be applied which will override those asked for a
job (i.e. max time, max CPUs, etc…)</p>
<p>To display available <em>partitions</em>, simply use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sinfo</span>
<span class="prompt2">PARTITION<span class="w"> </span>AVAIL<span class="w"> </span>TIMELIMIT<span class="w"> </span>NODES<span class="w"> </span>STATE<span class="w">  </span>NODELIST</span>
<span class="prompt2">batch<span class="w">     </span>up<span class="w">     </span>infinite<span class="w">     </span><span class="m">2</span><span class="w"> </span>alloc<span class="w">  </span>node<span class="o">[</span><span class="m">1</span>,3,5-9<span class="o">]</span></span>
<span class="prompt2">batch<span class="w">     </span>up<span class="w">     </span>infinite<span class="w">     </span><span class="m">6</span><span class="w"> </span>idle<span class="w">   </span>node<span class="o">[</span><span class="m">10</span>-15<span class="o">]</span></span>
<span class="prompt2">cpu<span class="w">       </span>up<span class="w">     </span>infinite<span class="w">     </span><span class="m">6</span><span class="w"> </span>idle<span class="w">   </span>cpu_node<span class="o">[</span><span class="m">1</span>-15<span class="o">]</span></span>
<span class="prompt2">gpu<span class="w">       </span>up<span class="w">     </span>infinite<span class="w">     </span><span class="m">6</span><span class="w"> </span>idle<span class="w">   </span>gpu_node<span class="o">[</span><span class="m">1</span>-15<span class="o">]</span></span>
</pre></div></div><p>To display available <em>nodes</em> and their status, you can use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sinfo<span class="w"> </span>-N<span class="w"> </span>-l</span>
<span class="prompt2">NODELIST<span class="w">    </span>NODES<span class="w"> </span>PARTITION<span class="w"> </span>STATE<span class="w">  </span>CPUS<span class="w"> </span>MEMORY<span class="w"> </span>TMP_DISK<span class="w"> </span>WEIGHT<span class="w"> </span>FEATURES<span class="w"> </span>REASON</span>
<span class="prompt2">node<span class="o">[</span><span class="m">1</span>,3,5-9<span class="o">]</span><span class="w">   </span><span class="m">2</span><span class="w"> </span>batch<span class="w">     </span>allocated<span class="w"> </span><span class="m">2</span><span class="w">    </span><span class="m">246</span><span class="w">    </span><span class="m">16000</span><span class="w">     </span><span class="m">0</span><span class="w">  </span><span class="o">(</span>null<span class="o">)</span><span class="w">   </span><span class="o">(</span>null<span class="o">)</span></span>
<span class="prompt2">node<span class="o">[</span><span class="m">2</span>,4<span class="o">]</span><span class="w">       </span><span class="m">2</span><span class="w"> </span>batch<span class="w">     </span>drain<span class="w">     </span><span class="m">2</span><span class="w">    </span><span class="m">246</span><span class="w">    </span><span class="m">16000</span><span class="w">     </span><span class="m">0</span><span class="w">  </span><span class="o">(</span>null<span class="o">)</span><span class="w">   </span><span class="o">(</span>null<span class="o">)</span></span>
<span class="prompt2">node<span class="o">[</span><span class="m">10</span>-15<span class="o">]</span><span class="w">     </span><span class="m">6</span><span class="w"> </span>batch<span class="w">     </span>idle<span class="w">      </span><span class="m">2</span><span class="w">    </span><span class="m">246</span><span class="w">    </span><span class="m">16000</span><span class="w">     </span><span class="m">0</span><span class="w">  </span><span class="o">(</span>null<span class="o">)</span><span class="w">   </span><span class="o">(</span>null<span class="o">)</span></span>
<span class="prompt2">...</span>
</pre></div></div><p>And to get statistics on a job running or terminated, use <code class="docutils literal notranslate"><span class="pre">sacct</span></code> with some of
the fields you want to display</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sacct<span class="w"> </span>--format<span class="o">=</span>User,JobID,Jobname,partition,state,time,start,end,elapsed,nnodes,ncpus,nodelist,workdir<span class="w"> </span>-u<span class="w"> </span><span class="nv">$USER</span></span>
<span class="prompt2"><span class="w">     </span>User<span class="w">        </span>JobID<span class="w">    </span>JobName<span class="w">  </span>Partition<span class="w">      </span>State<span class="w">  </span>Timelimit<span class="w">               </span>Start<span class="w">                 </span>End<span class="w">    </span>Elapsed<span class="w">   </span>NNodes<span class="w">      </span>NCPUS<span class="w">        </span>NodeList<span class="w">              </span>WorkDir</span>
<span class="prompt2">---------<span class="w"> </span>------------<span class="w"> </span>----------<span class="w"> </span>----------<span class="w"> </span>----------<span class="w"> </span>----------<span class="w"> </span>-------------------<span class="w"> </span>-------------------<span class="w"> </span>----------<span class="w"> </span>--------<span class="w"> </span>----------<span class="w"> </span>---------------<span class="w"> </span>--------------------</span>
<span class="prompt2">my_usern+<span class="w"> </span><span class="m">2398</span><span class="w">         </span>run_extra+<span class="w">      </span>batch<span class="w">    </span>RUNNING<span class="w"> </span><span class="m">130</span>-05:00+<span class="w"> </span><span class="m">2019</span>-03-27T18:33:43<span class="w">             </span>Unknown<span class="w"> </span><span class="m">1</span>-01:07:54<span class="w">        </span><span class="m">1</span><span class="w">         </span><span class="m">16</span><span class="w"> </span>node9<span class="w">           </span>/home/mila/my_usern+</span>
<span class="prompt2">my_usern+<span class="w"> </span><span class="m">2399</span><span class="w">         </span>run_extra+<span class="w">      </span>batch<span class="w">    </span>RUNNING<span class="w"> </span><span class="m">130</span>-05:00+<span class="w"> </span><span class="m">2019</span>-03-26T08:51:38<span class="w">             </span>Unknown<span class="w"> </span><span class="m">2</span>-10:49:59<span class="w">        </span><span class="m">1</span><span class="w">         </span><span class="m">16</span><span class="w"> </span>node9<span class="w">           </span>/home/mila/my_usern+</span>
</pre></div></div><p>Or to get the list of all your previous jobs, use the <code class="docutils literal notranslate"><span class="pre">--start=YYYY-MM-DD</span></code> flag. You can check <code class="docutils literal notranslate"><span class="pre">sacct(1)</span></code> for further information about additional time formats.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sacct<span class="w"> </span>-u<span class="w"> </span><span class="nv">$USER</span><span class="w"> </span>--start<span class="o">=</span><span class="m">2019</span>-01-01</span>
</pre></div></div><p><code class="docutils literal notranslate"><span class="pre">scontrol</span></code> (<a class="reference external" href="https://slurm.schedmd.com/scontrol.html">ref.</a>) can be used to
provide specific information on a job (currently running or recently terminated)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">scontrol<span class="w"> </span>show<span class="w"> </span>job<span class="w"> </span><span class="m">43123</span></span>
<span class="prompt2"><span class="nv">JobId</span><span class="o">=</span><span class="m">43123</span><span class="w"> </span><span class="nv">JobName</span><span class="o">=</span>python_script.py</span>
<span class="prompt2"><span class="nv">UserId</span><span class="o">=</span>my_username<span class="o">(</span><span class="m">1500000111</span><span class="o">)</span><span class="w"> </span><span class="nv">GroupId</span><span class="o">=</span>student<span class="o">(</span><span class="m">1500000000</span><span class="o">)</span><span class="w"> </span><span class="nv">MCS_label</span><span class="o">=</span>N/A</span>
<span class="prompt2"><span class="nv">Priority</span><span class="o">=</span><span class="m">645895</span><span class="w"> </span><span class="nv">Nice</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">Account</span><span class="o">=</span>my_username<span class="w"> </span><span class="nv">QOS</span><span class="o">=</span>normal</span>
<span class="prompt2"><span class="nv">JobState</span><span class="o">=</span>RUNNING<span class="w"> </span><span class="nv">Reason</span><span class="o">=</span>None<span class="w"> </span><span class="nv">Dependency</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">Requeue</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">Restarts</span><span class="o">=</span><span class="m">3</span><span class="w"> </span><span class="nv">BatchFlag</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">Reboot</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">ExitCode</span><span class="o">=</span><span class="m">0</span>:0</span>
<span class="prompt2"><span class="nv">RunTime</span><span class="o">=</span><span class="m">2</span>-10:41:57<span class="w"> </span><span class="nv">TimeLimit</span><span class="o">=</span><span class="m">130</span>-05:00:00<span class="w"> </span><span class="nv">TimeMin</span><span class="o">=</span>N/A</span>
<span class="prompt2"><span class="nv">SubmitTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:47:17<span class="w"> </span><span class="nv">EligibleTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:49:18</span>
<span class="prompt2"><span class="nv">AccrueTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:49:18</span>
<span class="prompt2"><span class="nv">StartTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:51:38<span class="w"> </span><span class="nv">EndTime</span><span class="o">=</span><span class="m">2019</span>-08-03T13:51:38<span class="w"> </span><span class="nv">Deadline</span><span class="o">=</span>N/A</span>
<span class="prompt2"><span class="nv">PreemptTime</span><span class="o">=</span>None<span class="w"> </span><span class="nv">SuspendTime</span><span class="o">=</span>None<span class="w"> </span><span class="nv">SecsPreSuspend</span><span class="o">=</span><span class="m">0</span></span>
<span class="prompt2"><span class="nv">LastSchedEval</span><span class="o">=</span><span class="m">2019</span>-03-26T08:49:18</span>
<span class="prompt2"><span class="nv">Partition</span><span class="o">=</span>slurm_partition<span class="w"> </span>AllocNode:Sid<span class="o">=</span>login-node-1:14586</span>
<span class="prompt2"><span class="nv">ReqNodeList</span><span class="o">=(</span>null<span class="o">)</span><span class="w"> </span><span class="nv">ExcNodeList</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">NodeList</span><span class="o">=</span>node2</span>
<span class="prompt2"><span class="nv">BatchHost</span><span class="o">=</span>node2</span>
<span class="prompt2"><span class="nv">NumNodes</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">NumCPUs</span><span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="nv">NumTasks</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>CPUs/Task<span class="o">=</span><span class="m">16</span><span class="w"> </span>ReqB:S:C:T<span class="o">=</span><span class="m">0</span>:0:*:*</span>
<span class="prompt2"><span class="nv">TRES</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span><span class="m">16</span>,mem<span class="o">=</span>32000M,node<span class="o">=</span><span class="m">1</span>,billing<span class="o">=</span><span class="m">3</span></span>
<span class="prompt2">Socks/Node<span class="o">=</span>*<span class="w"> </span>NtasksPerN:B:S:C<span class="o">=</span><span class="m">1</span>:0:*:*<span class="w"> </span><span class="nv">CoreSpec</span><span class="o">=</span>*</span>
<span class="prompt2"><span class="nv">MinCPUsNode</span><span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="nv">MinMemoryNode</span><span class="o">=</span>32000M<span class="w"> </span><span class="nv">MinTmpDiskNode</span><span class="o">=</span><span class="m">0</span></span>
<span class="prompt2"><span class="nv">Features</span><span class="o">=(</span>null<span class="o">)</span><span class="w"> </span><span class="nv">DelayBoot</span><span class="o">=</span><span class="m">00</span>:00:00</span>
<span class="prompt2"><span class="nv">OverSubscribe</span><span class="o">=</span>OK<span class="w"> </span><span class="nv">Contiguous</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">Licenses</span><span class="o">=(</span>null<span class="o">)</span><span class="w"> </span><span class="nv">Network</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">WorkDir</span><span class="o">=</span>/home/mila/my_username</span>
<span class="prompt2"><span class="nv">StdErr</span><span class="o">=</span>/home/mila/my_username/slurm-43123.out</span>
<span class="prompt2"><span class="nv">StdIn</span><span class="o">=</span>/dev/null</span>
<span class="prompt2"><span class="nv">StdOut</span><span class="o">=</span>/home/mila/my_username/slurm-43123.out</span>
<span class="prompt2"><span class="nv">Power</span><span class="o">=</span></span>
</pre></div></div><p>Or more info on a node and its resources</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">scontrol<span class="w"> </span>show<span class="w"> </span>node<span class="w"> </span>node9</span>
<span class="prompt2"><span class="nv">NodeName</span><span class="o">=</span>node9<span class="w"> </span><span class="nv">Arch</span><span class="o">=</span>x86_64<span class="w"> </span><span class="nv">CoresPerSocket</span><span class="o">=</span><span class="m">4</span></span>
<span class="prompt2"><span class="nv">CPUAlloc</span><span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="nv">CPUTot</span><span class="o">=</span><span class="m">16</span><span class="w"> </span><span class="nv">CPULoad</span><span class="o">=</span><span class="m">1</span>.38</span>
<span class="prompt2"><span class="nv">AvailableFeatures</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">ActiveFeatures</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">Gres</span><span class="o">=(</span>null<span class="o">)</span></span>
<span class="prompt2"><span class="nv">NodeAddr</span><span class="o">=</span><span class="m">10</span>.252.232.4<span class="w"> </span><span class="nv">NodeHostName</span><span class="o">=</span>mila20684000000<span class="w"> </span><span class="nv">Port</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">Version</span><span class="o">=</span><span class="m">18</span>.08</span>
<span class="prompt2"><span class="nv">OS</span><span class="o">=</span>Linux<span class="w"> </span><span class="m">4</span>.15.0-1036<span class="w"> </span><span class="c1">#38-Ubuntu SMP Fri Dec 7 02:47:47 UTC 2018</span></span>
<span class="prompt2"><span class="nv">RealMemory</span><span class="o">=</span><span class="m">32000</span><span class="w"> </span><span class="nv">AllocMem</span><span class="o">=</span><span class="m">32000</span><span class="w"> </span><span class="nv">FreeMem</span><span class="o">=</span><span class="m">23262</span><span class="w"> </span><span class="nv">Sockets</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="nv">Boards</span><span class="o">=</span><span class="m">1</span></span>
<span class="prompt2"><span class="nv">State</span><span class="o">=</span>ALLOCATED+CLOUD<span class="w"> </span><span class="nv">ThreadsPerCore</span><span class="o">=</span><span class="m">2</span><span class="w"> </span><span class="nv">TmpDisk</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">Weight</span><span class="o">=</span><span class="m">1</span><span class="w"> </span><span class="nv">Owner</span><span class="o">=</span>N/A<span class="w"> </span><span class="nv">MCS_label</span><span class="o">=</span>N/A</span>
<span class="prompt2"><span class="nv">Partitions</span><span class="o">=</span>slurm_partition</span>
<span class="prompt2"><span class="nv">BootTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:50:01<span class="w"> </span><span class="nv">SlurmdStartTime</span><span class="o">=</span><span class="m">2019</span>-03-26T08:51:15</span>
<span class="prompt2"><span class="nv">CfgTRES</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span><span class="m">16</span>,mem<span class="o">=</span>32000M,billing<span class="o">=</span><span class="m">3</span></span>
<span class="prompt2"><span class="nv">AllocTRES</span><span class="o">=</span><span class="nv">cpu</span><span class="o">=</span><span class="m">16</span>,mem<span class="o">=</span>32000M</span>
<span class="prompt2"><span class="nv">CapWatts</span><span class="o">=</span>n/a</span>
<span class="prompt2"><span class="nv">CurrentWatts</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">LowestJoules</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">ConsumedJoules</span><span class="o">=</span><span class="m">0</span></span>
<span class="prompt2"><span class="nv">ExtSensorsJoules</span><span class="o">=</span>n/s<span class="w"> </span><span class="nv">ExtSensorsWatts</span><span class="o">=</span><span class="m">0</span><span class="w"> </span><span class="nv">ExtSensorsTemp</span><span class="o">=</span>n/s</span>
</pre></div></div></section>
</section>
<section id="useful-commands">
<h3>Useful Commands<a class="headerlink" href="#useful-commands" title="Link to this heading"></a></h3>
<dl class="simple">
<dt>salloc</dt><dd><p>Get an interactive job and give you a shell. (ssh like) CPU only</p>
</dd>
<dt>salloc --gres=gpu:1 -c 2 --mem=12000</dt><dd><p>Get an interactive job with one GPU, 2 CPUs and 12000 MB RAM</p>
</dd>
<dt>sbatch</dt><dd><p>start a batch job (same options as salloc)</p>
</dd>
<dt>sattach --pty &lt;jobid&gt;.0</dt><dd><p>Re-attach a dropped interactive job</p>
</dd>
<dt>sinfo</dt><dd><p>status of all nodes</p>
</dd>
<dt>sinfo -Ogres:27,nodelist,features -tidle,mix,alloc</dt><dd><p>List GPU type and FEATURES that you can request</p>
</dd>
<dt>savail</dt><dd><p>(Custom) List available gpu</p>
</dd>
<dt>scancel &lt;jobid&gt;</dt><dd><p>Cancel a job</p>
</dd>
<dt>squeue</dt><dd><p>summary status of all active jobs</p>
</dd>
<dt>squeue -u $USER</dt><dd><p>summary status of all YOUR active jobs</p>
</dd>
<dt>squeue -j &lt;jobid&gt;</dt><dd><p>summary status of a specific job</p>
</dd>
<dt>squeue -Ojobid,name,username,partition,state,timeused,nodelist,gres,tres</dt><dd><p>status of all jobs including requested resources (see the SLURM squeue doc for all output options)</p>
</dd>
<dt>scontrol show job &lt;jobid&gt;</dt><dd><p>Detailed status of a running job</p>
</dd>
<dt>sacct -j &lt;job_id&gt; -o NodeList</dt><dd><p>Get the node where a finished job ran</p>
</dd>
<dt>sacct -u $USER -S &lt;start_time&gt; -E &lt;stop_time&gt;</dt><dd><p>Find info about old jobs</p>
</dd>
<dt>sacct -oJobID,JobName,User,Partition,Node,State</dt><dd><p>List of current and recent jobs</p>
</dd>
</dl>
</section>
<section id="special-gpu-requirements">
<h3>Special GPU requirements<a class="headerlink" href="#special-gpu-requirements" title="Link to this heading"></a></h3>
<p>Specific GPU <em>architecture</em> and <em>memory</em> can be easily requested through the
<code class="docutils literal notranslate"><span class="pre">--gres</span></code> flag by using either</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:architecture:number</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:memory:number</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:model:number</span></code></p></li>
</ul>
<p><em>Example:</em></p>
<p>To request 1 GPU with <em>at least</em> 48GB of memory use</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sbatch<span class="w"> </span>-c<span class="w"> </span><span class="m">4</span><span class="w"> </span>--gres<span class="o">=</span>gpu:48gb:1</span>
</pre></div></div><p>The full list of GPU and their features can be accessed <a class="reference internal" href="Information.html#node-list"><span class="std std-ref">here</span></a>.</p>
</section>
<section id="example-script">
<h3>Example script<a class="headerlink" href="#example-script" title="Link to this heading"></a></h3>
<p>Here is a <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> script that follows good practices on the Mila cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --partition=unkillable                           # Ask for unkillable job</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --cpus-per-task=2                                # Ask for 2 CPUs</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --gres=gpu:1                                     # Ask for 1 GPU</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --mem=10G                                        # Ask for 10 GB of RAM</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH --time=3:00:00                                   # The job will run for 3 hours</span>
<span class="linenos"> 8</span><span class="c1">#SBATCH -o /network/scratch/&lt;u&gt;/&lt;username&gt;/slurm-%j.out  # Write the log on scratch</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span><span class="c1"># 1. Load the required modules</span>
<span class="linenos">11</span>module<span class="w"> </span>--quiet<span class="w"> </span>load<span class="w"> </span>anaconda/3
<span class="linenos">12</span>
<span class="linenos">13</span><span class="c1"># 2. Load your environment</span>
<span class="linenos">14</span>conda<span class="w"> </span>activate<span class="w"> </span><span class="s2">&quot;&lt;env_name&gt;&quot;</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="c1"># 3. Copy your dataset on the compute node</span>
<span class="linenos">17</span>cp<span class="w"> </span>/network/datasets/&lt;dataset&gt;<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1"># 4. Launch your job, tell it to save the model in $SLURM_TMPDIR</span>
<span class="linenos">20</span><span class="c1">#    and look for the dataset into $SLURM_TMPDIR</span>
<span class="linenos">21</span>python<span class="w"> </span>main.py<span class="w"> </span>--path<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span><span class="w"> </span>--data_path<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>
<span class="linenos">22</span>
<span class="linenos">23</span><span class="c1"># 5. Copy whatever you want to save on $SCRATCH</span>
<span class="linenos">24</span>cp<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/&lt;to_save&gt;<span class="w"> </span>/network/scratch/&lt;u&gt;/&lt;username&gt;/
</pre></div>
</div>
</section>
</section>
<section id="portability-concerns-and-solutions">
<h2>Portability concerns and solutions<a class="headerlink" href="#portability-concerns-and-solutions" title="Link to this heading"></a></h2>
<p>When working on a software project, it is important to be aware of all the
software and libraries the project relies on and to list them explicitly and
<em>under a version control system</em> in such a way that they can easily be
installed and made available on different systems. The upsides are significant:</p>
<ul class="simple">
<li><p>Easily install and run on the cluster</p></li>
<li><p>Ease of collaboration</p></li>
<li><p>Better reproducibility</p></li>
</ul>
<p>To achieve this, try to always keep in mind the following aspects:</p>
<ul class="simple">
<li><p><strong>Versions:</strong> For each dependency, make sure you have some record of the
specific version you are using during development. That way, in the future, you
will be able to reproduce the original environment which you know to be
compatible. Indeed, the more time passes, the more likely it is that newer
versions of some dependency have breaking changes. The <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">freeze</span></code> command can create
such a record for Python dependencies.</p></li>
<li><p><strong>Isolation:</strong> Ideally, each of your software projects should be isolated from
the others. What this means is that updating the environment for project A
should <em>not</em> update the environment for project B. That way, you can freely
install and upgrade software and libraries for the former without worrying about
breaking the latter (which you might not notice until weeks later, the next time
you work on project B!) Isolation can be made easy using <a class="reference internal" href="#using-uv"><span class="std std-ref">UV</span></a>, as well as
<a class="reference internal" href="Theory_cluster.html#python-virtual-environments"><span class="std std-ref">Python Virtual environments</span></a> and, as a last resort, <a class="reference internal" href="Theory_cluster.html#containers"><span class="std std-ref">Containers</span></a>.</p></li>
</ul>
<section id="managing-your-environments">
<h3>Managing your environments<a class="headerlink" href="#managing-your-environments" title="Link to this heading"></a></h3>
</section>
</section>
<section id="virtual-environments">
<span id="python"></span><h2>Virtual environments<a class="headerlink" href="#virtual-environments" title="Link to this heading"></a></h2>
<p>A virtual environment in Python is a local, isolated environment in which you
can install or uninstall Python packages without interfering with the global
environment (or other virtual environments). It usually lives in a directory
(location varies depending on whether you use venv, conda or poetry). In order
to use a virtual environment, you have to <strong>activate</strong> it. Activating an
environment essentially sets environment variables in your shell so that:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">python</span></code> points to the right Python version for that environment (different
virtual environments can use different versions of Python!)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span></code> looks for packages in the virtual environment</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> installs packages into the virtual environment</p></li>
<li><p>Any shell commands installed via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> are made available</p></li>
</ul>
<p>To run experiments within a virtual environment, you can simply activate it
in the script given to <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<section id="pip-virtualenv">
<h3>Pip/Virtualenv<a class="headerlink" href="#pip-virtualenv" title="Link to this heading"></a></h3>
<p>Pip is the most widely used package manager for Python and each cluster provides
several Python versions through the associated module which comes with pip. In
order to install new packages, you will first have to create a personal space
for them to be stored.  The usual solution (as it is the recommended solution
on Digital Research Alliance of Canada clusters) is to use <a class="reference external" href="https://virtualenv.pypa.io/en/stable/">virtual
environments</a>, although <a class="reference internal" href="#using-uv"><span class="std std-ref">UV</span></a> is now
the recommended way to manage Python installations, virtual environments and dependencies.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We recommend you use <a class="reference external" href="https://docs.astral.sh/uv">UV</a> to manage your Python
virtual environments instead of doing it manually.
<a class="reference internal" href="#using-uv"><span class="std std-ref">The next section</span></a> will give an overview of how to install it and use it.</p>
</div>
<p>First, load the Python module you want to use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>load<span class="w"> </span>python/3.8</span>
</pre></div></div><p>Then, create a virtual environment in your <code class="docutils literal notranslate"><span class="pre">home</span></code> directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">python<span class="w"> </span>-m<span class="w"> </span>venv<span class="w"> </span><span class="nv">$HOME</span>/&lt;env&gt;</span>
</pre></div></div><p>Where <code class="docutils literal notranslate"><span class="pre">&lt;env&gt;</span></code> is the name of your environment. Finally, activate the environment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">source</span><span class="w"> </span><span class="nv">$HOME</span>/&lt;env&gt;/bin/activate</span>
</pre></div></div><p>You can now install any Python package you wish using the <code class="docutils literal notranslate"><span class="pre">pip</span></code> command, e.g.
<a class="reference external" href="https://pytorch.org/get-started/locally">pytorch</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt3:before {
  content: "(<env>)$ ";
}
</style><span class="prompt3">pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision</span>
</pre></div></div><p>Or <a class="reference external" href="https://www.tensorflow.org/install/gpu">Tensorflow</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt3">pip<span class="w"> </span>install<span class="w"> </span>tensorflow-gpu</span>
</pre></div></div></section>
<section id="using-uv">
<span id="id6"></span><h3>UV<a class="headerlink" href="#using-uv" title="Link to this heading"></a></h3>
<p>In many cases, where your dependencies are Python packages, we highly recommend using <a class="reference external" href="https://docs.astral.sh/uv">UV</a>, a modern package manager for Python.</p>
<p>In addition to all the same features as pip, it also manages Python installations,
virtual environments, and makes your environments easier to reproduce and reuse across compute clusters.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>UV is not currently available as a module on the Mila or DRAC clusters at the time of writing.
To use it, you first need to install it using this command on a cluster login node:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">curl<span class="w"> </span>-LsSf<span class="w"> </span>https://astral.sh/uv/install.sh<span class="w"> </span><span class="p">|</span><span class="w"> </span>sh</span>
</pre></div></div></div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Pip/virtualenv command</p></th>
<th class="head"><p>UV pip equivalent</p></th>
<th class="head"><p>UV <a class="reference external" href="https://docs.astral.sh/uv/guides/projects/">project</a> command (recommended)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Create your virtualenv</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">python/3.10</span></code>
then <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">venv</span></code></p></td>
<td><p><a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-venv">uv venv</a></p></td>
<td><p><a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-init">uv init</a> and <a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-sync">uv sync</a></p></td>
</tr>
<tr class="row-odd"><td><p>Activate the virtualenv</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">.venv/bin/activate</span></code></p></td>
<td><p>(same)</p></td>
<td><p>(same, but often unnecessary)</p></td>
</tr>
<tr class="row-even"><td><p>Install a package</p></td>
<td><p>activate venv then <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code></p></td>
<td><p><a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-pip-install">uv pip install</a></p></td>
<td><p><a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-add">uv add</a></p></td>
</tr>
<tr class="row-odd"><td><p>Run a command
(ex. <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">main.py</span></code>)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">python</span></code>, then
<code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">&lt;venv&gt;/bin/activate</span></code>, then
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">main.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">.</span> <span class="pre">&lt;venv&gt;/bin/activate</span></code>,
then <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">main.py</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">run</span> <span class="pre">python</span> <span class="pre">main.py</span></code></p></td>
</tr>
<tr class="row-even"><td><p>Where are
dependencies declared?</p></td>
<td><p><em>Maybe</em> in a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code>,
<code class="docutils literal notranslate"><span class="pre">setup.py</span></code> or <code class="docutils literal notranslate"><span class="pre">pyproject.toml</span></code></p></td>
<td><p><em>Maybe</em> in a <code class="docutils literal notranslate"><span class="pre">requirements.txt</span></code>,
<code class="docutils literal notranslate"><span class="pre">setup.py</span></code> or <code class="docutils literal notranslate"><span class="pre">pyproject.toml</span></code></p></td>
<td><p><a class="reference external" href="https://docs.astral.sh/uv/guides/projects/#pyprojecttoml">pyproject.toml</a></p></td>
</tr>
<tr class="row-odd"><td><p>Easy to change Python
versions?</p></td>
<td><p>No</p></td>
<td><p>somewhat</p></td>
<td><p>Yes: <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">python</span> <span class="pre">pin</span> <span class="pre">&lt;version&gt;</span></code> or
<code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">sync</span> <span class="pre">--python</span> <span class="pre">&lt;version&gt;</span></code></p></td>
</tr>
</tbody>
</table>
<p>While you can use UV as a drop-in replacement for pip, we recommend adopting a <a class="reference external" href="https://docs.astral.sh/uv/guides/projects/">project-based workflow</a>:</p>
<ul>
<li><p>Use <a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-init">uv init</a> to create a new project. A <code class="docutils literal notranslate"><span class="pre">pyproject.toml</span></code> file will be created. This is where your dependencies are listed.</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">uv<span class="w"> </span>init<span class="w"> </span>--python<span class="o">=</span><span class="m">3</span>.12</span>
</pre></div></div></div></blockquote>
</li>
<li><p>Use <a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-add">uv add</a> to add (and <a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-remove">uv remove</a> to remove) dependencies to your project. This will update the <code class="docutils literal notranslate"><span class="pre">pyproject.toml</span></code> file and update the virtual environment.</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">uv<span class="w"> </span>add<span class="w"> </span>torch</span>
</pre></div></div></div></blockquote>
</li>
<li><dl>
<dt>Use <a class="reference external" href="https://docs.astral.sh/uv/reference/cli/#uv-run">uv run</a> to run commands, for example <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">run</span> <span class="pre">python</span> <span class="pre">train.py</span></code>. This will automatically do the following:</dt><dd><ol class="arabic simple">
<li><p>Create or update the virtualenv (with the correct Python version) if necessary, based the dependencies in <code class="docutils literal notranslate"><span class="pre">pyproject.toml</span></code>.</p></li>
<li><p>Activates the virtualenv.</p></li>
<li><p>Runs the command you provided, e.g. <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">train.py</span></code>.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">uv<span class="w"> </span>run<span class="w"> </span>python<span class="w"> </span>main.py</span>
</pre></div></div></dd>
</dl>
</li>
</ul>
</section>
<section id="conda">
<h3>Conda<a class="headerlink" href="#conda" title="Link to this heading"></a></h3>
<p>Another solution for Python is to use <a class="reference external" href="https://docs.conda.io/en/latest/miniconda.html">miniconda</a> or <a class="reference external" href="https://docs.anaconda.com">anaconda</a> which are also available through the <code class="docutils literal notranslate"><span class="pre">module</span></code>
command: (the use of Conda is not recommended for Digital Research Alliance of
Canada clusters due to the availability of custom-built packages for pip)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>load<span class="w"> </span>miniconda/3</span>
<span class="prompt2"><span class="o">===</span><span class="w"> </span>Module<span class="w"> </span>miniconda/3<span class="w"> </span><span class="nv">loaded</span><span class="w"> </span><span class="o">===]</span></span>
<span class="prompt2">o<span class="w"> </span><span class="nb">enable</span><span class="w"> </span>conda<span class="w"> </span>environment<span class="w"> </span>functions,<span class="w"> </span>first<span class="w"> </span>use:</span>
</pre></div></div><p>To create an environment (see <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">here</a>
for details) using a specific Python version, you may write:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>&lt;env&gt;<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.9</span>
</pre></div></div><p>Where <code class="docutils literal notranslate"><span class="pre">&lt;env&gt;</span></code> is the name of your environment. You can now activate it by doing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">conda<span class="w"> </span>activate<span class="w"> </span>&lt;env&gt;</span>
</pre></div></div><p>You are now ready to install any Python package you want in this environment.
For instance, to install PyTorch, you can find the Conda command of any version
you want on <a class="reference external" href="https://pytorch.org/get-started/locally">pytorch’s website</a>, e.g:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt3">conda<span class="w"> </span>install<span class="w"> </span>pytorch<span class="w"> </span>torchvision<span class="w"> </span><span class="nv">cudatoolkit</span><span class="o">=</span><span class="m">10</span>.0<span class="w"> </span>-c<span class="w"> </span>pytorch</span>
</pre></div></div><p>If you make a lot of environments and install/uninstall a lot of packages, it
can be good to periodically clean up Conda’s cache:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt3">conda<span class="w"> </span>clean<span class="w"> </span>-it</span>
</pre></div></div><section id="mamba">
<h4>Mamba<a class="headerlink" href="#mamba" title="Link to this heading"></a></h4>
<p>When installing new packages with <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span></code>, conda uses a built-in
dependency solver for solving the dependency graph of all packages (and their
versions) requested such that package dependency conflicts are avoided.</p>
<p>In some cases, especially when there are many packages already installed in a
conda environment, conda’s built-in dependency solver can struggle to solve the
dependency graph, taking several to tens of minutes, and sometimes never
solving. In these cases, it is recommended to try <a class="reference external" href="https://conda.github.io/conda-libmamba-solver/getting-started/">libmamba</a>.</p>
<p>To install and set the <code class="docutils literal notranslate"><span class="pre">libmamba</span></code> solver, run the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt2"><span class="c1"># Install miniconda</span></span>
<span class="prompt2"><span class="c1"># (you can not use the preinstalled anaconda/miniconda as installing libmamba</span></span>
<span class="prompt2"><span class="c1">#  requires ownership over the anaconda/miniconda install directory)</span></span>
<span class="prompt1">wget<span class="w"> </span>https://repo.anaconda.com/miniconda/Miniconda3-py310_22.11.1-1-Linux-x86_64.sh</span>
<span class="prompt1">bash<span class="w"> </span>Miniconda3-py310_22.11.1-1-Linux-x86_64.sh</span>
<span class="prompt2"></span>
<span class="prompt2"><span class="c1"># Install libmamba</span></span>
<span class="prompt1">conda<span class="w"> </span>install<span class="w"> </span>-n<span class="w"> </span>base<span class="w"> </span>conda-libmamba-solver</span>
</pre></div></div><p>By default, conda uses the built-in solver when installing packages, even after
installing other solvers. To try <code class="docutils literal notranslate"><span class="pre">libmamba</span></code> once, add <code class="docutils literal notranslate"><span class="pre">--solver=libmamba</span></code> in
your <code class="docutils literal notranslate"><span class="pre">`conda</span> <span class="pre">install`</span></code> command. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">conda<span class="w"> </span>install<span class="w"> </span>tensorflow<span class="w"> </span>--solver<span class="o">=</span>libmamba</span>
</pre></div></div><p>You can set <code class="docutils literal notranslate"><span class="pre">libmamba</span></code> as the default solver by adding <code class="docutils literal notranslate"><span class="pre">solver:</span> <span class="pre">libmamba</span></code>
to your <code class="docutils literal notranslate"><span class="pre">.condarc</span></code> configuration file located under your <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> directory.
You can create it if it doesn’t exist. You can also run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">conda<span class="w"> </span>config<span class="w"> </span>--set<span class="w"> </span>solver<span class="w"> </span>libmamba</span>
</pre></div></div></section>
</section>
<section id="using-modules">
<h3>Using Modules<a class="headerlink" href="#using-modules" title="Link to this heading"></a></h3>
<p>A lot of software, such as Python and Conda, is already compiled and available on
the cluster through the <code class="docutils literal notranslate"><span class="pre">module</span></code> command and its sub-commands. In particular,
if you wish to use <code class="docutils literal notranslate"><span class="pre">Python</span> <span class="pre">3.7</span></code> you can simply do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>load<span class="w"> </span>python/3.7</span>
</pre></div></div><section id="the-module-command">
<h4>The module command<a class="headerlink" href="#the-module-command" title="Link to this heading"></a></h4>
<p>For a list of available modules, simply use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>avail</span>
<span class="prompt2">--------------------------------------------------------------------------------------------------------------<span class="w"> </span>Global<span class="w"> </span>Aliases<span class="w"> </span>---------------------------------------------------------------------------------------------------------------</span>
<span class="prompt2"><span class="w">  </span>cuda/10.0<span class="w"> </span>-&gt;<span class="w"> </span>cudatoolkit/10.0<span class="w">    </span>cuda/9.2<span class="w">      </span>-&gt;<span class="w"> </span>cudatoolkit/9.2<span class="w">                                 </span>pytorch/1.4.1<span class="w">       </span>-&gt;<span class="w"> </span>python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.1<span class="w">    </span>tensorflow/1.15<span class="w"> </span>-&gt;<span class="w"> </span>python/3.7/tensorflow/1.15</span>
<span class="prompt2"><span class="w">  </span>cuda/10.1<span class="w"> </span>-&gt;<span class="w"> </span>cudatoolkit/10.1<span class="w">    </span>mujoco-py<span class="w">     </span>-&gt;<span class="w"> </span>python/3.7/mujoco-py/2.0<span class="w">                        </span>pytorch/1.5.0<span class="w">       </span>-&gt;<span class="w"> </span>python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.0<span class="w">    </span>tensorflow/2.2<span class="w">  </span>-&gt;<span class="w"> </span>python/3.7/tensorflow/2.2</span>
<span class="prompt2"><span class="w">  </span>cuda/10.2<span class="w"> </span>-&gt;<span class="w"> </span>cudatoolkit/10.2<span class="w">    </span>mujoco-py/2.0<span class="w"> </span>-&gt;<span class="w"> </span>python/3.7/mujoco-py/2.0<span class="w">                        </span>pytorch/1.5.1<span class="w">       </span>-&gt;<span class="w"> </span>python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1</span>
<span class="prompt2"><span class="w">  </span>cuda/11.0<span class="w"> </span>-&gt;<span class="w"> </span>cudatoolkit/11.0<span class="w">    </span>pytorch<span class="w">       </span>-&gt;<span class="w"> </span>python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1<span class="w">    </span>tensorflow<span class="w">          </span>-&gt;<span class="w"> </span>python/3.7/tensorflow/2.2</span>
<span class="prompt2"><span class="w">  </span>cuda/9.0<span class="w">  </span>-&gt;<span class="w"> </span>cudatoolkit/9.0<span class="w">     </span>pytorch/1.4.0<span class="w"> </span>-&gt;<span class="w"> </span>python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.0<span class="w">    </span>tensorflow-cpu/1.15<span class="w"> </span>-&gt;<span class="w"> </span>python/3.7/tensorflow/1.15</span>
<span class="prompt2"></span>
<span class="prompt2">--------------------------------------------------------------------------------------------------<span class="w"> </span>/cvmfs/config.mila.quebec/modules/Core<span class="w"> </span>---------------------------------------------------------------------------------------------------</span>
<span class="prompt2"><span class="w">  </span>Mila<span class="w">       </span><span class="o">(</span>S,L<span class="o">)</span><span class="w">    </span>anaconda/3<span class="w"> </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>go/1.13.5<span class="w">        </span>miniconda/2<span class="w">        </span>mujoco/1.50<span class="w">        </span>python/2.7<span class="w">    </span>python/3.6<span class="w">        </span>python/3.8<span class="w">           </span>singularity/3.0.3<span class="w">    </span>singularity/3.2.1<span class="w">    </span>singularity/3.5.3<span class="w"> </span><span class="o">(</span>D<span class="o">)</span></span>
<span class="prompt2"><span class="w">  </span>anaconda/2<span class="w">          </span>go/1.12.4<span class="w">         </span>go/1.14<span class="w">   </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>miniconda/3<span class="w"> </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>mujoco/2.0<span class="w">  </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>python/3.5<span class="w">    </span>python/3.7<span class="w"> </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>singularity/2.6.1<span class="w">    </span>singularity/3.1.1<span class="w">    </span>singularity/3.4.2</span>
<span class="prompt2"></span>
<span class="prompt2">------------------------------------------------------------------------------------------------<span class="w"> </span>/cvmfs/config.mila.quebec/modules/Compiler<span class="w"> </span>-------------------------------------------------------------------------------------------------</span>
<span class="prompt2"><span class="w">  </span>python/3.7/mujoco-py/2.0</span>
<span class="prompt2"></span>
<span class="prompt2">--------------------------------------------------------------------------------------------------<span class="w"> </span>/cvmfs/config.mila.quebec/modules/Cuda<span class="w"> </span>---------------------------------------------------------------------------------------------------</span>
<span class="prompt2"><span class="w">  </span>cuda/10.0/cudnn/7.3<span class="w">        </span>cuda/10.0/nccl/2.4<span class="w">         </span>cuda/10.1/nccl/2.4<span class="w">     </span>cuda/11.0/nccl/2.7<span class="w">        </span>cuda/9.0/nccl/2.4<span class="w">     </span>cudatoolkit/9.0<span class="w">     </span>cudatoolkit/10.1<span class="w">        </span>cudnn/7.6/cuda/10.0/tensorrt/7.0</span>
<span class="prompt2"><span class="w">  </span>cuda/10.0/cudnn/7.5<span class="w">        </span>cuda/10.1/cudnn/7.5<span class="w">        </span>cuda/10.2/cudnn/7.6<span class="w">    </span>cuda/9.0/cudnn/7.3<span class="w">        </span>cuda/9.2/cudnn/7.6<span class="w">    </span>cudatoolkit/9.2<span class="w">     </span>cudatoolkit/10.2<span class="w">        </span>cudnn/7.6/cuda/10.1/tensorrt/7.0</span>
<span class="prompt2"><span class="w">  </span>cuda/10.0/cudnn/7.6<span class="w"> </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>cuda/10.1/cudnn/7.6<span class="w"> </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>cuda/10.2/nccl/2.7<span class="w">     </span>cuda/9.0/cudnn/7.5<span class="w"> </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>cuda/9.2/nccl/2.4<span class="w">     </span>cudatoolkit/10.0<span class="w">    </span>cudatoolkit/11.0<span class="w"> </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>cudnn/7.6/cuda/9.0/tensorrt/7.0</span>
<span class="prompt2"></span>
<span class="prompt2">------------------------------------------------------------------------------------------------<span class="w"> </span>/cvmfs/config.mila.quebec/modules/Pytorch<span class="w"> </span>--------------------------------------------------------------------------------------------------</span>
<span class="prompt2"><span class="w">  </span>python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.4.1<span class="w">    </span>python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.5.1<span class="w"> </span><span class="o">(</span>D<span class="o">)</span><span class="w">    </span>python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.0</span>
<span class="prompt2"><span class="w">  </span>python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.5.0<span class="w">    </span>python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.1<span class="w">        </span>python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1<span class="w"> </span><span class="o">(</span>D<span class="o">)</span></span>
<span class="prompt2"></span>
<span class="prompt2">-----------------------------------------------------------------------------------------------<span class="w"> </span>/cvmfs/config.mila.quebec/modules/Tensorflow<span class="w"> </span>------------------------------------------------------------------------------------------------</span>
<span class="prompt2"><span class="w">  </span>python/3.7/tensorflow/1.15<span class="w">    </span>python/3.7/tensorflow/2.0<span class="w">    </span>python/3.7/tensorflow/2.2<span class="w"> </span><span class="o">(</span>D<span class="o">)</span></span>
</pre></div></div><p>Modules can be loaded using the <code class="docutils literal notranslate"><span class="pre">load</span></code> command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>load<span class="w"> </span>&lt;module&gt;</span>
</pre></div></div><p>To search for a module or a software, use the command <code class="docutils literal notranslate"><span class="pre">spider</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>spider<span class="w"> </span>search_term</span>
</pre></div></div><p>E.g.: by default, <code class="docutils literal notranslate"><span class="pre">python2</span></code> will refer to the os-shipped installation of <code class="docutils literal notranslate"><span class="pre">python2.7</span></code> and <code class="docutils literal notranslate"><span class="pre">python3</span></code> to <code class="docutils literal notranslate"><span class="pre">python3.6</span></code>.
If you want to use <code class="docutils literal notranslate"><span class="pre">python3.7</span></code> you can type:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>load<span class="w"> </span>python3.7</span>
</pre></div></div></section>
<section id="available-software">
<h4>Available Software<a class="headerlink" href="#available-software" title="Link to this heading"></a></h4>
<p>Modules are divided in 5 main sections:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Section</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Core</p></td>
<td><p>Base interpreter and software (Python, go, etc…)</p></td>
</tr>
<tr class="row-odd"><td><p>Compiler</p></td>
<td><p>Interpreter-dependent software (<em>see the note below</em>)</p></td>
</tr>
<tr class="row-even"><td><p>Cuda</p></td>
<td><p>Toolkits, cudnn and related libraries</p></td>
</tr>
<tr class="row-odd"><td><p>Pytorch/Tensorflow</p></td>
<td><p>Pytorch/TF built with a specific Cuda/Cudnn
version for Mila’s GPUs (<em>see the related paragraph</em>)</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Modules which are nested (../../..) usually depend on other software/module
loaded alongside the main module.  No need to load the dependent software,
the complex naming scheme allows an automatic detection of the dependent
module(s):</p>
<p>i.e.: Loading <code class="docutils literal notranslate"><span class="pre">cudnn/7.6/cuda/9.0/tensorrt/7.0</span></code> will load <code class="docutils literal notranslate"><span class="pre">cudnn/7.6</span></code> and
<code class="docutils literal notranslate"><span class="pre">cuda/9.0</span></code> alongside</p>
<p><code class="docutils literal notranslate"><span class="pre">python/3.X</span></code> is a particular dependency which can be served through
<code class="docutils literal notranslate"><span class="pre">python/3.X</span></code> or <code class="docutils literal notranslate"><span class="pre">anaconda/3</span></code> and is not automatically loaded to let the
user pick his favorite flavor.</p>
</div>
</section>
<section id="default-package-location">
<h4>Default package location<a class="headerlink" href="#default-package-location" title="Link to this heading"></a></h4>
<p>Python by default uses the user site package first and packages provided by
<code class="docutils literal notranslate"><span class="pre">module</span></code> last to not interfere with your installation.  If you want to skip
packages installed in your site-packages folder (in your /home directory), you
have to start Python with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> flag.</p>
<p>To check which package is loaded at import, you can print <code class="docutils literal notranslate"><span class="pre">package.__file__</span></code>
to get the full path of the package.</p>
<p><em>Example:</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>load<span class="w"> </span>pytorch/1.5.0</span>
<span class="prompt1">python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import torch;print(torch.__file__)&#39;</span></span>
<span class="prompt2">home/mila/my_home/.local/lib/python3.7/site-packages/torch/__init__.py<span class="w">   </span>&lt;<span class="o">==</span><span class="w"> </span>package<span class="w"> </span>from<span class="w"> </span>your<span class="w"> </span>own<span class="w"> </span>site-package</span>
</pre></div></div><p>Now with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> flag:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">module<span class="w"> </span>load<span class="w"> </span>pytorch/1.5.0</span>
<span class="prompt1">python<span class="w"> </span>-s<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import torch;print(torch.__file__)&#39;</span></span>
<span class="prompt2">cvmfs/ai.mila.quebec/apps/x86_64/debian/pytorch/python3.7-cuda10.1-cudnn7.6-v1.5.0/lib/python3.7/site-packages/torch/__init__.py<span class="err">&#39;</span></span>
</pre></div></div></section>
</section>
<section id="on-using-containers">
<h3>On using containers<a class="headerlink" href="#on-using-containers" title="Link to this heading"></a></h3>
<p>Another option for creating portable code is <a class="reference internal" href="#using-containers"><span class="std std-ref">Using containers</span></a>.</p>
<p>Containers are a popular approach at deploying applications by packaging a lot
of the required dependencies together. The most popular tool for this is
<a class="reference external" href="https://www.docker.com/">Docker</a>, but Docker cannot be used on the Mila
cluster (nor the other clusters from Digital Research Alliance of Canada).</p>
<p>One popular mechanism for containerisation on a computational cluster is called
<a class="reference external" href="https://singularity-docs.readthedocs.io/en/latest/">Singularity</a>.
This is the recommended approach for running containers on the
Mila cluster. See section <a class="reference internal" href="#id8"><span class="std std-ref">Singularity</span></a> for more details.</p>
</section>
</section>
<section id="using-containers">
<span id="id7"></span><h2>Using containers<a class="headerlink" href="#using-containers" title="Link to this heading"></a></h2>
<p>Podman containers are now available as tech preview on the Mila cluster
without root privileges using <a class="reference external" href="https://podman.io">podman</a>.</p>
<p>Generally any command-line argument accepted by docker will work with podman.
This means that you can mostly use the docker examples you find on the web by
replacing <cite>docker</cite> with <cite>podman</cite> in the command line.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Complete Podman Documentation: <a class="reference external" href="https://docs.podman.io/en/stable/">https://docs.podman.io/en/stable/</a></p>
</div>
<section id="using-in-slurm">
<h3>Using in SLURM<a class="headerlink" href="#using-in-slurm" title="Link to this heading"></a></h3>
<p>To use podman you can just use the <cite>podman</cite> command in either a batch script or
an interactive job.</p>
<p>One difference in configuration is that for certain technical reasons all the
storage for podman (images, containers, …) is on a job-specific location and
will be lost after the job is complete or preempted. If you have data that must
be preseved across jobs, you can <a class="reference external" href="https://docs.podman.io/en/v5.2.4/markdown/podman-run.1.html#mount-type-type-type-specific-option">mount</a>
a local folder inside the container, such as <cite>$SCRATCH</cite> or your home to save
data.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>podman<span class="w"> </span>run<span class="w"> </span>--mount<span class="w"> </span><span class="nv">type</span><span class="o">=</span>bind,source<span class="o">=</span><span class="nv">$SCRATCH</span>/exp,destination<span class="o">=</span>/data/exp<span class="w"> </span>bash<span class="w"> </span>touch<span class="w"> </span>/data/exp/file
$<span class="w"> </span>ls<span class="w"> </span><span class="nv">$SCRATCH</span>/exp
file
</pre></div>
</div>
<p>You can use multiple containers in a single job, but you have to be careful
about the memory and CPU limits of the job.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Due to the cluster environment you may see warning messages like
<code class="docutils literal notranslate"><span class="pre">WARN[0000]</span> <span class="pre">&quot;/&quot;</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">a</span> <span class="pre">shared</span> <span class="pre">mount,</span> <span class="pre">this</span> <span class="pre">could</span> <span class="pre">cause</span> <span class="pre">issues</span> <span class="pre">or</span> <span class="pre">missing</span> <span class="pre">mounts</span> <span class="pre">with</span> <span class="pre">rootless</span> <span class="pre">containers</span></code>,
<code class="docutils literal notranslate"><span class="pre">ERRO[0000]</span> <span class="pre">cannot</span> <span class="pre">find</span> <span class="pre">UID/GID</span> <span class="pre">for</span> <span class="pre">user</span> <span class="pre">&lt;user&gt;:</span> <span class="pre">no</span> <span class="pre">subuid</span> <span class="pre">ranges</span> <span class="pre">found</span> <span class="pre">for</span> <span class="pre">user</span> <span class="pre">&quot;&lt;user&gt;&quot;</span> <span class="pre">in</span> <span class="pre">/etc/subuid</span> <span class="pre">-</span> <span class="pre">check</span> <span class="pre">rootless</span> <span class="pre">mode</span> <span class="pre">in</span> <span class="pre">man</span> <span class="pre">pages.</span></code>,
<code class="docutils literal notranslate"><span class="pre">WARN[0000]</span> <span class="pre">Using</span> <span class="pre">rootless</span> <span class="pre">single</span> <span class="pre">mapping</span> <span class="pre">into</span> <span class="pre">the</span> <span class="pre">namespace.</span> <span class="pre">This</span> <span class="pre">might</span> <span class="pre">break</span> <span class="pre">some</span> <span class="pre">images.</span> <span class="pre">Check</span> <span class="pre">/etc/subuid</span> <span class="pre">and</span> <span class="pre">/etc/subgid</span> <span class="pre">for</span> <span class="pre">adding</span> <span class="pre">sub*ids</span> <span class="pre">if</span> <span class="pre">not</span> <span class="pre">using</span> <span class="pre">a</span> <span class="pre">network</span> <span class="pre">user</span></code>
or
<code class="docutils literal notranslate"><span class="pre">WARN[0005]</span> <span class="pre">Failed</span> <span class="pre">to</span> <span class="pre">add</span> <span class="pre">pause</span> <span class="pre">process</span> <span class="pre">to</span> <span class="pre">systemd</span> <span class="pre">sandbox</span> <span class="pre">cgroup:</span> <span class="pre">dbus:</span> <span class="pre">couldn't</span> <span class="pre">determine</span> <span class="pre">address</span> <span class="pre">of</span> <span class="pre">session</span> <span class="pre">bus</span></code>
but as far as we can see those can be safely ignored and should not have
an impact on your images.</p>
</div>
</section>
<section id="gpu">
<h3>GPU<a class="headerlink" href="#gpu" title="Link to this heading"></a></h3>
<p>To use a GPU in a container, you need a GPU job and then use <code class="docutils literal notranslate"><span class="pre">--device</span>
<span class="pre">nvidia.com/gpu=all</span></code> to make all GPUs allocated available in the container or
<code class="docutils literal notranslate"><span class="pre">--device</span> <span class="pre">nvidia.com/gpu=N</span></code> where <cite>N</cite> is the gpu index you want in the
container, starting at 0.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ nvidia-smi
Fri Dec 13 12:47:34 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:4A:00.0 Off |                    0 |
| N/A   25C    P8             36W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40S                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   26C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
$ podman run --device nvidia.com/gpu=all nvidia/cuda:11.6.1-base-ubuntu20.04 nvidia-smi
Fri Dec 13 17:48:21 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:4A:00.0 Off |                    0 |
| N/A   25C    P8             36W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA L40S                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   25C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
$ podman run --device nvidia.com/gpu=0 nvidia/cuda:11.6.1-base-ubuntu20.04 nvidia-smi
Fri Dec 13 17:48:33 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:4A:00.0 Off |                    0 |
| N/A   25C    P8             36W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
$ podman run --device nvidia.com/gpu=1 nvidia/cuda:11.6.1-base-ubuntu20.04 nvidia-smi
Fri Dec 13 17:48:40 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA L40S                    On  |   00000000:61:00.0 Off |                    0 |
| N/A   25C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
</pre></div>
</div>
<p>You can pass <code class="docutils literal notranslate"><span class="pre">--device</span></code> multiple times to add more than one gpus to the container.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>CDI (GPU) support documentation:
<a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html#running-a-workload-with-cdi">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html#running-a-workload-with-cdi</a></p>
</div>
</section>
</section>
<section id="id8">
<h2>Singularity<a class="headerlink" href="#id8" title="Link to this heading"></a></h2>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h3>
<section id="what-is-singularity">
<h4>What is Singularity?<a class="headerlink" href="#what-is-singularity" title="Link to this heading"></a></h4>
<p>Running Docker on SLURM is a security problem (e.g. running as root, being able
to mount any directory).  The alternative is to use Singularity, which is a
popular solution in the world of HPC.</p>
<p>There is a good level of compatibility between Docker and Singularity,
and we can find many exaggerated claims about able to convert containers
from Docker to Singularity without any friction.
Oftentimes, Docker images from DockerHub are 100% compatible with Singularity,
and they can indeed be used without friction, but things get messy when
we try to convert our own Docker build files to Singularity recipes.</p>
</section>
<section id="links-to-official-documentation">
<h4>Links to official documentation<a class="headerlink" href="#links-to-official-documentation" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>official <a class="reference external" href="https://singularity-docs.readthedocs.io/en/latest/">Singularity user guide</a> (this is the one you
will use most often)</p></li>
<li><p>official <a class="reference external" href="https://sylabs.io/guides/latest/admin-guide/">Singularity admin guide</a></p></li>
</ul>
</section>
<section id="overview-of-the-steps-used-in-practice">
<h4>Overview of the steps used in practice<a class="headerlink" href="#overview-of-the-steps-used-in-practice" title="Link to this heading"></a></h4>
<p>Most often, the process to create and use a Singularity container is:</p>
<ul class="simple">
<li><p>on your Linux computer (at home or work)</p>
<ul>
<li><p>select a Docker image from DockerHub (e.g. <em>pytorch/pytorch</em>)</p></li>
<li><p>make a recipe file for Singularity that starts with that DockerHub image</p></li>
<li><p>build the recipe file, thus creating the image file (e.g. <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>)</p></li>
<li><p>test your singularity container before send it over to the cluster</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rsync</span> <span class="pre">-av</span> <span class="pre">my-pytorch-image.sif</span> <span class="pre">&lt;login-node&gt;:Documents/my-singularity-images</span></code></p></li>
</ul>
</li>
<li><p>on the login node for that cluster</p>
<ul>
<li><p>queue your jobs with <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">...</span></code></p></li>
<li><p>(note that your jobs will copy over the <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code> to $SLURM_TMPDIR
and will then launch Singularity with that image)</p></li>
<li><p>do something else while you wait for them to finish</p></li>
<li><p>queue more jobs with the same <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>,
reusing it many times over</p></li>
</ul>
</li>
</ul>
<p>In the following sections you will find specific examples or tips to accomplish
in practice the steps highlighted above.</p>
</section>
<section id="nope-not-on-macos">
<h4>Nope, not on MacOS<a class="headerlink" href="#nope-not-on-macos" title="Link to this heading"></a></h4>
<p>Singularity does not work on MacOS, as of the time of this writing in 2021.
Docker does not <em>actually</em> run on MacOS, but there Docker silently installs a
virtual machine running Linux, which makes it a pleasant experience,
and the user does not need to care about the details of how Docker does it.</p>
<p>Given its origins in HPC, Singularity does not provide that kind of seamless
experience on MacOS, even though it’s technically possible to run it
inside a Linux virtual machine on MacOS.</p>
</section>
<section id="where-to-build-images">
<h4>Where to build images<a class="headerlink" href="#where-to-build-images" title="Link to this heading"></a></h4>
<p>Building Singularity images is a rather heavy task, which can take 20 minutes
if you have a lot of steps in your recipe. This makes it a bad task to run on
the login nodes of our clusters, especially if it needs to be run regularly.</p>
<p>On the Mila cluster, we are lucky to have unrestricted internet access on the compute
nodes, which means that anyone can request an interactive CPU node (no need for GPU)
and build their images there without problem.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not build Singularity images from scratch every time your run a
job in a large batch.  This will be a colossal waste of GPU time as well as
internet bandwidth.  If you setup your workflow properly (e.g. using bind
paths for your code and data), you can spend months reusing the same
Singularity image <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>.</p>
</div>
</section>
</section>
<section id="building-the-containers">
<h3>Building the containers<a class="headerlink" href="#building-the-containers" title="Link to this heading"></a></h3>
<p>Building a container is like creating a new environment except that containers
are much more powerful since they are self-contained systems.  With
singularity, there are two ways to build containers.</p>
<p>The first one is by yourself, it’s like when you got a new Linux laptop and you
don’t really know what you need, if you see that something is missing, you
install it. Here you can get a vanilla container with Ubuntu called a sandbox,
you log in and you install each packages by yourself.  This procedure can take
time but will allow you to understand how things work and what you need. This is
recommended if you need to figure out how things will be compiled or if you want
to install packages on the fly. We’ll refer to this procedure as singularity
sandboxes.</p>
<p>The second way is more like you know what you want, so you write a list of
everything you need, you send it to singularity and it will install everything
for you. Those lists are called singularity recipes.</p>
<section id="first-way-build-and-use-a-sandbox">
<h4>First way: Build and use a sandbox<a class="headerlink" href="#first-way-build-and-use-a-sandbox" title="Link to this heading"></a></h4>
<p>You might ask yourself: <em>On which machine should I build a container?</em></p>
<p>First of all, you need to choose where you’ll build your container. This
operation requires <strong>memory and high cpu usage</strong>.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do NOT build containers on any login nodes !</p>
</div>
<ul>
<li><p>(Recommended for beginner) If you need to <strong>use apt-get</strong>, you should <strong>build
the container on your laptop</strong> with sudo privileges. You’ll only need to
install singularity on your laptop. Windows/Mac users can look <a class="reference external" href="https://www.sylabs.io/guides/3.0/user-guide/installation.html#install-on-windows-or-mac">there</a> and
Ubuntu/Debian users can use directly:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>singularity-container</span>
</pre></div></div></div></blockquote>
</li>
<li><p>If you <strong>can’t install singularity</strong> on your laptop and you <strong>don’t need
apt-get</strong>, you can reserve a <strong>cpu node on the Mila cluster</strong> to build your
container.</p></li>
</ul>
<p>In this case, in order to avoid too much I/O over the network, you should define
the singularity cache locally:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">export</span><span class="w"> </span><span class="nv">SINGULARITY_CACHEDIR</span><span class="o">=</span><span class="nv">$SLURM_TMPDIR</span></span>
</pre></div></div></div></blockquote>
<ul class="simple">
<li><p>If you <strong>can’t install singularity</strong> on your laptop and you <strong>want to use
apt-get</strong>, you can use <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> to build your containers and read
<a class="reference internal" href="#recipe-section">Recipe_section</a>.</p></li>
</ul>
<section id="download-containers-from-the-web">
<h5>Download containers from the web<a class="headerlink" href="#download-containers-from-the-web" title="Link to this heading"></a></h5>
<p>Hopefully, you may not need to create containers from scratch as many have been
already built for the most common deep learning software. You can find most of
them on <a class="reference external" href="https://hub.docker.com/">dockerhub</a>.</p>
<p>Go on <a class="reference external" href="https://hub.docker.com/">dockerhub</a> and select the container you want to pull.</p>
<p>For example, if you want to get the latest PyTorch version with GPU support
(Replace <em>runtime</em> by <em>devel</em> if you need the full Cuda toolkit):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span>pull<span class="w"> </span>docker://pytorch/pytorch:1.0.1-cuda10.0-cudnn7-runtime</span>
</pre></div></div><p>Or the latest TensorFlow:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span>pull<span class="w"> </span>docker://tensorflow/tensorflow:latest-gpu-py3</span>
</pre></div></div><p>Currently the pulled image <code class="docutils literal notranslate"><span class="pre">pytorch.simg</span></code> or <code class="docutils literal notranslate"><span class="pre">tensorflow.simg</span></code> is read-only
meaning that you won’t be able to install anything on it.  Starting now, PyTorch
will be taken as example. If you use TensorFlow, simply replace every
<strong>pytorch</strong> occurrences by <strong>tensorflow</strong>.</p>
</section>
<section id="how-to-add-or-install-stuff-in-a-container">
<h5>How to add or install stuff in a container<a class="headerlink" href="#how-to-add-or-install-stuff-in-a-container" title="Link to this heading"></a></h5>
<p>The first step is to transform your read only container
<code class="docutils literal notranslate"><span class="pre">pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</span></code> in a writable version that will
allow you to add packages.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Depending on the version of singularity you are using, singularity
will build a container with the extension .simg or .sif. If you’re using
.sif files, replace every occurences of .simg by .sif.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If you want to use <strong>apt-get</strong> you have to put <strong>sudo</strong> ahead of the
following commands</p>
</div>
<p>This command will create a writable image in the folder <code class="docutils literal notranslate"><span class="pre">pytorch</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span>build<span class="w"> </span>--sandbox<span class="w"> </span>pytorch<span class="w"> </span>pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</span>
</pre></div></div><p>Then you’ll need the following command to log inside the container.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span>shell<span class="w"> </span>--writable<span class="w"> </span>-H<span class="w"> </span><span class="nv">$HOME</span>:/home<span class="w"> </span>pytorch</span>
</pre></div></div><p>Once you get into the container, you can use pip and install anything you need
(Or with <code class="docutils literal notranslate"><span class="pre">apt-get</span></code> if you built the container with sudo).</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Singularity mounts your home folder, so if you install things into
the <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> of your container, they will be installed in your real
<code class="docutils literal notranslate"><span class="pre">$HOME</span></code>!</p>
</div>
<p>You should install your stuff in /usr/local instead.</p>
</section>
<section id="creating-useful-directories">
<h5>Creating useful directories<a class="headerlink" href="#creating-useful-directories" title="Link to this heading"></a></h5>
<p>One of the benefits of containers is that you’ll be able to use them across
different clusters. However for each cluster the datasets and experiments
folder location can be different. In order to be invariant to those locations,
we will create some useful mount points inside the container:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt4:before {
  content: "<Singularity_container>$ ";
}
</style><span class="prompt4">mkdir<span class="w"> </span>/dataset</span>
<span class="prompt4">mkdir<span class="w"> </span>/tmp_log</span>
<span class="prompt4">mkdir<span class="w"> </span>/final_log</span>
</pre></div></div><p>From now, you won’t need to worry anymore when you write your code to specify
where to pick up your dataset. Your dataset will always be in <code class="docutils literal notranslate"><span class="pre">/dataset</span></code>
independently of the cluster you are using.</p>
</section>
<section id="testing">
<h5>Testing<a class="headerlink" href="#testing" title="Link to this heading"></a></h5>
<p>If you have some code that you want to test before finalizing your container,
you have two choices.  You can either log into your container and run Python
code inside it with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span>shell<span class="w"> </span>--nv<span class="w"> </span>pytorch</span>
</pre></div></div><p>Or you can execute your command directly with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>pytorch<span class="w"> </span>Python<span class="w"> </span>YOUR_CODE.py</span>
</pre></div></div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>—nv allows the container to use gpus. You don’t need this if you
don’t plan to use a gpu.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Don’t forget to clear the cache of the packages you installed in
the containers.</p>
</div>
</section>
<section id="creating-a-new-image-from-the-sandbox">
<h5>Creating a new image from the sandbox<a class="headerlink" href="#creating-a-new-image-from-the-sandbox" title="Link to this heading"></a></h5>
<p>Once everything you need is installed inside the container, you need to convert
it back to a read-only singularity image with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span>build<span class="w"> </span>pytorch_final.simg<span class="w"> </span>pytorch</span>
</pre></div></div></section>
</section>
<section id="second-way-use-recipes">
<span id="recipe-section"></span><h4>Second way: Use recipes<a class="headerlink" href="#second-way-use-recipes" title="Link to this heading"></a></h4>
<p>A singularity recipe is a file including specifics about installation software,
environment variables, files to add, and container metadata.  It is a starting
point for designing any custom container. Instead of pulling a container and
installing your packages manually, you can specify in this file the packages
you want and then build your container from this file.</p>
<p>Here is a toy example of a singularity recipe installing some stuff:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">################# Header: Define the base system you want to use ################</span>
<span class="c1"># Reference of the kind of base you want to use (e.g., docker, debootstrap, shub).</span>
Bootstrap:<span class="w"> </span>docker
<span class="c1"># Select the docker image you want to use (Here we choose tensorflow)</span>
From:<span class="w"> </span>tensorflow/tensorflow:latest-gpu-py3

<span class="c1">################# Section: Defining the system #################################</span>
<span class="c1"># Commands in the %post section are executed within the container.</span>
%post
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing Tools with apt-get&quot;</span>
<span class="w">        </span>apt-get<span class="w"> </span>update
<span class="w">        </span>apt-get<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>cmake<span class="w"> </span>libcupti-dev<span class="w"> </span>libyaml-dev<span class="w"> </span>wget<span class="w"> </span>unzip
<span class="w">        </span>apt-get<span class="w"> </span>clean
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Installing things with pip&quot;</span>
<span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>tqdm
<span class="w">        </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Creating mount points&quot;</span>
<span class="w">        </span>mkdir<span class="w"> </span>/dataset
<span class="w">        </span>mkdir<span class="w"> </span>/tmp_log
<span class="w">        </span>mkdir<span class="w"> </span>/final_log


<span class="c1"># Environment variables that should be sourced at runtime.</span>
%environment
<span class="w">        </span><span class="c1"># use bash as default shell</span>
<span class="w">        </span><span class="nv">SHELL</span><span class="o">=</span>/bin/bash
<span class="w">        </span><span class="nb">export</span><span class="w"> </span>SHELL
</pre></div>
</div>
<p>A recipe file contains two parts: the <code class="docutils literal notranslate"><span class="pre">header</span></code> and <code class="docutils literal notranslate"><span class="pre">sections</span></code>. In the
<code class="docutils literal notranslate"><span class="pre">header</span></code> you specify which base system you want to use, it can be any docker
or singularity container. In <code class="docutils literal notranslate"><span class="pre">sections</span></code>, you can list the things you want to
install in the subsection <code class="docutils literal notranslate"><span class="pre">post</span></code> or list the environment’s variable you need
to source at each runtime in the subsection <code class="docutils literal notranslate"><span class="pre">environment</span></code>. For a more detailed
description, please look at the <a class="reference external" href="https://www.sylabs.io/guides/2.6/user-guide/container_recipes.html#container-recipes">singularity documentation</a>.</p>
<p>In order to build a singularity container from a singularity recipe file, you
should use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">sudo<span class="w"> </span>singularity<span class="w"> </span>build<span class="w"> </span>&lt;NAME_CONTAINER&gt;<span class="w"> </span>&lt;YOUR_RECIPE_FILES&gt;</span>
</pre></div></div><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You always need to use sudo when you build a container from a
recipe. As there is no access to sudo on the cluster, a personal computer or
the use singularity hub is needed to build a container</p>
</div>
<section id="build-recipe-on-singularity-hub">
<h5>Build recipe on singularity hub<a class="headerlink" href="#build-recipe-on-singularity-hub" title="Link to this heading"></a></h5>
<p>Singularity hub allows users to build containers from recipes directly on
singularity-hub’s cloud meaning that you don’t need to build containers by
yourself.  You need to register on <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> and link your
singularity-hub account to your GitHub account, then:</p>
<blockquote>
<div><ol class="arabic">
<li><p>Create a new github repository.</p></li>
<li><p>Add a collection on <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> and select the github repository your created.</p></li>
<li><p>Clone the github repository on your computer.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ git clone &lt;url&gt;
</pre></div>
</div>
</li>
<li><p>Write the singularity recipe and save it as a file named <strong>Singularity</strong>.</p></li>
<li><p>Git add <strong>Singularity</strong>, commit and push on the master branch</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ git add Singularity
$ git commit
$ git push origin master
</pre></div>
</div>
</li>
</ol>
</div></blockquote>
<p>At this point, robots from singularity-hub will build the container for you, you
will be able to download your container from the website or directly with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span>pull<span class="w"> </span>shub://&lt;github_username&gt;/&lt;repository_name&gt;</span>
</pre></div></div></section>
<section id="example-recipe-with-openai-gym-mujoco-and-miniworld">
<h5>Example: Recipe with OpenAI gym, MuJoCo and Miniworld<a class="headerlink" href="#example-recipe-with-openai-gym-mujoco-and-miniworld" title="Link to this heading"></a></h5>
<p>Here is an example on how you can use a singularity recipe to install complex
environment such as OpenAI gym, MuJoCo and Miniworld on a PyTorch based
container. In order to use MuJoCo, you’ll need to copy the key stored on the
Mila cluster in <cite>/ai/apps/mujoco/license/mjkey.txt</cite> to your current directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#This is a dockerfile that sets up a full Gym install with test dependencies</span>
Bootstrap:<span class="w"> </span>docker

<span class="c1"># Here we ll build our container upon the pytorch container</span>
From:<span class="w"> </span>pytorch/pytorch:1.0-cuda10.0-cudnn7-runtime

<span class="c1"># Now we&#39;ll copy the mjkey file located in the current directory inside the container&#39;s root</span>
<span class="c1"># directory</span>
%files
<span class="w">        </span>mjkey.txt

<span class="c1"># Then we put everything we need to install</span>
%post
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:/opt/conda/bin
<span class="w">        </span>apt<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>keyboard-configuration<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python3-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python-pyglet<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python3-opengl<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libhdf5-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libjpeg-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libboost-all-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libsdl2-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libosmesa6-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>patchelf<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>ffmpeg<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>xvfb<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libhdf5-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>openjdk-8-jdk<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>wget<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>git<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>unzip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>apt<span class="w"> </span>clean<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>rm<span class="w"> </span>-rf<span class="w"> </span>/var/lib/apt/lists/*
<span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>h5py

<span class="w">        </span><span class="c1"># Download Gym and MuJoCo</span>
<span class="w">        </span>mkdir<span class="w"> </span>/Gym<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/Gym
<span class="w">        </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/openai/gym.git<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>mkdir<span class="w"> </span>/Gym/.mujoco<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/Gym/.mujoco
<span class="w">        </span>wget<span class="w"> </span>https://www.roboti.us/download/mjpro150_linux.zip<span class="w">  </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>unzip<span class="w"> </span>mjpro150_linux.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>wget<span class="w"> </span>https://www.roboti.us/download/mujoco200_linux.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>unzip<span class="w"> </span>mujoco200_linux.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>mv<span class="w"> </span>mujoco200_linux<span class="w"> </span>mujoco200

<span class="w">        </span><span class="c1"># Export global environment variables</span>
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">MUJOCO_PY_MJKEY_PATH</span><span class="o">=</span>/Gym/.mujoco/mjkey.txt
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">MUJOCO_PY_MUJOCO_PATH</span><span class="o">=</span>/Gym/.mujoco/mujoco150/
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mjpro150/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mujoco200/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/bin
<span class="w">        </span>cp<span class="w"> </span>/mjkey.txt<span class="w"> </span>/Gym/.mujoco/mjkey.txt
<span class="w">        </span><span class="c1"># Install Python dependencies</span>
<span class="w">        </span>wget<span class="w"> </span>https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt
<span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
<span class="w">        </span><span class="c1"># Install Gym and MuJoCo</span>
<span class="w">        </span><span class="nb">cd</span><span class="w"> </span>/Gym/gym
<span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span><span class="s1">&#39;.[all]&#39;</span>
<span class="w">        </span><span class="c1"># Change permission to use mujoco_py as non sudoer user</span>
<span class="w">        </span>chmod<span class="w"> </span>-R<span class="w"> </span><span class="m">777</span><span class="w"> </span>/opt/conda/lib/python3.6/site-packages/mujoco_py/
<span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>minerl

<span class="c1"># Export global environment variables</span>
%environment
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">SHELL</span><span class="o">=</span>/bin/sh
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">MUJOCO_PY_MJKEY_PATH</span><span class="o">=</span>/Gym/.mujoco/mjkey.txt
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">MUJOCO_PY_MUJOCO_PATH</span><span class="o">=</span>/Gym/.mujoco/mujoco150/
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mjpro150/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mujoco200/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>/Gym/gym/.tox/py3/bin:<span class="nv">$PATH</span>

%runscript
<span class="w">        </span><span class="nb">exec</span><span class="w"> </span>/bin/sh<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Here is the same recipe but written for TensorFlow:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#This is a dockerfile that sets up a full Gym install with test dependencies</span>
Bootstrap:<span class="w"> </span>docker

<span class="c1"># Here we ll build our container upon the tensorflow container</span>
From:<span class="w"> </span>tensorflow/tensorflow:latest-gpu-py3

<span class="c1"># Now we&#39;ll copy the mjkey file located in the current directory inside the container&#39;s root</span>
<span class="c1"># directory</span>
%files
<span class="w">        </span>mjkey.txt

<span class="c1"># Then we put everything we need to install</span>
%post
<span class="w">        </span>apt<span class="w"> </span>-y<span class="w"> </span>update<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>keyboard-configuration<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>apt<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python3-setuptools<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python3-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python-pyglet<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python3-opengl<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libjpeg-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libboost-all-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libsdl2-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>libosmesa6-dev<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>patchelf<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>ffmpeg<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>xvfb<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>wget<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>git<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>unzip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>apt<span class="w"> </span>clean<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>rm<span class="w"> </span>-rf<span class="w"> </span>/var/lib/apt/lists/*

<span class="w">        </span><span class="c1"># Download Gym and MuJoCo</span>
<span class="w">        </span>mkdir<span class="w"> </span>/Gym<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/Gym
<span class="w">        </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/openai/gym.git<span class="w"> </span><span class="o">||</span><span class="w"> </span><span class="nb">true</span><span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>mkdir<span class="w"> </span>/Gym/.mujoco<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">cd</span><span class="w"> </span>/Gym/.mujoco
<span class="w">        </span>wget<span class="w"> </span>https://www.roboti.us/download/mjpro150_linux.zip<span class="w">  </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>unzip<span class="w"> </span>mjpro150_linux.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>wget<span class="w"> </span>https://www.roboti.us/download/mujoco200_linux.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>unzip<span class="w"> </span>mujoco200_linux.zip<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>mv<span class="w"> </span>mujoco200_linux<span class="w"> </span>mujoco200

<span class="w">        </span><span class="c1"># Export global environment variables</span>
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">MUJOCO_PY_MJKEY_PATH</span><span class="o">=</span>/Gym/.mujoco/mjkey.txt
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">MUJOCO_PY_MUJOCO_PATH</span><span class="o">=</span>/Gym/.mujoco/mujoco150/
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mjpro150/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mujoco200/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/bin
<span class="w">        </span>cp<span class="w"> </span>/mjkey.txt<span class="w"> </span>/Gym/.mujoco/mjkey.txt

<span class="w">        </span><span class="c1"># Install Python dependencies</span>
<span class="w">        </span>wget<span class="w"> </span>https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt
<span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
<span class="w">        </span><span class="c1"># Install Gym and MuJoCo</span>
<span class="w">        </span><span class="nb">cd</span><span class="w"> </span>/Gym/gym
<span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span><span class="s1">&#39;.[all]&#39;</span>
<span class="w">        </span><span class="c1"># Change permission to use mujoco_py as non sudoer user</span>
<span class="w">        </span>chmod<span class="w"> </span>-R<span class="w"> </span><span class="m">777</span><span class="w"> </span>/usr/local/lib/python3.5/dist-packages/mujoco_py/

<span class="w">        </span><span class="c1"># Then install miniworld</span>
<span class="w">        </span><span class="nb">cd</span><span class="w"> </span>/usr/local/
<span class="w">        </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/maximecb/gym-miniworld.git
<span class="w">        </span><span class="nb">cd</span><span class="w"> </span>gym-miniworld
<span class="w">        </span>pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.

<span class="c1"># Export global environment variables</span>
%environment
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">SHELL</span><span class="o">=</span>/bin/bash
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">MUJOCO_PY_MJKEY_PATH</span><span class="o">=</span>/Gym/.mujoco/mjkey.txt
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">MUJOCO_PY_MUJOCO_PATH</span><span class="o">=</span>/Gym/.mujoco/mujoco150/
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mjpro150/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/Gym/.mujoco/mujoco200/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:/usr/local/bin
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">PATH</span><span class="o">=</span>/Gym/gym/.tox/py3/bin:<span class="nv">$PATH</span>

%runscript
<span class="w">        </span><span class="nb">exec</span><span class="w"> </span>/bin/bash<span class="w"> </span><span class="s2">&quot;</span><span class="nv">$@</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Keep in mind that those environment variables are sourced at runtime and not at
build time. This is why, you should also define them in the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section
since they are required to install MuJoCo.</p>
</section>
</section>
</section>
<section id="using-containers-on-clusters">
<span id="id11"></span><h3>Using containers on clusters<a class="headerlink" href="#using-containers-on-clusters" title="Link to this heading"></a></h3>
<section id="how-to-use-containers-on-clusters">
<h4>How to use containers on clusters<a class="headerlink" href="#how-to-use-containers-on-clusters" title="Link to this heading"></a></h4>
<p>On every cluster with Slurm, datasets and intermediate results should go in
<code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> while the final experiment results should go in <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code>.
In order to use the container you built, you need to copy it on the cluster you
want to use.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You should always store your container in $SCRATCH !</p>
</div>
<p>Then reserve a node with srun/sbatch, copy the container and your dataset on the
node given by SLURM (i.e in <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code>) and execute the code
<code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CODE&gt;</span></code> within the container <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CONTAINER&gt;</span></code> with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>-H<span class="w"> </span><span class="nv">$HOME</span>:/home<span class="w"> </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/dataset/<span class="w"> </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/tmp_log/<span class="w"> </span>-B<span class="w"> </span><span class="nv">$SCRATCH</span>:/final_log/<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt;<span class="w"> </span>python<span class="w"> </span>&lt;YOUR_CODE&gt;</span>
</pre></div></div><p>Remember that <code class="docutils literal notranslate"><span class="pre">/dataset</span></code>, <code class="docutils literal notranslate"><span class="pre">/tmp_log</span></code> and <code class="docutils literal notranslate"><span class="pre">/final_log</span></code> were created in the
previous section. Now each time, we’ll use singularity, we are explicitly
telling it to mount <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> on the cluster’s node in the folder
<code class="docutils literal notranslate"><span class="pre">/dataset</span></code> inside the container with the option <code class="docutils literal notranslate"><span class="pre">-B</span></code> such that each dataset
downloaded by PyTorch in <code class="docutils literal notranslate"><span class="pre">/dataset</span></code> will be available in <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code>.</p>
<p>This will allow us to have code and scripts that are invariant to the cluster
environment. The option <code class="docutils literal notranslate"><span class="pre">-H</span></code> specify what will be the container’s home. For
example, if you have your code in <code class="docutils literal notranslate"><span class="pre">$HOME/Project12345/Version35/</span></code> you can
specify <code class="docutils literal notranslate"><span class="pre">-H</span> <span class="pre">$HOME/Project12345/Version35:/home</span></code>, thus the container will only
have access to the code inside <code class="docutils literal notranslate"><span class="pre">Version35</span></code>.</p>
<p>If you want to run multiple commands inside the container you can use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span>-H<span class="w"> </span><span class="nv">$HOME</span>:/home<span class="w"> </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/dataset/<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/tmp_log/<span class="w"> </span>-B<span class="w"> </span><span class="nv">$SCRATCH</span>:/final_log/<span class="w"> </span><span class="se">\</span>
<span class="w">   </span><span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt;<span class="w"> </span>bash<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;pwd &amp;&amp; ls &amp;&amp; python &lt;YOUR_CODE&gt;&#39;</span></span>
</pre></div></div><section id="example-interactive-case-srun-salloc">
<h5>Example: Interactive case (srun/salloc)<a class="headerlink" href="#example-interactive-case-srun-salloc" title="Link to this heading"></a></h5>
<p>Once you get an interactive session with SLURM, copy <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CONTAINER&gt;</span></code> and
<code class="docutils literal notranslate"><span class="pre">&lt;YOUR_DATASET&gt;</span></code> to <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt5:before {
  content: "# ";
}
</style><span class="prompt5"><span class="m">0</span>.<span class="w"> </span>Get<span class="w"> </span>an<span class="w"> </span>interactive<span class="w"> </span>session</span>
<span class="prompt1">srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1</span>
<span class="prompt5"><span class="m">1</span>.<span class="w"> </span>Copy<span class="w"> </span>your<span class="w"> </span>container<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>compute<span class="w"> </span>node</span>
<span class="prompt1">rsync<span class="w"> </span>-avz<span class="w"> </span><span class="nv">$SCRATCH</span>/&lt;YOUR_CONTAINER&gt;<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span></span>
<span class="prompt5"><span class="m">2</span>.<span class="w"> </span>Copy<span class="w"> </span>your<span class="w"> </span>dataset<span class="w"> </span>on<span class="w"> </span>the<span class="w"> </span>compute<span class="w"> </span>node</span>
<span class="prompt1">rsync<span class="w"> </span>-avz<span class="w"> </span><span class="nv">$SCRATCH</span>/&lt;YOUR_DATASET&gt;<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span></span>
</pre></div></div><p>Then use <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">shell</span></code> to get a shell inside the container</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt5"><span class="m">3</span>.<span class="w"> </span>Get<span class="w"> </span>a<span class="w"> </span>shell<span class="w"> </span><span class="k">in</span><span class="w"> </span>your<span class="w"> </span>environment</span>
<span class="prompt1">singularity<span class="w"> </span>shell<span class="w"> </span>--nv<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-H<span class="w"> </span><span class="nv">$HOME</span>:/home<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/dataset/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/tmp_log/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SCRATCH</span>:/final_log/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span><span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt;</span>
</pre></div></div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt5"><span class="m">4</span>.<span class="w"> </span>Execute<span class="w"> </span>your<span class="w"> </span>code</span>
<span class="prompt4">python<span class="w"> </span>&lt;YOUR_CODE&gt;</span>
</pre></div></div><p><strong>or</strong> use <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">exec</span></code> to execute <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CODE&gt;</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt5"><span class="m">3</span>.<span class="w"> </span>Execute<span class="w"> </span>your<span class="w"> </span>code</span>
<span class="prompt1">singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-H<span class="w"> </span><span class="nv">$HOME</span>:/home<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/dataset/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/tmp_log/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SCRATCH</span>:/final_log/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span><span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python<span class="w"> </span>&lt;YOUR_CODE&gt;</span>
</pre></div></div><p>You can create also the following alias to make your life easier.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">alias</span><span class="w"> </span><span class="nv">my_env</span><span class="o">=</span><span class="s1">&#39;singularity exec --nv \</span>
<span class="s1">        -H $HOME:/home \</span>
<span class="s1">        -B $SLURM_TMPDIR:/dataset/ \</span>
<span class="s1">        -B $SLURM_TMPDIR:/tmp_log/ \</span>
<span class="s1">        -B $SCRATCH:/final_log/ \</span>
<span class="s1">        $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;&#39;</span></span>
</pre></div></div><p>This will allow you to run any code with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">my_env<span class="w"> </span>python<span class="w"> </span>&lt;YOUR_CODE&gt;</span>
</pre></div></div></section>
<section id="example-sbatch-case">
<h5>Example: sbatch case<a class="headerlink" href="#example-sbatch-case" title="Link to this heading"></a></h5>
<p>You can also create a <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>:linenos:

<span class="c1">#!/bin/bash</span>
<span class="c1">#SBATCH --cpus-per-task=6         # Ask for 6 CPUs</span>
<span class="c1">#SBATCH --gres=gpu:1              # Ask for 1 GPU</span>
<span class="c1">#SBATCH --mem=10G                 # Ask for 10 GB of RAM</span>
<span class="c1">#SBATCH --time=0:10:00            # The job will run for 10 minutes</span>

<span class="c1"># 1. Copy your container on the compute node</span>
rsync<span class="w"> </span>-avz<span class="w"> </span><span class="nv">$SCRATCH</span>/&lt;YOUR_CONTAINER&gt;<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>
<span class="c1"># 2. Copy your dataset on the compute node</span>
rsync<span class="w"> </span>-avz<span class="w"> </span><span class="nv">$SCRATCH</span>/&lt;YOUR_DATASET&gt;<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>
<span class="c1"># 3. Executing your code with singularity</span>
singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-H<span class="w"> </span><span class="nv">$HOME</span>:/home<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/dataset/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/tmp_log/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>-B<span class="w"> </span><span class="nv">$SCRATCH</span>:/final_log/<span class="w"> </span><span class="se">\</span>
<span class="w">        </span><span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>python<span class="w"> </span><span class="s2">&quot;&lt;YOUR_CODE&gt;&quot;</span>
<span class="c1"># 4. Copy whatever you want to save on $SCRATCH</span>
rsync<span class="w"> </span>-avz<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/&lt;to_save&gt;<span class="w"> </span><span class="nv">$SCRATCH</span>
</pre></div>
</div>
</section>
<section id="issue-with-pybullet-and-opengl-libraries">
<h5>Issue with PyBullet and OpenGL libraries<a class="headerlink" href="#issue-with-pybullet-and-opengl-libraries" title="Link to this heading"></a></h5>
<p>If you are running certain gym environments that require <code class="docutils literal notranslate"><span class="pre">pyglet</span></code>, you may
encounter a problem when running your singularity instance with the Nvidia
drivers using the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag. This happens because the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag also
provides the OpenGL libraries:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>libGL.so.1<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/.singularity.d/libs/libGL.so.1
libGLX.so.0<span class="w"> </span><span class="o">=</span>&gt;<span class="w"> </span>/.singularity.d/libs/libGLX.so.0
</pre></div>
</div>
<p>If you don’t experience those problems with <code class="docutils literal notranslate"><span class="pre">pyglet</span></code>, you probably don’t need
to address this. Otherwise, you can resolve those problems by <code class="docutils literal notranslate"><span class="pre">apt-get</span> <span class="pre">install</span>
<span class="pre">-y</span> <span class="pre">libosmesa6-dev</span> <span class="pre">mesa-utils</span> <span class="pre">mesa-utils-extra</span> <span class="pre">libgl1-mesa-glx</span></code>, and then making
sure that your <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> points to those libraries before the ones in
<code class="docutils literal notranslate"><span class="pre">/.singularity.d/libs</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>%environment
<span class="w">        </span><span class="c1"># ...</span>
<span class="w">        </span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/usr/lib/x86_64-linux-gnu/mesa:<span class="nv">$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</section>
<section id="mila-cluster">
<h5>Mila cluster<a class="headerlink" href="#mila-cluster" title="Link to this heading"></a></h5>
<p>On the Mila cluster <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> is not yet defined, you should add the
experiment results you want to keep in <code class="docutils literal notranslate"><span class="pre">/network/scratch/&lt;u&gt;/&lt;username&gt;/</span></code>. In
order to use the sbatch script above and to match other cluster environment’s
names, you can define <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> as an alias for
<code class="docutils literal notranslate"><span class="pre">/network/scratch/&lt;u&gt;/&lt;username&gt;</span></code> with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1"><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;export SCRATCH=/network/scratch/</span><span class="si">${</span><span class="nv">USER</span><span class="p">:</span><span class="nv">0</span><span class="p">:</span><span class="nv">1</span><span class="si">}</span><span class="s2">/</span><span class="nv">$USER</span><span class="s2">&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>~/.bashrc</span>
</pre></div></div><p>Then, you can follow the general procedure explained above.</p>
</section>
<section id="digital-research-alliance-of-canada">
<h5>Digital Research Alliance of Canada<a class="headerlink" href="#digital-research-alliance-of-canada" title="Link to this heading"></a></h5>
<p>Using singularity on Digital Research Alliance of Canada is similar except that
you need to add Yoshua’s account name and load singularity. Here is an example
of a <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> script using singularity on compute Canada cluster:</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You should use singularity/2.6 or singularity/3.4. There is a bug
in singularity/3.2 which makes gpu unusable.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH --account=rpp-bengioy     # Yoshua pays for your job</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --cpus-per-task=6         # Ask for 6 CPUs</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --gres=gpu:1              # Ask for 1 GPU</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --mem=32G                 # Ask for 32 GB of RAM</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --time=0:10:00            # The job will run for 10 minutes</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH --output=&quot;/scratch/&lt;user&gt;/slurm-%j.out&quot; # Modify the output of sbatch</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1"># 1. You have to load singularity</span>
<span class="linenos">10</span>module<span class="w"> </span>load<span class="w"> </span>singularity
<span class="linenos">11</span><span class="c1"># 2. Then you copy the container to the local disk</span>
<span class="linenos">12</span>rsync<span class="w"> </span>-avz<span class="w"> </span><span class="nv">$SCRATCH</span>/&lt;YOUR_CONTAINER&gt;<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>
<span class="linenos">13</span><span class="c1"># 3. Copy your dataset on the compute node</span>
<span class="linenos">14</span>rsync<span class="w"> </span>-avz<span class="w"> </span><span class="nv">$SCRATCH</span>/&lt;YOUR_DATASET&gt;<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>
<span class="linenos">15</span><span class="c1"># 4. Executing your code with singularity</span>
<span class="linenos">16</span>singularity<span class="w"> </span><span class="nb">exec</span><span class="w"> </span>--nv<span class="w"> </span><span class="se">\</span>
<span class="linenos">17</span><span class="w">        </span>-H<span class="w"> </span><span class="nv">$HOME</span>:/home<span class="w"> </span><span class="se">\</span>
<span class="linenos">18</span><span class="w">        </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/dataset/<span class="w"> </span><span class="se">\</span>
<span class="linenos">19</span><span class="w">        </span>-B<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>:/tmp_log/<span class="w"> </span><span class="se">\</span>
<span class="linenos">20</span><span class="w">        </span>-B<span class="w"> </span><span class="nv">$SCRATCH</span>:/final_log/<span class="w"> </span><span class="se">\</span>
<span class="linenos">21</span><span class="w">        </span><span class="nv">$SLURM_TMPDIR</span>/&lt;YOUR_CONTAINER&gt;<span class="w"> </span><span class="se">\</span>
<span class="linenos">22</span><span class="w">        </span>python<span class="w"> </span><span class="s2">&quot;&lt;YOUR_CODE&gt;&quot;</span>
<span class="linenos">23</span><span class="c1"># 5. Copy whatever you want to save on $SCRATCH</span>
<span class="linenos">24</span>rsync<span class="w"> </span>-avz<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span>/&lt;to_save&gt;<span class="w"> </span><span class="nv">$SCRATCH</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>
<section id="sharing-data-with-acls">
<h2>Sharing Data with ACLs<a class="headerlink" href="#sharing-data-with-acls" title="Link to this heading"></a></h2>
<p>Regular permissions bits are extremely blunt tools: They control access through
only three sets of bits owning user, owning group and all others. Therefore,
access is either too narrow (<code class="docutils literal notranslate"><span class="pre">0700</span></code> allows access only by oneself) or too wide
(<code class="docutils literal notranslate"><span class="pre">770</span></code> gives all permissions to everyone in the same group, and <code class="docutils literal notranslate"><span class="pre">777</span></code> to
literally everyone).</p>
<p>ACLs (Access Control Lists) are an expansion of the permissions bits that allow
more fine-grained, granular control of accesses to a file. They can be used to
permit specific users access to files and folders even if conservative default
permissions would have denied them such access.</p>
<p>As an illustrative example, to use ACLs to allow <code class="docutils literal notranslate"><span class="pre">$USER</span></code> (<strong>oneself</strong>) to
share with <code class="docutils literal notranslate"><span class="pre">$USER2</span></code> (<strong>another person</strong>) a “playground” folder hierarchy in
Mila’s scratch filesystem at a location</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">$SCRATCH/X/Y/Z/...</span></code></p>
</div></blockquote>
<p>in a safe and secure fashion that allows both users to read, write, execute,
search and delete each others’ files:</p>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>1.</strong> Grant <strong>oneself</strong> permissions to access any <strong>future</strong> files/folders created
by the other <em>(or oneself)</em></div>
<div class="line">(<code class="docutils literal notranslate"><span class="pre">-d</span></code> renders this permission a “default” / inheritable one)</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>setfacl<span class="w"> </span>-Rdm<span class="w"> </span>user:<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>:rwx<span class="w">  </span><span class="nv">$SCRATCH</span>/X/Y/Z/
</pre></div>
</div>
<hr class="docutils" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The importance of doing this seemingly-redundant step first is that files
and folders are <strong>always</strong> owned by only one person, almost always their
creator (the UID will be the creator’s, the GID typically as well). If that
user is not yourself, you will not have access to those files unless the
other person specifically gives them to you – or these files inherited a
default ACL allowing you full access.</p>
<p><strong>This</strong> is the inherited, default ACL serving that purpose.</p>
</div>
<div class="line-block">
<div class="line"><strong>2.</strong> Grant <strong>the other</strong> permission to access any <strong>future</strong> files/folders created
by the other <em>(or oneself)</em></div>
<div class="line">(<code class="docutils literal notranslate"><span class="pre">-d</span></code> renders this permission a “default” / inheritable one)</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>setfacl<span class="w"> </span>-Rdm<span class="w"> </span>user:<span class="si">${</span><span class="nv">USER2</span><span class="p">:?defineme</span><span class="si">}</span>:rwx<span class="w"> </span><span class="nv">$SCRATCH</span>/X/Y/Z/
</pre></div>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>3.</strong> Grant <strong>the other</strong> permission to access any <strong>existing</strong> files/folders created
by <em>oneself</em>.</div>
<div class="line">Such files and folders were created before the new default ACLs were added
above and thus did not inherit them from their parent folder at the moment of
their creation.</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>setfacl<span class="w"> </span>-Rm<span class="w">  </span>user:<span class="si">${</span><span class="nv">USER2</span><span class="p">:?defineme</span><span class="si">}</span>:rwx<span class="w"> </span><span class="nv">$SCRATCH</span>/X/Y/Z/
</pre></div>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>4.</strong> Grant <strong>another</strong> permission to search through one’s hierarchy down to the
shared location in question.</div>
</div>
<ul class="simple">
<li><p><strong>Non</strong>-recursive (!!!!)</p></li>
<li><p>May also grant <code class="docutils literal notranslate"><span class="pre">:rx</span></code> in unlikely event others listing your folders on the
path is not troublesome or desirable.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>setfacl<span class="w"> </span>-m<span class="w">   </span>user:<span class="si">${</span><span class="nv">USER2</span><span class="p">:?defineme</span><span class="si">}</span>:x<span class="w">   </span><span class="nv">$SCRATCH</span>/X/Y/
setfacl<span class="w"> </span>-m<span class="w">   </span>user:<span class="si">${</span><span class="nv">USER2</span><span class="p">:?defineme</span><span class="si">}</span>:x<span class="w">   </span><span class="nv">$SCRATCH</span>/X/
setfacl<span class="w"> </span>-m<span class="w">   </span>user:<span class="si">${</span><span class="nv">USER2</span><span class="p">:?defineme</span><span class="si">}</span>:x<span class="w">   </span><span class="nv">$SCRATCH</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The purpose of granting permissions first for <em>future</em> files and then for
<em>existing</em> files is to prevent a <strong>race condition</strong> whereby after the first
<code class="docutils literal notranslate"><span class="pre">setfacl</span></code> command the other person could create files to which the
second <code class="docutils literal notranslate"><span class="pre">setfacl</span></code> command does not apply.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to access a file, all folders from the root (<code class="docutils literal notranslate"><span class="pre">/</span></code>) down to the
parent folder in question must be searchable (<code class="docutils literal notranslate"><span class="pre">+x</span></code>) by the concerned user.
This is already the case for all users for folders such as <code class="docutils literal notranslate"><span class="pre">/</span></code>,
<code class="docutils literal notranslate"><span class="pre">/network</span></code> and <code class="docutils literal notranslate"><span class="pre">/network/scratch</span></code>, but users must explicitly grant access
to some or all users either through base permissions or by adding ACLs, for
at least <code class="docutils literal notranslate"><span class="pre">/network/scratch/${USER:0:1}/$USER</span></code> (= <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code>), <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> and subfolders.</p>
<p>To bluntly allow <strong>all</strong> users to search through a folder (<strong>think twice!</strong>),
the following command can be used:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>chmod<span class="w"> </span>a+X<span class="w"> </span><span class="nv">$SCRATCH</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information on <code class="docutils literal notranslate"><span class="pre">setfacl</span></code> and path resolution/access checking,
consider the following documentation viewing commands:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">setfacl</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">path_resolution</span></code></p></li>
</ul>
</div>
<section id="viewing-and-verifying-acls">
<h3>Viewing and Verifying ACLs<a class="headerlink" href="#viewing-and-verifying-acls" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>getfacl<span class="w"> </span>/path/to/folder/or/file
<span class="w">           </span><span class="m">1</span>:<span class="w">  </span><span class="c1"># file: somedir/</span>
<span class="w">           </span><span class="m">2</span>:<span class="w">  </span><span class="c1"># owner: lisa</span>
<span class="w">           </span><span class="m">3</span>:<span class="w">  </span><span class="c1"># group: staff</span>
<span class="w">           </span><span class="m">4</span>:<span class="w">  </span><span class="c1"># flags: -s-</span>
<span class="w">           </span><span class="m">5</span>:<span class="w">  </span>user::rwx
<span class="w">           </span><span class="m">6</span>:<span class="w">  </span>user:joe:rwx<span class="w">               </span><span class="c1">#effective:r-x</span>
<span class="w">           </span><span class="m">7</span>:<span class="w">  </span>group::rwx<span class="w">                 </span><span class="c1">#effective:r-x</span>
<span class="w">           </span><span class="m">8</span>:<span class="w">  </span>group:cool:r-x
<span class="w">           </span><span class="m">9</span>:<span class="w">  </span>mask::r-x
<span class="w">          </span><span class="m">10</span>:<span class="w">  </span>other::r-x
<span class="w">          </span><span class="m">11</span>:<span class="w">  </span>default:user::rwx
<span class="w">          </span><span class="m">12</span>:<span class="w">  </span>default:user:joe:rwx<span class="w">       </span><span class="c1">#effective:r-x</span>
<span class="w">          </span><span class="m">13</span>:<span class="w">  </span>default:group::r-x
<span class="w">          </span><span class="m">14</span>:<span class="w">  </span>default:mask::r-x
<span class="w">          </span><span class="m">15</span>:<span class="w">  </span>default:other::---
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">getfacl</span></code></p></li>
</ul>
</div>
</section>
</section>
<section id="contributing-datasets">
<h2>Contributing datasets<a class="headerlink" href="#contributing-datasets" title="Link to this heading"></a></h2>
<p>If a dataset could help the research of others at Mila, <a class="reference external" href="https://forms.gle/vDVwD2rZBmYHENgZA">this form</a> can be filled to request its addition
to <a class="reference external" href="Information.html#storage">/network/datasets</a>.</p>
<section id="publicly-share-a-mila-dataset">
<h3>Publicly share a Mila dataset<a class="headerlink" href="#publicly-share-a-mila-dataset" title="Link to this heading"></a></h3>
<p>Mila offers two ways to publicly share a Mila dataset:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://academictorrents.com">Academic Torrent</a></p></li>
<li><p><a class="reference external" href="https://drive.google.com/drive/folders/1peJ6VF9wQ-LeETgcdGxu1e4fo28JbtUt">Google Drive</a></p></li>
</ul>
<p>Note that these options are not mutually exclusive and both can be used.</p>
<section id="id12">
<h4>Academic Torrent<a class="headerlink" href="#id12" title="Link to this heading"></a></h4>
<p>Mila hosts/seeds some datasets created by the Mila community through <a class="reference external" href="https://academictorrents.com">Academic
Torrent</a>. The first step is to create <a class="reference external" href="https://academictorrents.com/upload.php">an
account and a torrent file</a>.</p>
<p>Then drop the dataset in <code class="docutils literal notranslate"><span class="pre">/network/scratch/.transit_datasets</span></code> and send the
Academic Torrent URL to <a class="reference external" href="https://it-support.mila.quebec">Mila’s helpdesk</a>. If
the dataset does not reside on the Mila cluster, only the Academic Torrent URL
would be needed to proceed with the initial download. Then you can delete /
stop sharing your copy.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Avoid mentioning <em>dataset</em> in the name of the dataset</p></li>
<li><p>Avoid capital letters, special charaters (including spaces) in files and
directories names. Spaces can be replaced by hyphens (<code class="docutils literal notranslate"><span class="pre">-</span></code>).</p></li>
<li><p>Multiple archives can be provided to spread the data (e.g. dataset splits,
raw data, extra data, …)</p></li>
</ul>
</div>
<section id="generate-a-torrent-file-to-be-uploaded-to-academic-torrent">
<h5>Generate a .torrent file to be uploaded to Academic Torrent<a class="headerlink" href="#generate-a-torrent-file-to-be-uploaded-to-academic-torrent" title="Link to this heading"></a></h5>
<p>The command line / Python utility <a class="reference external" href="https://github.com/idlesign/torrentool">torrentool</a> can be used to create a
<cite>DATASET_NAME.torrent</cite> file:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install torrentool</span>
python3<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>torrentool<span class="w"> </span>click
<span class="c1"># Change Directory to the location of the dataset to be hosted by Mila</span>
<span class="nb">cd</span><span class="w"> </span>/network/scratch/.transit_datasets
torrent<span class="w"> </span>create<span class="w"> </span>--tracker<span class="w"> </span>https://academictorrents.com/announce.php<span class="w"> </span>DATASET_NAME
</pre></div>
</div>
<p>The resulting <cite>DATASET_NAME.torrent</cite> can then be used to register a new dataset
on Academic Torrent.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<ul class="simple">
<li><p>The creation of a <cite>DATASET_NAME.torrent</cite> file requires the computation of
checksums for the dataset content which can quickly become CPU-heavy. This
process should <em>not</em> be executed on a login node</p></li>
</ul>
</div>
</section>
<section id="download-a-dataset-from-academic-torrent">
<h5>Download a dataset from Academic Torrent<a class="headerlink" href="#download-a-dataset-from-academic-torrent" title="Link to this heading"></a></h5>
<p>Academic Torrent provides a <a class="reference external" href="https://github.com/academictorrents/at-python">Python API</a> to easily download a dataset
from it’s registered list:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install the Python API with:</span>
<span class="c1"># python3 -m pip install academictorrents</span>
<span class="kn">import</span> <span class="nn">academictorrents</span> <span class="k">as</span> <span class="nn">at</span>
<span class="n">mnist_path</span> <span class="o">=</span> <span class="n">at</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;323a0048d87ca79b68f12a6350a57776b6a3b7fb&quot;</span><span class="p">,</span> <span class="n">datastore</span><span class="o">=</span><span class="s2">&quot;~/scratch/.academictorrents-datastore&quot;</span><span class="p">)</span> <span class="c1"># Download the mnist dataset</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Current needs have been evaluated to be for a download speed of about 10
MB/s. This speed can be higher if more users also seeds the dataset.</p>
</div>
</section>
</section>
<section id="id14">
<h4>Google Drive<a class="headerlink" href="#id14" title="Link to this heading"></a></h4>
<p>Only a member of the staff team can upload to <a class="reference external" href="https://drive.google.com/drive/folders/1peJ6VF9wQ-LeETgcdGxu1e4fo28JbtUt">Mila’s Google Drive</a>
which requires to first drop the dataset in
<code class="docutils literal notranslate"><span class="pre">/network/scratch/.transit_datasets</span></code>. Then, contact <a class="reference external" href="https://it-support.mila.quebec">Mila’s helpdesk</a> and provide the following informations:</p>
<ul class="simple">
<li><p>directory containing the archived dataset (zip is favored) in
<code class="docutils literal notranslate"><span class="pre">/network/scratch/.transit_datasets</span></code></p></li>
<li><p>the name of the dataset</p></li>
<li><p>a licence in <code class="docutils literal notranslate"><span class="pre">.txt</span></code> format. One of the <a class="reference external" href="https://creativecommons.org/about/cclicenses/">the creative common</a> licenses can be used. It is
recommended to at least have the <em>Attribution</em> option. The <em>No Derivatives</em>
option is discouraged unless the dataset should not be modified by others.</p></li>
<li><p>MD5 checksum of the archive</p></li>
<li><p>the arXiv and GitHub URLs (those can be sent later if the article is still in
the submission process)</p></li>
<li><p>instructions to know if the dataset needs to be <code class="docutils literal notranslate"><span class="pre">unzip</span></code>ed, <code class="docutils literal notranslate"><span class="pre">untar</span></code>ed or
else before uploading to Google Drive</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Avoid mentioning <em>dataset</em> in the name of the dataset</p></li>
<li><p>Avoid capital letters, special charaters (including spaces) in files and
directories names. Spaces can be replaced by hyphens (<code class="docutils literal notranslate"><span class="pre">-</span></code>).</p></li>
<li><p>Multiple archives can be provided to spread the data (e.g. dataset splits,
raw data, extra data, …)</p></li>
</ul>
</div>
<section id="download-a-dataset-from-mila-s-google-drive-with-gdown">
<h5>Download a dataset from Mila’s Google Drive with  <code class="docutils literal notranslate"><span class="pre">gdown</span></code><a class="headerlink" href="#download-a-dataset-from-mila-s-google-drive-with-gdown" title="Link to this heading"></a></h5>
<p>The utility <a class="reference external" href="https://github.com/wkentaro/gdown">gdown</a> is a simple utility to
download data from Google Drive from the command line shell or in a Python
script and requires no setup.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>A limitation however is that it uses a shared client id which can cause a
quota block when too many users uses it in the same day. It is described in
a <a class="reference external" href="https://github.com/wkentaro/gdown/issues/43#issuecomment-642182100">GitHub issue</a>.</p>
</div>
</section>
<section id="download-a-dataset-from-mila-s-google-drive-with-rclone">
<h5>Download a dataset from Mila’s Google Drive with <code class="docutils literal notranslate"><span class="pre">rclone</span></code><a class="headerlink" href="#download-a-dataset-from-mila-s-google-drive-with-rclone" title="Link to this heading"></a></h5>
<p><a class="reference external" href="https://rclone.org/">Rclone</a> is a command line program to manage files on
cloud storage. In the context of a Google Drive remote, it allows to specify a
client id to avoid sharing with other users which avoid quota limits. Rclone
describes the creation of a <a class="reference external" href="https://rclone.org/drive/#making-your-own-client-id">client id in its documentaton</a>. Once this is done, a
remote for Mila’s Google Drive can be configured from the command line:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>rclone<span class="w"> </span>config<span class="w"> </span>create<span class="w"> </span>mila-gdrive<span class="w"> </span>drive<span class="w"> </span>client_id<span class="w"> </span>XXXXXXXXXXXX-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.apps.googleusercontent.com<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>client_secret<span class="w"> </span>XXXXXXXXXXXXX-XXXXXXXXXX<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>scope<span class="w"> </span><span class="s1">&#39;drive.readonly&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>root_folder_id<span class="w"> </span>1peJ6VF9wQ-LeETgcdGxu1e4fo28JbtUt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>config_is_local<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>config_refresh_token<span class="w"> </span><span class="nb">false</span>
</pre></div>
</div>
<p>The remote can then be used to download a dataset:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>rclone<span class="w"> </span>copy<span class="w"> </span>--progress<span class="w"> </span>mila-gdrive:DATASET_NAME/<span class="w"> </span>~/scratch/datasets/DATASET_NAME/
</pre></div>
</div>
<p>Rclone is available from the <a class="reference external" href="https://anaconda.org/conda-forge/rclone">conda channel conda-forge</a>.</p>
</section>
</section>
<section id="digital-object-identifier-doi">
<h4>Digital Object Identifier (DOI)<a class="headerlink" href="#digital-object-identifier-doi" title="Link to this heading"></a></h4>
<p>It is recommended to get a DOI to reference the dataset. A DOI is a permanent
id/URL which prevents losing references of online scientific data.
<a class="reference external" href="https://figshare.com">https://figshare.com</a> can be used to create a DOI:</p>
<ul class="simple">
<li><p>Go in <cite>My Data</cite></p></li>
<li><p>Create an item by clicking <cite>Create new item</cite></p></li>
<li><p>Check <cite>Metadata record only</cite> at the top</p></li>
<li><p>Fill the metadata fields</p></li>
</ul>
<p>Then reference the dataset using <a class="reference external" href="https://doi.org">https://doi.org</a> like this:
<a class="reference external" href="https://doi.org/10.6084/m9.figshare.2066037">https://doi.org/10.6084/m9.figshare.2066037</a></p>
</section>
</section>
</section>
<section id="data-transmission-using-globus-connect-personal">
<h2>Data Transmission using Globus Connect Personal<a class="headerlink" href="#data-transmission-using-globus-connect-personal" title="Link to this heading"></a></h2>
<p>Mila doesn’t own a Globus license but if the source or destination provides a
Globus account, like Digital Research Alliance of Canada for example, it’s
possible to setup Globus Connect Personal to create a personal endpoint on the
Mila cluster by following the Globus guide to <a class="reference external" href="https://docs.globus.org/how-to/globus-connect-personal-linux/">Install, Configure, and
Uninstall Globus Connect Personal for Linux</a>.</p>
<p>This endpoint can then be used to transfer data to and from the Mila cluster.</p>
</section>
<section id="jupyterhub">
<h2>JupyterHub<a class="headerlink" href="#jupyterhub" title="Link to this heading"></a></h2>
<p><strong>JupyterHub</strong> is a platform connected to SLURM to start a <strong>JupyterLab</strong>
session as a batch job then connects it when the allocation has been granted.
It does not require any ssh tunnel or port redirection, the hub acts as a proxy
server that will redirect you to a session as soon as it is available.</p>
<p>It is currently available for Mila clusters and some Digital Research Alliance
of Canada (Alliance) clusters.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Cluster</p></th>
<th class="head"><p>Address</p></th>
<th class="head"><p>Login type</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mila Local</p></td>
<td><p><a class="reference external" href="https://jupyterhub.server.mila.quebec">https://jupyterhub.server.mila.quebec</a></p></td>
<td><p>Google Oauth</p></td>
</tr>
<tr class="row-odd"><td><p>Alliance</p></td>
<td><p><a class="reference external" href="https://docs.alliancecan.ca/wiki/JupyterHub">https://docs.alliancecan.ca/wiki/JupyterHub</a></p></td>
<td><p>DRAC login</p></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Do not forget to close the JupyterLab session! Closing the window leaves
running the session and the SLURM job it is linked to.</p>
<p>To close it, use the <code class="docutils literal notranslate"><span class="pre">hub</span></code> menu and then <code class="docutils literal notranslate"><span class="pre">Control</span> <span class="pre">Panel</span> <span class="pre">&gt;</span> <span class="pre">Stop</span> <span class="pre">my</span> <span class="pre">server</span></code></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>For Mila Clusters:</strong></p>
<p><em>mila.quebec</em> account credentials should be used to login and start a
<strong>JupyterLab</strong> session.</p>
</div>
<section id="access-mila-storage-in-jupyterlab">
<h3>Access Mila Storage in JupyterLab<a class="headerlink" href="#access-mila-storage-in-jupyterlab" title="Link to this heading"></a></h3>
<p>Unfortunately, JupyterLab does not allow the navigation to parent directories of
<code class="docutils literal notranslate"><span class="pre">$HOME</span></code>. This makes some file systems like <code class="docutils literal notranslate"><span class="pre">/network/datasets</span></code> or
<code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> unavailable through their absolute path in the interface. It
is however possible to create symbolic links to those resources. To do so, you
can use the <code class="docutils literal notranslate"><span class="pre">ln</span> <span class="pre">-s</span></code> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ln<span class="w"> </span>-s<span class="w"> </span>/network/datasets<span class="w"> </span><span class="nv">$HOME</span>
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> is a directory that is dynamically created for each
job so you would need to recreate the symbolic link every time you start a
JupyterHub session:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ln<span class="w"> </span>-sf<span class="w"> </span><span class="nv">$SLURM_TMPDIR</span><span class="w"> </span><span class="nv">$HOME</span>
</pre></div>
</div>
</section>
</section>
<section id="advanced-slurm-usage-and-multiple-gpu-jobs">
<h2>Advanced SLURM usage and Multiple GPU jobs<a class="headerlink" href="#advanced-slurm-usage-and-multiple-gpu-jobs" title="Link to this heading"></a></h2>
<section id="handling-preemption">
<h3>Handling preemption<a class="headerlink" href="#handling-preemption" title="Link to this heading"></a></h3>
<p id="advanced-preemption">On the Mila cluster, jobs can preempt one-another depending on their priority
(unkillable&gt;high&gt;low) (See the <a class="reference external" href="https://slurm.schedmd.com/preempt.html">Slurm documentation</a>)</p>
<p>The default preemption mechanism is to kill and re-queue the job automatically
without any notice. To allow a different preemption mechanism, every partition
have been duplicated (i.e. have the same characteristics as their counterparts)
allowing a <strong>120sec</strong> grace period before killing your job <em>but don’t requeue
it automatically</em>: those partitions are referred by the suffix: <code class="docutils literal notranslate"><span class="pre">-grace</span></code>
(<code class="docutils literal notranslate"><span class="pre">main-grace,</span> <span class="pre">long-grace,</span> <span class="pre">main-cpu-grace,</span> <span class="pre">long-cpu-grace</span></code>).</p>
<p>When using a partition with a grace period, a series of signals consisting of
first <code class="docutils literal notranslate"><span class="pre">SIGCONT</span></code> and <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> then <code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code> will be sent to the SLURM
job.  It’s good practice to catch those signals using the Linux <code class="docutils literal notranslate"><span class="pre">trap</span></code> command
to properly terminate a job and save what’s necessary to restart the job.  On
each cluster, you’ll be allowed a <em>grace period</em> before SLURM actually kills
your job (<code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code>).</p>
<p>The easiest way to handle preemption is by trapping the <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> signal</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="c1">#SBATCH --ntasks=1</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH ....</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span>exit_script<span class="o">()</span><span class="w"> </span><span class="o">{</span>
<span class="linenos"> 5</span><span class="w">    </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;Preemption signal, saving myself&quot;</span>
<span class="linenos"> 6</span><span class="w">    </span><span class="nb">trap</span><span class="w"> </span>-<span class="w"> </span>SIGTERM<span class="w"> </span><span class="c1"># clear the trap</span>
<span class="linenos"> 7</span><span class="w">    </span><span class="c1"># Optional: sends SIGTERM to child/sub processes</span>
<span class="linenos"> 8</span><span class="w">    </span><span class="nb">kill</span><span class="w"> </span>--<span class="w"> </span>-<span class="nv">$$</span>
<span class="linenos"> 9</span><span class="o">}</span>
<span class="linenos">10</span>
<span class="linenos">11</span><span class="nb">trap</span><span class="w"> </span>exit_script<span class="w"> </span>SIGTERM
<span class="linenos">12</span>
<span class="linenos">13</span><span class="c1"># The main script part</span>
<span class="linenos">14</span>python3<span class="w"> </span>my_script
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line"><strong>Requeuing</strong>:</div>
<div class="line">The Slurm scheduler on the cluster does not allow a grace period before</div>
<div class="line">preempting a job while requeuing it automatically, therefore your job will</div>
<div class="line">be cancelled at the end of the grace period.</div>
<div class="line">To automatically requeue it, you can just add the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command inside</div>
<div class="line">your <code class="docutils literal notranslate"><span class="pre">exit_script</span></code> function.</div>
</div>
</div>
</section>
<section id="packing-jobs">
<h3>Packing jobs<a class="headerlink" href="#packing-jobs" title="Link to this heading"></a></h3>
<section id="sharing-a-gpu-between-processes">
<h4>Sharing a GPU between processes<a class="headerlink" href="#sharing-a-gpu-between-processes" title="Link to this heading"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">srun</span></code>, when used in a batch job is responsible for starting tasks on the
allocated resources (see srun) SLURM batch script</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="c1">#SBATCH --ntasks-per-node=2</span>
<span class="linenos">2</span><span class="c1">#SBATCH --output=myjob_output_wrapper.out</span>
<span class="linenos">3</span><span class="c1">#SBATCH --ntasks=2</span>
<span class="linenos">4</span><span class="c1">#SBATCH --gres=gpu:1</span>
<span class="linenos">5</span><span class="c1">#SBATCH --cpus-per-task=4</span>
<span class="linenos">6</span><span class="c1">#SBATCH --mem=18G</span>
<span class="linenos">7</span>srun<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>myjob_output_%t.out<span class="w"> </span>python<span class="w"> </span>script<span class="w"> </span>args
</pre></div>
</div>
<p>This will run Python 2 times, each process with 4 CPUs with the same arguments
<code class="docutils literal notranslate"><span class="pre">--output=myjob_output_%t.out</span></code> will create 2 output files appending the task
id (<code class="docutils literal notranslate"><span class="pre">%t</span></code>) to the filename and 1 global log file for things happening outside
the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command.</p>
<p>Knowing that, if you want to have 2 different arguments to the Python program,
you can use a multi-prog configuration file: <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">-l</span> <span class="pre">--multi-prog</span> <span class="pre">silly.conf</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span>  <span class="n">python</span> <span class="n">script</span> <span class="n">firstarg</span>
<span class="mi">1</span>  <span class="n">python</span> <span class="n">script</span> <span class="n">secondarg</span>
</pre></div>
</div>
<p>Or by specifying a range of tasks</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">0</span><span class="o">-</span><span class="mi">1</span>  <span class="n">python</span> <span class="n">script</span> <span class="o">%</span><span class="n">t</span>
</pre></div>
</div>
<p>%t being the taskid that your Python script will parse.  Note the <code class="docutils literal notranslate"><span class="pre">-l</span></code> on the
<code class="docutils literal notranslate"><span class="pre">srun</span></code> command: this will prepend each line with the taskid (0:, 1:)</p>
</section>
<section id="sharing-a-node-with-multiple-gpu-1process-gpu">
<h4>Sharing a node with multiple GPU 1process/GPU<a class="headerlink" href="#sharing-a-node-with-multiple-gpu-1process-gpu" title="Link to this heading"></a></h4>
<p>On Digital Research Alliance of Canada, several nodes, especially nodes with
<code class="docutils literal notranslate"><span class="pre">largeGPU</span></code> (P100) are reserved for jobs requesting the whole node, therefore
packing multiple processes in a single job can leverage faster GPU.</p>
<p>If you want different tasks to access different GPUs in a single allocation you
need to create an allocation requesting a whole node and using <code class="docutils literal notranslate"><span class="pre">srun</span></code> with a
subset of those resources (1 GPU).</p>
<p>Keep in mind that every resource not specified on the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command while
inherit the global allocation specification so you need to split each resource
in a subset (except –cpu-per-task which is a per-task requirement)</p>
<p>Each <code class="docutils literal notranslate"><span class="pre">srun</span></code> represents a job step (<code class="docutils literal notranslate"><span class="pre">%s</span></code>).</p>
<p>Example for a GPU node with 24 cores and 4 GPUs and 128G of RAM
Requesting 1 task per GPU</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH --nodes=1-1</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --ntasks-per-node=4</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --output=myjob_output_wrapper.out</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --gres=gpu:4</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --cpus-per-task=6</span>
<span class="linenos"> 7</span>srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-n1<span class="w"> </span>--mem<span class="o">=</span>30G<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>%j-step-%s.out<span class="w"> </span>--exclusive<span class="w"> </span>--multi-prog<span class="w"> </span>python<span class="w"> </span>script<span class="w"> </span>args1<span class="w"> </span><span class="p">&amp;</span>
<span class="linenos"> 8</span>srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-n1<span class="w"> </span>--mem<span class="o">=</span>30G<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>%j-step-%s.out<span class="w"> </span>--exclusive<span class="w"> </span>--multi-prog<span class="w"> </span>python<span class="w"> </span>script<span class="w"> </span>args2<span class="w"> </span><span class="p">&amp;</span>
<span class="linenos"> 9</span>srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-n1<span class="w"> </span>--mem<span class="o">=</span>30G<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>%j-step-%s.out<span class="w"> </span>--exclusive<span class="w"> </span>--multi-prog<span class="w"> </span>python<span class="w"> </span>script<span class="w"> </span>args3<span class="w"> </span><span class="p">&amp;</span>
<span class="linenos">10</span>srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-n1<span class="w"> </span>--mem<span class="o">=</span>30G<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>%j-step-%s.out<span class="w"> </span>--exclusive<span class="w"> </span>--multi-prog<span class="w"> </span>python<span class="w"> </span>script<span class="w"> </span>args4<span class="w"> </span><span class="p">&amp;</span>
<span class="linenos">11</span><span class="nb">wait</span>
</pre></div>
</div>
<p>This will create 4 output files:</p>
<ul class="simple">
<li><p>JOBID-step-0.out</p></li>
<li><p>JOBID-step-1.out</p></li>
<li><p>JOBID-step-2.out</p></li>
<li><p>JOBID-step-3.out</p></li>
</ul>
</section>
<section id="sharing-a-node-with-multiple-gpu-multiple-processes-gpu">
<h4>Sharing a node with multiple GPU &amp; multiple processes/GPU<a class="headerlink" href="#sharing-a-node-with-multiple-gpu-multiple-processes-gpu" title="Link to this heading"></a></h4>
<p>Combining both previous sections, we can create a script requesting a whole node
with four GPUs, allocating 1 GPU per <code class="docutils literal notranslate"><span class="pre">srun</span></code> and sharing each GPU with multiple
processes</p>
<p>Example still with a 24 cores/4 GPUs/128G RAM
Requesting 2 tasks per GPU</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span><span class="c1">#SBATCH --nodes=1-1</span>
<span class="linenos"> 3</span><span class="c1">#SBATCH --ntasks-per-node=8</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --output=myjob_output_wrapper.out</span>
<span class="linenos"> 5</span><span class="c1">#SBATCH --gres=gpu:4</span>
<span class="linenos"> 6</span><span class="c1">#SBATCH --cpus-per-task=3</span>
<span class="linenos"> 7</span>srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-n2<span class="w"> </span>--mem<span class="o">=</span>30G<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>%j-step-%s-task-%t.out<span class="w"> </span>--exclusive<span class="w"> </span>--multi-prog<span class="w"> </span>silly.conf<span class="w"> </span><span class="p">&amp;</span>
<span class="linenos"> 8</span>srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-n2<span class="w"> </span>--mem<span class="o">=</span>30G<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>%j-step-%s-task-%t.out<span class="w"> </span>--exclusive<span class="w"> </span>--multi-prog<span class="w"> </span>silly.conf<span class="w"> </span><span class="p">&amp;</span>
<span class="linenos"> 9</span>srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-n2<span class="w"> </span>--mem<span class="o">=</span>30G<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>%j-step-%s-task-%t.out<span class="w"> </span>--exclusive<span class="w"> </span>--multi-prog<span class="w"> </span>silly.conf<span class="w"> </span><span class="p">&amp;</span>
<span class="linenos">10</span>srun<span class="w"> </span>--gres<span class="o">=</span>gpu:1<span class="w"> </span>-n2<span class="w"> </span>--mem<span class="o">=</span>30G<span class="w"> </span>-l<span class="w"> </span>--output<span class="o">=</span>%j-step-%s-task-%t.out<span class="w"> </span>--exclusive<span class="w"> </span>--multi-prog<span class="w"> </span>silly.conf<span class="w"> </span><span class="p">&amp;</span>
<span class="linenos">11</span><span class="nb">wait</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--exclusive</span></code> is important to specify subsequent step/srun to bind to different cpus.</p>
<p>This will produce 8 output files, 2 for each step:</p>
<ul class="simple">
<li><p>JOBID-step-0-task-0.out</p></li>
<li><p>JOBID-step-0-task-1.out</p></li>
<li><p>JOBID-step-1-task-0.out</p></li>
<li><p>JOBID-step-1-task-1.out</p></li>
<li><p>JOBID-step-2-task-0.out</p></li>
<li><p>JOBID-step-2-task-1.out</p></li>
<li><p>JOBID-step-3-task-0.out</p></li>
<li><p>JOBID-step-3-task-1.out</p></li>
</ul>
<p>Running <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> in silly.conf, while parsing the output, we can see 4
GPUs allocated and 2 tasks per GPU</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">cat<span class="w"> </span>JOBID-step-*<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Tesla
<span class="m">0</span>:<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>P100-PCIE...<span class="w">  </span>On<span class="w">   </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:04:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="m">1</span>:<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>P100-PCIE...<span class="w">  </span>On<span class="w">   </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:04:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="m">0</span>:<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>P100-PCIE...<span class="w">  </span>On<span class="w">   </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:83:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="m">1</span>:<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>P100-PCIE...<span class="w">  </span>On<span class="w">   </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:83:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="m">0</span>:<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>P100-PCIE...<span class="w">  </span>On<span class="w">   </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:82:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="m">1</span>:<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>P100-PCIE...<span class="w">  </span>On<span class="w">   </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:82:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="m">0</span>:<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>P100-PCIE...<span class="w">  </span>On<span class="w">   </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:03:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span>
<span class="m">1</span>:<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span><span class="w">  </span>Tesla<span class="w"> </span>P100-PCIE...<span class="w">  </span>On<span class="w">   </span><span class="p">|</span><span class="w"> </span><span class="m">00000000</span>:03:00.0<span class="w"> </span>Off<span class="w"> </span><span class="p">|</span><span class="w">                    </span><span class="m">0</span><span class="w"> </span><span class="p">|</span></span>
</pre></div></div></section>
</section>
</section>
<section id="multiple-nodes">
<h2>Multiple Nodes<a class="headerlink" href="#multiple-nodes" title="Link to this heading"></a></h2>
<section id="data-parallel">
<h3>Data Parallel<a class="headerlink" href="#data-parallel" title="Link to this heading"></a></h3>
<img alt="_images/dataparallel.png" src="_images/dataparallel.png" />
<p>Request 3 nodes with at least 4 GPUs each.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="ch">#!/bin/bash</span>
<span class="linenos"> 2</span>
<span class="linenos"> 3</span><span class="c1"># Number of Nodes</span>
<span class="linenos"> 4</span><span class="c1">#SBATCH --nodes=3</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="c1"># Number of tasks. 3 (1 per node)</span>
<span class="linenos"> 7</span><span class="c1">#SBATCH --ntasks=3</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="c1"># Number of GPU per node</span>
<span class="linenos">10</span><span class="c1">#SBATCH --gres=gpu:4</span>
<span class="linenos">11</span><span class="c1">#SBATCH --gpus-per-node=4</span>
<span class="linenos">12</span>
<span class="linenos">13</span><span class="c1"># 16 CPUs per node</span>
<span class="linenos">14</span><span class="c1">#SBATCH --cpus-per-gpu=4</span>
<span class="linenos">15</span>
<span class="linenos">16</span><span class="c1"># 16Go per nodes (4Go per GPU)</span>
<span class="linenos">17</span><span class="c1">#SBATCH --mem=16G</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1"># we need all nodes to be ready at the same time</span>
<span class="linenos">20</span><span class="c1">#SBATCH --wait-all-nodes=1</span>
<span class="linenos">21</span>
<span class="linenos">22</span><span class="c1"># Total resources:</span>
<span class="linenos">23</span><span class="c1">#   CPU: 16 * 3 = 48</span>
<span class="linenos">24</span><span class="c1">#   RAM: 16 * 3 = 48 Go</span>
<span class="linenos">25</span><span class="c1">#   GPU:  4 * 3 = 12</span>
<span class="linenos">26</span>
<span class="linenos">27</span><span class="c1"># Setup our rendez-vous point</span>
<span class="linenos">28</span><span class="nv">RDV_ADDR</span><span class="o">=</span><span class="k">$(</span>hostname<span class="k">)</span>
<span class="linenos">29</span><span class="nv">WORLD_SIZE</span><span class="o">=</span><span class="nv">$SLURM_JOB_NUM_NODES</span>
<span class="linenos">30</span><span class="c1"># -----</span>
<span class="linenos">31</span>
<span class="linenos">32</span>srun<span class="w"> </span>-l<span class="w"> </span>torchrun<span class="w"> </span><span class="se">\</span>
<span class="linenos">33</span><span class="w">   </span>--nproc_per_node<span class="o">=</span><span class="nv">$SLURM_GPUS_PER_NODE</span><span class="se">\</span>
<span class="linenos">34</span><span class="w">   </span>--nnodes<span class="o">=</span><span class="nv">$WORLD_SIZE</span><span class="se">\</span>
<span class="linenos">35</span><span class="w">   </span>--rdzv_id<span class="o">=</span><span class="nv">$SLURM_JOB_ID</span><span class="se">\</span>
<span class="linenos">36</span><span class="w">   </span>--rdzv_backend<span class="o">=</span>c10d<span class="se">\</span>
<span class="linenos">37</span><span class="w">   </span>--rdzv_endpoint<span class="o">=</span><span class="nv">$RDV_ADDR</span><span class="se">\</span>
<span class="linenos">38</span><span class="w">   </span>training_script.py
</pre></div>
</div>
<p>You can find below a pytorch script outline on what a multi-node trainer could look like.</p>
<div class="highlight-python notranslate" id="training-script-outline-for-multi-node-training"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
   <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="o">=</span> <span class="o">...</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="o">...</span>

   <span class="nd">@property</span>
   <span class="k">def</span> <span class="nf">device_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span>

   <span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="o">=</span> <span class="n">path</span>
      <span class="c1"># ...</span>

   <span class="k">def</span> <span class="nf">should_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="c1"># Note: only one worker saves its weights</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="mi">0</span>

   <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

      <span class="c1"># Save your states here</span>
      <span class="c1"># Note: you should save the weights of self.model not ddp_model</span>
      <span class="c1"># ...</span>

   <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RANK&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

      <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Global rank should be set (Only Rank 0 can save checkpoints)&#39;</span>
      <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Local rank should be set&#39;</span>

      <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;gloo|nccl&quot;</span><span class="p">)</span>

   <span class="k">def</span> <span class="nf">sync_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">resuming</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">resuming</span><span class="p">:</span>
            <span class="c1"># in the case of resuming all workers need to load the same checkpoint</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">()</span>

            <span class="c1"># Wait for everybody to finish loading the checkpoint</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
            <span class="k">return</span>

      <span class="c1"># Make sure all workers have the same initial weights</span>
      <span class="c1"># This makes the leader save his weights</span>
      <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

      <span class="c1"># All workers wait for the leader to finish</span>
      <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

      <span class="c1"># All followers load the leader&#39;s weights</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">()</span>

      <span class="c1"># Leader waits for the follower to load the weights</span>
      <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

   <span class="k">def</span> <span class="nf">dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
      <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">ElasticDistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
      <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="k">return</span> <span class="n">train_loader</span>

   <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
      <span class="c1"># Your batch processing step here</span>
      <span class="c1"># ...</span>
      <span class="k">pass</span>

   <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">()</span>

      <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span><span class="p">],</span>
            <span class="n">output_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span>
      <span class="p">)</span>

      <span class="n">loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

      <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
               <span class="bp">self</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

               <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
                  <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
   <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
   <span class="n">trainer</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
   <span class="n">tainer</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

   <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To bypass Python GIL (Global interpreter lock) pytorch spawn one process for each GPU.
In the example above this means at least 12 processes are spawn, at least 4 on each node.</p>
</div>
</section>
</section>
<section id="weight-and-biases-wandb">
<h2>Weight and Biases (WandB)<a class="headerlink" href="#weight-and-biases-wandb" title="Link to this heading"></a></h2>
<p>Students supervised by core professors are elligible to the Mila organization on wandb.
To request access, write to <a class="reference external" href="mailto:it-support&#37;&#52;&#48;mila&#46;quebec">it-support<span>&#64;</span>mila<span>&#46;</span>quebec</a>.
Then please follow the guidelines below to get your account created or linked to Mila’s organization.</p>
<section id="logging-in-for-the-first-time">
<h3>Logging in for the first time<a class="headerlink" href="#logging-in-for-the-first-time" title="Link to this heading"></a></h3>
<section id="for-those-who-already-have-a-wandb-account">
<h4>For those who already have a WandB account<a class="headerlink" href="#for-those-who-already-have-a-wandb-account" title="Link to this heading"></a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you already have an account and want to have access to
mila-org with it, first add your email &#64;mila.quebec to your account
and make it your primary email. See documentation <a class="reference external" href="https://docs.wandb.ai/guides/app/settings-page/emails">here</a>
on how to do so. Then log out so that you can make your first
connection with single sign-on. Make sure to do this before following
the next steps otherwise you will end up with 2 separate accounts</p>
</div>
<ul>
<li><p>Go to <a class="reference external" href="https://wandb.ai">https://wandb.ai</a> and click sign in.</p></li>
<li><p>Enter your email &#64;mila.quebec at the bottom.</p></li>
<li><p>The password box should disappear when you are done typing your email. Then click log in,
and you will be redirected to a single sign-on page.</p></li>
<li><p>Select your account mila.quebec. If you already have an account, wandb will offer to link
it, otherwise, you will be invited to create a new account.</p>
<blockquote>
<div><ul class="simple">
<li><p>For new accounts, select Professional.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</section>
</section>
</section>
<section id="comet">
<h2>Comet<a class="headerlink" href="#comet" title="Link to this heading"></a></h2>
<p>Students supervised by core professors are elligible to the Mila organization on Comet.
To request access, write to <a class="reference external" href="mailto:it-support&#37;&#52;&#48;mila&#46;quebec">it-support<span>&#64;</span>mila<span>&#46;</span>quebec</a>.
Then please follow the guidelines below to get your account created within Mila’s organization.
This account will be independant from your personal account if you already have one.</p>
<section id="id17">
<h3>Logging in for the first time<a class="headerlink" href="#id17" title="Link to this heading"></a></h3>
<p>To access mila-org, you need to login using the url <a class="reference external" href="https://comet.mila.quebec/">https://comet.mila.quebec/</a>.
On first login, one of the following will apply:</p>
<ol class="arabic simple">
<li><p>If you have no Comet account or another account with an email address other than
&#64;mila.quebec: Comet will create a new account.</p></li>
<li><p>If you have an account with our email address &#64;mila.quebec. Comet will link your account
to mila-org.</p></li>
</ol>
</section>
</section>
<section id="frequently-asked-questions-faqs">
<h2>Frequently asked questions (FAQs)<a class="headerlink" href="#frequently-asked-questions-faqs" title="Link to this heading"></a></h2>
<section id="connection-ssh-issues">
<h3>Connection/SSH issues<a class="headerlink" href="#connection-ssh-issues" title="Link to this heading"></a></h3>
<section id="i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node">
<h4>I’m getting <code class="docutils literal notranslate"><span class="pre">connection</span> <span class="pre">refused</span></code> while trying to connect to a login node<a class="headerlink" href="#i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node" title="Link to this heading"></a></h4>
<p>Login nodes are protected against brute force attacks and might ban your IP if
it detects too many connections/failures. You will be automatically unbanned
after 1 hour. For any further problem, please <a class="reference external" href="https://mila-iqia.atlassian.net/servicedesk/customer/portals">submit a support ticket.</a></p>
</section>
</section>
<section id="shell-issues">
<h3>Shell issues<a class="headerlink" href="#shell-issues" title="Link to this heading"></a></h3>
<section id="how-do-i-change-my-shell">
<h4>How do I change my shell ?<a class="headerlink" href="#how-do-i-change-my-shell" title="Link to this heading"></a></h4>
<p>By default you will be assigned <code class="docutils literal notranslate"><span class="pre">/bin/bash</span></code> as a shell. If you would like to
change for another one, please <a class="reference external" href="https://mila-iqia.atlassian.net/servicedesk/customer/portals">submit a support ticket.</a></p>
</section>
</section>
<section id="slurm-issues">
<h3>SLURM issues<a class="headerlink" href="#slurm-issues" title="Link to this heading"></a></h3>
<section id="how-can-i-get-an-interactive-shell-on-the-cluster">
<h4>How can I get an interactive shell on the cluster ?<a class="headerlink" href="#how-can-i-get-an-interactive-shell-on-the-cluster" title="Link to this heading"></a></h4>
<p>Use <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">[--slurm_options]</span></code> without any executable at the end of the
command, this will launch your default shell on an interactive session. Remember
that an interactive session is bound to the login node where you start it so you
could risk losing your job if the login node becomes unreachable.</p>
</section>
<section id="how-can-i-reset-my-cluster-password">
<h4>How can I reset my cluster password ?<a class="headerlink" href="#how-can-i-reset-my-cluster-password" title="Link to this heading"></a></h4>
<p>To reset your password, please <a class="reference external" href="https://mila-iqia.atlassian.net/servicedesk/customer/portals">submit a support ticket.</a></p>
<p><strong>Warning</strong>: your cluster password is the same as your Google Workspace account. So,
after reset, you must use the new password for all your Google services.</p>
</section>
<section id="srun-error-mem-and-mem-per-cpu-are-mutually-exclusive">
<h4>srun: error: –mem and –mem-per-cpu are mutually exclusive<a class="headerlink" href="#srun-error-mem-and-mem-per-cpu-are-mutually-exclusive" title="Link to this heading"></a></h4>
<p>You can safely ignore this, <code class="docutils literal notranslate"><span class="pre">salloc</span></code> has a default memory flag in case you
don’t provide one.</p>
</section>
<section id="how-can-i-see-where-and-if-my-jobs-are-running">
<h4>How can I see where and if my jobs are running ?<a class="headerlink" href="#how-can-i-see-where-and-if-my-jobs-are-running" title="Link to this heading"></a></h4>
<p>Use <code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-u</span> <span class="pre">YOUR_USERNAME</span></code> to see all your job status and locations.
To get more info on a running job, try <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">job</span> <span class="pre">#JOBID</span></code></p>
</section>
<section id="unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified">
<h4>Unable to allocate resources: Invalid account or account/partition combination specified<a class="headerlink" href="#unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified" title="Link to this heading"></a></h4>
<p>Chances are your account is not setup properly. You should <a class="reference external" href="https://mila-iqia.atlassian.net/servicedesk/customer/portals">submit a support
ticket.</a></p>
</section>
<section id="how-do-i-cancel-a-job">
<h4>How do I cancel a job?<a class="headerlink" href="#how-do-i-cancel-a-job" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>To cancel a specific job, use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">#JOBID</span></code></p></li>
<li><p>To cancel all your jobs (running and pending), use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-u</span> <span class="pre">YOUR_USERNAME</span></code></p></li>
<li><p>To cancel all your pending jobs only, use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-t</span> <span class="pre">PD</span></code></p></li>
</ul>
</section>
<section id="how-can-i-access-a-node-on-which-one-of-my-jobs-is-running">
<h4>How can I access a node on which one of my jobs is running ?<a class="headerlink" href="#how-can-i-access-a-node-on-which-one-of-my-jobs-is-running" title="Link to this heading"></a></h4>
<p>You can ssh into a node on which you have a job running, your ssh connection
will be adopted by your job, i.e.  if your job finishes your ssh connection will
be automatically terminated. In order to connect to a node, you need to have
password-less ssh either with a key present in your home or with an
<code class="docutils literal notranslate"><span class="pre">ssh-agent</span></code>. You can generate a key on the login node like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ssh-keygen<span class="w"> </span><span class="o">(</span>3xENTER<span class="o">)</span></span>
<span class="prompt1">cat<span class="w"> </span>~/.ssh/id_rsa.pub<span class="w"> </span>&gt;&gt;<span class="w"> </span>~/.ssh/authorized_keys</span>
<span class="prompt1">chmod<span class="w"> </span><span class="m">600</span><span class="w"> </span>~/.ssh/authorized_keys</span>
<span class="prompt1">chmod<span class="w"> </span><span class="m">700</span><span class="w"> </span>~/.ssh</span>
</pre></div></div><p>The ECDSA, RSA and ED25519 fingerprints for Mila’s compute nodes are:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>SHA256:hGH64v72h/c0SfngAWB8WSyMj8WSAf5um3lqVsa7Cfk (ECDSA)
SHA256:4Es56W5ANNMQza2sW2O056ifkl8QBvjjNjfMqpB7/1U (RSA)
SHA256:gUQJw6l1lKjM1cCyennetPoQ6ST0jMhQAs/57LhfakA (ED25519)
</pre></div>
</div>
</section>
<section id="i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node">
<h4>I’m getting <code class="docutils literal notranslate"><span class="pre">Permission</span> <span class="pre">denied</span> <span class="pre">(publickey)</span></code> while trying to connect to a node<a class="headerlink" href="#i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node" title="Link to this heading"></a></h4>
<p>See previous question</p>
</section>
<section id="where-do-i-put-my-data-during-a-job">
<h4>Where do I put my data during a job ?<a class="headerlink" href="#where-do-i-put-my-data-during-a-job" title="Link to this heading"></a></h4>
<p>Your <code class="docutils literal notranslate"><span class="pre">/home</span></code> as well as the datasets are on shared file-systems, it is
recommended to copy them to the <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> to better process them and
leverage higher-speed local drives. If you run a low priority job subject to
preemption, it’s better to save any output you want to keep on the shared file
systems, because the <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> is deleted at the end of each job.</p>
</section>
<section id="slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup">
<h4>slurmstepd: error: Detected 1 oom-kill event(s) in step #####.batch cgroup<a class="headerlink" href="#slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup" title="Link to this heading"></a></h4>
<p>You exceeded the amount of memory allocated to your job, either you did not
request enough memory or you have a memory leak in your process. Try increasing
the amount of memory requested with <code class="docutils literal notranslate"><span class="pre">--mem=</span></code> or <code class="docutils literal notranslate"><span class="pre">--mem-per-cpu=</span></code>.</p>
</section>
<section id="fork-retry-resource-temporarily-unavailable">
<h4>fork: retry: Resource temporarily unavailable<a class="headerlink" href="#fork-retry-resource-temporarily-unavailable" title="Link to this heading"></a></h4>
<p>You exceeded the limit of 2000 tasks/PIDs in your job, it probably means there
is an issue with a sub-process spawning too many processes in your script. For
any help with your software, please <a class="reference external" href="https://mila-iqia.atlassian.net/servicedesk/customer/portals">submit a support ticket.</a></p>
</section>
</section>
<section id="pytorch-issues">
<h3>PyTorch issues<a class="headerlink" href="#pytorch-issues" title="Link to this heading"></a></h3>
<section id="i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263">
<h4>I randomly get <code class="docutils literal notranslate"><span class="pre">INTERNAL</span> <span class="pre">ASSERT</span> <span class="pre">FAILED</span> <span class="pre">at</span> <span class="pre">&quot;../aten/src/ATen/MapAllocator.cpp&quot;:263</span></code><a class="headerlink" href="#i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263" title="Link to this heading"></a></h4>
<p>You are using PyTorch 1.10.x and hitting <a class="reference external" href="https://github.com/pytorch/pytorch/issues/67864">#67864</a>,
for which the solution is <a class="reference external" href="https://github.com/pytorch/pytorch/pull/72232">PR #72232</a>
merged in PyTorch 1.11.x. For an immediate fix, consider the following compilable Gist:
<a class="reference external" href="https://gist.github.com/obilaniu/b133470cb70410d841faca819d3921e5">hack.cpp</a>.
Compile the patch to <code class="docutils literal notranslate"><span class="pre">hack.so</span></code> and then <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_PRELOAD=/absolute/path/to/hack.so</span></code>
before executing the Python process that <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch</span></code> a broken PyTorch 1.10.</p>
<p>For Hydra users who are using the submitit launcher plug-in, the <code class="docutils literal notranslate"><span class="pre">env_set</span></code> key cannot
be used to set <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> in the environment as it does so too late at runtime. The
dynamic loader reads <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> only once and very early during the startup of any
process, before the variable can be set from inside the process. The hack must therefore
be injected using the <code class="docutils literal notranslate"><span class="pre">setup</span></code> key in Hydra YAML config file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hydra</span><span class="p">:</span>
  <span class="n">launcher</span><span class="p">:</span>
    <span class="n">setup</span><span class="p">:</span>
      <span class="o">-</span> <span class="n">export</span> <span class="n">LD_PRELOAD</span><span class="o">=/</span><span class="n">absolute</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">hack</span><span class="o">.</span><span class="n">so</span>
</pre></div>
</div>
</section>
<section id="on-mig-gpus-i-get-torch-cuda-device-count-0-despite-torch-cuda-is-available">
<h4>On MIG GPUs, I get <code class="docutils literal notranslate"><span class="pre">torch.cuda.device_count()</span> <span class="pre">==</span> <span class="pre">0</span></code> despite <code class="docutils literal notranslate"><span class="pre">torch.cuda.is_available()</span></code><a class="headerlink" href="#on-mig-gpus-i-get-torch-cuda-device-count-0-despite-torch-cuda-is-available" title="Link to this heading"></a></h4>
<p>You are using PyTorch 1.13.x and hitting <a class="reference external" href="https://github.com/pytorch/pytorch/issues/90543">#90543</a>,
for which the solution is <a class="reference external" href="https://github.com/pytorch/pytorch/pull/92315">PR #92315</a>
merged in PyTorch 2.0.</p>
<p>To avoid thus problem, update to PyTorch 2.0. If PyTorch 1.13.x is required, a
workaround is to add the following to your script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">unset</span><span class="w"> </span>CUDA_VISIBLE_DEVICES
</pre></div>
</div>
<p>But this is no longer necessary with PyTorch &gt;= 2.0.</p>
</section>
<section id="i-am-told-my-pytorch-job-abuses-the-filesystem-with-extreme-amounts-of-iops">
<h4>I am told my PyTorch job abuses the filesystem with extreme amounts of IOPS<a class="headerlink" href="#i-am-told-my-pytorch-job-abuses-the-filesystem-with-extreme-amounts-of-iops" title="Link to this heading"></a></h4>
<p>A fairly common issue in PyTorch is:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>RuntimeError: one of the variables needed for gradient computation has been
modified by an inplace operation: [torch.cuda.FloatTensor [1, 50, 300]],
which is output 0 of SplitBackward, is at version 2; expected version 0
instead. Hint: enable anomaly detection to find the operation that failed to
compute its gradient, with torch.autograd.set_detect_anomaly(True).
</pre></div>
</div>
<p>PyTorch’s autograd engine contains an “anomaly detection mode”, which detects
such things as NaN/infinities being created, and helps debugging in-place
Tensor modifications. It is activated with</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">set_detect_anomaly</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>PyTorch’s implementation of the anomaly-detection mode tracks where every Tensor
was created in the program. This involves the collection of the backtrace at the
point the Tensor was created.</p>
<p><strong>Unfortunately</strong>, the collection of a backtrace involves a <code class="docutils literal notranslate"><span class="pre">stat()</span></code> system
call to <strong>every</strong> source file in the backtrace. This is considered a metadata
access to <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> and results in intolerably heavy traffic to the shared
filesystem containing the source code, usually <code class="docutils literal notranslate"><span class="pre">$HOME</span></code>, <em>whatever the location
of the dataset</em>, and <em>even if it is on</em> <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code>. It is the source-code
files being polled, not the dataset. As there can be hundreds of PyTorch tensors
created per iteration and thousands of iterations per second, this mode results
in <strong>extreme</strong> amounts of IOPS to the filesystem.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<ul class="simple">
<li><p><strong>Do not use</strong> <code class="docutils literal notranslate"><span class="pre">torch.autograd.set_detect_anomaly(True)</span></code> except for
debugging an individual job interactively, and switch it off as soon as
done using it.</p></li>
<li><p><strong>Do not set</strong> <code class="docutils literal notranslate"><span class="pre">torch.autograd.set_detect_anomaly(True)</span></code> enabled
unconditionally in all your jobs. <strong>It is not a consequence-free aid</strong>.
Due to heavy use of filesystem calls, <strong>it has a performance impact and
slows down your code</strong>, <em>on top</em> of abusing the filesystem.</p></li>
<li><p><strong>You will be contacted</strong> if you violate these guidelines due to the
severity of its impact on shared filesystems.</p></li>
</ul>
</div>
</section>
<section id="conda-refuses-to-create-an-environment-with-your-installed-cuda-driver-is-not-available">
<h4>Conda refuses to create an environment with <code class="docutils literal notranslate"><span class="pre">Your</span> <span class="pre">installed</span> <span class="pre">CUDA</span> <span class="pre">driver</span> <span class="pre">is:</span> <span class="pre">not</span> <span class="pre">available</span></code><a class="headerlink" href="#conda-refuses-to-create-an-environment-with-your-installed-cuda-driver-is-not-available" title="Link to this heading"></a></h4>
<p>Anaconda attempts to auto-detect the NVIDIA driver version of the system and
thus the maximum CUDA toolkit supported, in an attempt at choosing an
appropriate CUDA Toolkit version.</p>
<p>However, on login and CPU nodes, there is no NVIDIA GPU and thus no need for
NVIDIA drivers. But that means <code class="docutils literal notranslate"><span class="pre">conda</span></code>’s auto-detection will not work on
those nodes, and packages declaring a minimum requirement on the drivers will
fail to install.</p>
<p>The solution in such a situation is to set the environment variable
<code class="docutils literal notranslate"><span class="pre">CONDA_OVERRIDE_CUDA</span></code> to the desired CUDA Toolkit version; For example,</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CONDA_OVERRIDE_CUDA</span><span class="o">=</span><span class="m">11</span>.8<span class="w"> </span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>ENVNAME<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10<span class="w"> </span>pytorch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>pytorch-cuda<span class="o">=</span><span class="m">11</span>.8<span class="w"> </span>-c<span class="w"> </span>pytorch<span class="w"> </span>-c<span class="w"> </span>nvidia
</pre></div>
</div>
<p>This and other <code class="docutils literal notranslate"><span class="pre">CONDA_OVERRIDE_*</span></code> variables <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-virtual.html#overriding-detected-packages">are documented in the conda manual</a>.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Purpose.html" class="btn btn-neutral float-left" title="Purpose of this documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="Handbook.html" class="btn btn-neutral float-right" title="AI tooling and methodology handbook" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
  
<script type="text/javascript">
  window.onload = function() {
      $(".toggle > *").hide();
      $(".toggle .header").show();
      $(".toggle .header").click(function() {
          $(this).parent().children().not(".header").toggle(400);
          $(this).parent().children(".header").toggleClass("open");
      })
  };
</script>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>