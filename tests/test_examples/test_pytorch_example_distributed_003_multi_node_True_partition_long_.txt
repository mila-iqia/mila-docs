Task 0 output:
PyTorch Distributed available.
  Backends:
    Gloo: True
    NCCL: True
    MPI:  False
[(date) (time)] INFO     [0/4] __main__ - World size: 4, global   main.py:55
                             rank: 0, local rank: 0                             
Using downloaded and verified file: $SLURM_TMPDIR/data/cifar-10-python.tar.gz
Extracting $SLURM_TMPDIR/data/cifar-10-python.tar.gz to $SLURM_TMPDIR/data
Files already downloaded and verified
[(date) (time)] INFO     [0/4] __main__ - Effective batch size:   main.py:86
                             512                                                
[(date) (time)] INFO     [0/4]                           distributed.py:1140
                             torch.nn.parallel.distributed -                    
                             Reducer buckets have been                          
                             rebuilt in this iteration.                         
[(date) (time)] INFO     [0/4] __main__ - Epoch 0: Val loss:     main.py:180
                             55.032 accuracy: 50.08%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 1: Val loss:     main.py:180
                             48.975 accuracy: 56.76%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 2: Val loss:     main.py:180
                             53.192 accuracy: 55.32%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 3: Val loss:     main.py:180
                             47.434 accuracy: 59.68%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 4: Val loss:     main.py:180
                             44.753 accuracy: 63.28%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 5: Val loss:     main.py:180
                             56.168 accuracy: 59.26%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 6: Val loss:     main.py:180
                             54.097 accuracy: 63.38%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 7: Val loss:     main.py:180
                             54.764 accuracy: 63.02%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 8: Val loss:     main.py:180
                             64.655 accuracy: 61.20%                            
[(date) (time)] INFO     [0/4] __main__ - Epoch 9: Val loss:     main.py:180
                             61.904 accuracy: 63.20%                            
Done!
Task 1 output:
PyTorch Distributed available.
  Backends:
    Gloo: True
    NCCL: True
    MPI:  False
[(date) (time)] INFO     [1/4] __main__ - World size: 4, global   main.py:55
                             rank: 1, local rank: 1                             
[(date) (time)] INFO     [1/4]                           distributed.py:1140
                             torch.nn.parallel.distributed -                    
                             Reducer buckets have been                          
                             rebuilt in this iteration.                         
Done!
Task 2 output:
PyTorch Distributed available.
  Backends:
    Gloo: True
    NCCL: True
    MPI:  False
[(date) (time)] INFO     [2/4] __main__ - World size: 4, global   main.py:55
                             rank: 2, local rank: 0                             
Using downloaded and verified file: $SLURM_TMPDIR/data/cifar-10-python.tar.gz
Extracting $SLURM_TMPDIR/data/cifar-10-python.tar.gz to $SLURM_TMPDIR/data
Files already downloaded and verified
[(date) (time)] INFO     [2/4]                           distributed.py:1140
                             torch.nn.parallel.distributed -                    
                             Reducer buckets have been                          
                             rebuilt in this iteration.                         
Done!
Task 3 output:
PyTorch Distributed available.
  Backends:
    Gloo: True
    NCCL: True
    MPI:  False
[(date) (time)] INFO     [3/4] __main__ - World size: 4, global   main.py:55
                             rank: 3, local rank: 1                             
[(date) (time)] INFO     [3/4]                           distributed.py:1140
                             torch.nn.parallel.distributed -                    
                             Reducer buckets have been                          
                             rebuilt in this iteration.                         
Done!