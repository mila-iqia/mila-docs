{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Mila technical documentation","text":"<p>Welcome to Mila's technical documentation. If this is your first time here, we recommend you start by checking out the short quick start guide.</p> <p>Note</p> <p>Researchers are requested to acknowledge Mila when publications or reports are published on research that used compute resources provided by the organization. Our ability to showcase the successful use of our resources builds an important case for long-term and sustainable funding of Mila and our operations, i.e. have more compute resources available to researchers!</p> <p>The exact wording of the acknowledgment may vary, but please ensure that Mila is mentioned. The following can be used as a guideline:</p> <p>\"This research was enabled in part by compute resources provided by Mila (mila.quebec).\"</p> <p>If you find useful some of the software developed at Mila or the technical help you can get from our experts you can also mention it in the acknowledgment:</p> <p>\"This research was enabled in part by compute resources, software and technical help provided by Mila (mila.quebec).\"</p>"},{"location":"#support","title":"Support","text":"<p>To reach the Mila infrastructure support, please submit a support ticket.</p>"},{"location":"#contribution","title":"Contribution","text":"<p>If you find any errors in the documentation, missing or unclear sections, or would simply like to contribute, please open an issue or make a pull request on the github page.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Introduction<ul> <li>Purpose of this documentation</li> <li>Contributing</li> </ul> </li> <li>How-tos and Guides<ul> <li>Quick Start</li> <li>Logging in to the cluster</li> <li>Running your code</li> <li>Portability concerns and solutions</li> <li>Using containers</li> <li>Sharing Data with ACLs</li> <li>Contributing datasets</li> <li>Data Transmission using Globus Connect Personal</li> <li>Advanced SLURM usage and Multiple GPU jobs</li> <li>Multiple Nodes</li> <li>Weights and Biases (WandB)</li> <li>Comet</li> <li>JupyterHub</li> <li>Singularity</li> <li>Frequently asked questions (FAQ)</li> </ul> </li> <li>Systems and services<ul> <li>Computing infrastructure and policies<ul> <li>Roles and computing resources</li> <li>Node profile description</li> <li>Storage</li> <li>Data sharing policies</li> <li>Data Transmission</li> <li>Monitoring</li> </ul> </li> <li>Computational resources outside of Mila</li> </ul> </li> <li>Minimal Examples<ul> <li>Software Frameworks<ul> <li>PyTorch Setup</li> <li>Jax Setup</li> <li>Jax</li> </ul> </li> <li>Distributed Training<ul> <li>Single GPU Job</li> <li>Multi-GPU Job</li> <li>Multi-node Job</li> </ul> </li> <li>Good Practices<ul> <li>Checkpointing</li> <li>Weights &amp; Biases (wandb) setup</li> <li>Launch many jobs from same shell script</li> <li>Hyperparameter Optimization with Orion</li> <li>Launch many tasks on the same GPU</li> <li>Launch many jobs using SLURM job arrays</li> </ul> </li> <li>Advanced Examples<ul> <li>Multi-Node / Multi-GPU ImageNet Training</li> </ul> </li> <li>\ud83d\udd17 Research Project Template</li> </ul> </li> <li>General Theory<ul> <li>What is a computer cluster?</li> <li>Unix</li> <li>The workload manager</li> <li>Processing data</li> <li>Software on the cluster</li> </ul> </li> <li>Extras<ul> <li>Acknowledging Mila</li> <li>Mila Datasets</li> <li>Audio and video resources at Mila</li> <li>Visual Studio Code</li> <li>Who, what, where is IDT</li> <li>Cheat Sheet</li> <li>Environmental Impact</li> </ul> </li> </ul>"},{"location":"Acknowledgement/","title":"Acknowledging Mila","text":"<p>Note</p> <p>Researchers are requested to acknowledge Mila when publications or reports are published on research that used compute resources provided by the organization. Our ability to showcase the successful use of our resources builds an important case for long-term and sustainable funding of Mila and our operations, i.e. have more compute resources available to researchers!</p> <p>The exact wording of the acknowledgment may vary, but please ensure that Mila is mentioned. The following can be used as a guideline:</p> <p>\"This research was enabled in part by compute resources provided by Mila (mila.quebec).\"</p> <p>If you find useful some of the software developed at Mila or the technical help you can get from our experts you can also mention it in the acknowledgment:</p> <p>\"This research was enabled in part by compute resources, software and technical help provided by Mila (mila.quebec).\"</p> <p>If you've used DRAC resources as well for your research, please remember to acknowledge their use in your papers.</p>"},{"location":"Acknowledgement_text/","title":"Acknowledgement text","text":"<p>Note</p> <p>Researchers are requested to acknowledge Mila when publications or reports are published on research that used compute resources provided by the organization. Our ability to showcase the successful use of our resources builds an important case for long-term and sustainable funding of Mila and our operations, i.e. have more compute resources available to researchers!</p> <p>The exact wording of the acknowledgment may vary, but please ensure that Mila is mentioned. The following can be used as a guideline:</p> <p>\"This research was enabled in part by compute resources provided by Mila (mila.quebec).\"</p> <p>If you find useful some of the software developed at Mila or the technical help you can get from our experts you can also mention it in the acknowledgment:</p> <p>\"This research was enabled in part by compute resources, software and technical help provided by Mila (mila.quebec).\"</p>"},{"location":"Audio_video/","title":"Audio and video resources at Mila","text":"<p>See the intranet section on audio and video for complete information on audio and video systems made available at Mila.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to the Mila Docs","text":"<p>Thank you for your interest into making a better documentation for all at Mila.</p> <p>Here are some guidelines to help bring your contributions to life.</p>"},{"location":"CONTRIBUTING/#what-should-be-included-in-the-mila-docs","title":"What should be included in the Mila Docs","text":"<ul> <li>Mila cluster usage</li> <li>Digital Research Alliance of Canada cluster usage</li> <li>Job management tips / tricks</li> <li>Research good practices</li> <li>Software development good practices</li> <li>Useful tools</li> </ul> <p>NOTE: Examples should aim to not consume much more than 1 GPU/hour and 2 CPU/hour</p>"},{"location":"CONTRIBUTING/#issues","title":"Issues","text":"<p>Issues can be used to report any error in the documentation, missing or  unclear sections, broken tools or other suggestions to improve the  overall documentation.</p>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>PRs are welcome and we value the contents of contributions over the  appearance or functionality of the Pull Request. If you encounter problems  with the Markdown formatting, simply provide the content you would like to  add in the PR with instructions to format it. In the PR, reference the  related issues like this:</p> <pre><code>Resolves: #123\nSee also: #456, #789\n</code></pre> <p>If you would like to contribute directly in the code of the documentation,  keep the lines width to 80 characters or less. You can attempt to build  the docs yourself to see if the formating is right. This is done using <code>uv</code>:</p>"},{"location":"CONTRIBUTING/#building-the-docs","title":"Building the docs","text":"<p>First, install <code>uv</code> if you don't have it yet, using the commands described in the Getting Started section of the uv documentation.</p> <p>You can use it to:</p> <ul> <li>build the documentation and view it by opening an HTML file, or</li> <li>serve the docs locally on localhost.</li> </ul> <p>This command will build the documentation, which can be viewed by opening the local file <code>site/index.html</code>:</p> <pre><code>uv run mkdocs build\n</code></pre>"},{"location":"CONTRIBUTING/#serving-the-docs-locally","title":"Serving the docs locally","text":"<p>You can also serve the site with a simple HTTP server with live reloading when a file changes. This is particularly useful if you want to improve the docs and see your changes in real time.</p> <pre><code>uv run mkdocs serve --livereload\n</code></pre> <p>You can then access the local site through your browser at the URL http://127.0.0.1:8000/docs/.</p> <p>If you have any trouble building the docs, don't hesitate to open an issue to request help.</p>"},{"location":"CONTRIBUTING/#markdown-examples","title":"Markdown examples","text":"<p>The markup language used for the Mila Docs is Markdown. The documentation framework used is MkDocs, with the Material for MkDocs theme.</p> <p>Here are some examples of the most common Markdown constructs used in the Mila Docs. We encourage you to take a look a the mkdocs and mkdocs-material documentation for more examples:</p> <ul> <li>MkDocs documentation</li> <li>MkDocs-Material documentation</li> </ul>"},{"location":"CONTRIBUTING/#inline-markup","title":"Inline markup","text":"<ul> <li>one asterisk: <code>*text*</code> for emphasis (italics),</li> <li>two asterisks: <code>**text**</code> for strong emphasis (boldface), and</li> <li>backquotes: <code>`text`</code> for <code>code samples</code>, and</li> <li>external links: <code>[Link text](http://target)`</code>.</li> </ul>"},{"location":"CONTRIBUTING/#lists","title":"Lists","text":"<pre><code>* this is\n* a list\n  * with a nested list\n  * and some subitems\n\n* and here the parent list continues\n</code></pre>"},{"location":"CONTRIBUTING/#sections","title":"Sections","text":"<pre><code># This is one of the main headers\n\nAnd this is its alternative\n===========================\n\n## This is a sub-header\n\nAnd this is its alternative\n---------------------------\n\n### This is a sub-sub-header\n#### Etc\n</code></pre>"},{"location":"CONTRIBUTING/#note-box","title":"Note box","text":"<p>Admonitions are used to add panels. This is done through the <code>!!!</code> shortcut, depicted below. There are several types, such as <code>note</code>, <code>abstract</code>, <code>info</code>, <code>tip</code>, <code>success</code>, <code>question</code>, <code>warning</code>, <code>failure</code>, <code>danger</code>, <code>bug</code>, <code>example</code> or <code>quote</code>. <pre><code>!!! note\n   This is a note panel.\n</code></pre></p> <p>Note</p> <p>This is a note panel.</p> <pre><code>!!! example\n   This is an example panel, such as below.\n</code></pre> <p>Example</p> <p>This is an example panel.</p> <p>Panels could also be collapsible by using <code>???</code> instead of <code>!!!</code>, such as:</p> <pre><code>??? tip\n    I am collapsible!\n</code></pre> Tip <p>I am collapsible!</p>"},{"location":"CONTRIBUTING/#tables","title":"Tables","text":"<p>Tables can be added by following the format below: <pre><code>| Header 1      | Header 2         | Header 3 |\n| ------------- | ---------------- | -------- |\n| `First line`  | Hello world      |          |\n| `Second line` | Juste a new line | The end  |\n</code></pre></p> Header 1 Header 2 Header 3 <code>First line</code> Hello world <code>Second line</code> Juste a new line The end <p>For more examples of what is possible with Markdown in MkDocs, please refer to the mkdocs documentation and mkdocs-material documentation pages:</p> <ul> <li>MkDocs documentation</li> <li>MkDocs-Material documentation</li> </ul>"},{"location":"Cheatsheet/","title":"Cheat Sheet","text":"<p>A \"cheat sheet\" is available to provide you with information about the Mila and DRAC clusters at a glance.</p> <p>The production run consists of about 250 copies. If you want to have a copy, you can always come to the IDT lab during office hours (usually Tuesday 3pm-5pm and Wednesday 2pm-4pm).</p> <p>The IDT Cheat Sheet pdf is available if you want to access it online. The layout of the pdf has been set to be compatible with the printers at Mila so you can always print your own copy on regular paper (hint: set printer scale 100% with no margins).</p> <p>Keep in mind that the cheat sheet is not a replacement for the official documentation, which is the original source of information. Moreover, the official documentation is updated regularly, whereas the cheat sheet is probably going to be updated once a year. Usually this happens around April when the new DRAC allocations are announced, but in 2025, the update happened in July because of the new DRAC allocations that got delayed.</p> <p>Comments and suggestions are welcome (idt.cheatsheet@mila.quebec). Please also signal errors if you spot them before we do.</p>"},{"location":"Cheatsheet/#errata","title":"Errata","text":"<p>Here is a list of the known errors in the cheat sheet that will have to be fixed in the next version.</p>"},{"location":"Cheatsheet/#typo-in-account-name-for-drac-allocation","title":"Typo in account name for DRAC allocation","text":"<p>There is a place that reads <code>rrg-bengio-ad_gpu</code> instead of <code>rrg-bengioy-ad_gpu</code>. Same goes for <code>rrg-bengio-ad_cpu</code> instead of <code>rrg-bengioy-ad_cpu</code>.</p>"},{"location":"Cheatsheet/#partition-preemption-is-not-explained-accurately-on-page-2","title":"Partition preemption is not explained accurately on page 2","text":"<p>This is is not technically an error, but it is an oversimplification that might lead to some confusion.</p> <p>The preemption on the Mila cluster is a bit more complicated than what is described in the cheat sheet. The jobs from the <code>long</code> partition can be preempted to allow jobs in the <code>main</code> partition to run, but jobs in <code>main</code> are never going to be preempted to allow for other jobs in <code>main</code>, no matter how much of the \"fair use\" a user has already consumed.</p> <p>Jobs for the <code>unkillable</code> partition can preempt everything except other jobs on the <code>unkillable</code> partition.</p> <p>A good explanation of priorities and preemption will be added to the documentation, but the cheat sheet (or the cheat sheet errata) is not the place for this.</p>"},{"location":"Cheatsheet/#archive-is-5tb-and-not-500gb","title":"<code>$ARCHIVE</code> is 5TB and not 500GB","text":"<p>This is probably outdated information that we forgot to update. Your quota on <code>$ARCHIVE</code> is 5TB.</p>"},{"location":"Environmental_impact/","title":"Environmental Impact","text":"<p>These are our environmental impact calculations for the different clusters we have access to.</p>"},{"location":"Environmental_impact/#mila-cluster","title":"Mila Cluster","text":""},{"location":"Environmental_impact/#co2-emissions-for-power-consumption","title":"CO2 emissions for power consumption","text":"<p>The hardware for the Mila cluster is hosted in the province of Quebec, where the electricity is produced by Hydro-Qu\u00e9bec, almost exclusively from hydroelectricity. The CO2 emissions are therefore very low, and we can find the exact values in CO\u2082 Emissions and Hydro-Qu\u00e9bec Electricity, 1990-2021.</p> <p>We use the most recent value in the table for the year 2021, which is 0.6 kg/MWh. The Mila cluster consumes about 115 kW of power, and is running 24/7.</p> <p>By multiplying power by time, we get that 150 kW * (24*365 hours) is equal to 1314000 kWh, or 1314 MWh if we convert kilo- to mega-.</p> <p>We multiply that by the CO2 emissions per MWh, and 1314 times 0.6 is 788.4 kg, which is less a ton of CO2. Looking online, it is relatively easy to find Gold Standard carbon credits for $25 per ton of CO2, so we can offset the entire Mila cluster by spending approximately $20 per year.</p>"},{"location":"Environmental_impact/#hardware","title":"Hardware","text":"<p>No estimate has been done about the environmental impact of manufacturing the hardware itself.</p>"},{"location":"Environmental_impact/#digital-research-alliance-of-canada-clusters","title":"Digital Research Alliance of Canada Clusters","text":""},{"location":"Environmental_impact/#co2-emissions-for-power-consumption_1","title":"CO2 emissions for power consumption","text":"<p>Our current mega-allocation with DRAC is on the Narval, Beluga and Cedar clusters.</p> <p>Narval and Beluga are hosted in the \u00c9cole de technologie sup\u00e9rieure in Montreal (QC) so the same kind of reasoning and calculations apply as with the Mila cluster.</p> <p>The Cedar cluster is hosted at the Simon Fraser University in Vancouver (BC), in a province where 87% of the electricity is hydroelectricity.</p> <p>However, we do not have access to the exact numbers for those clusters at the moment.</p>"},{"location":"Environmental_impact/#hardware_1","title":"Hardware","text":"<p>No estimate has been done about the environmental impact of manufacturing the hardware itself.</p>"},{"location":"Extra_compute/","title":"Computational resources outside of Mila","text":"<p>This section seeks to provide insights and information on computational resources outside the Mila cluster itself.</p>"},{"location":"Extra_compute/#digital-research-alliance-of-canada-clusters","title":"Digital Research Alliance of Canada Clusters","text":"<p>The clusters named <code>Fir</code>, <code>Nibi</code>, <code>Narval</code>, <code>Rorqual</code> and <code>Trillium</code> are clusters provided by the Digital Research Alliance of Canada organisation (the Alliance). For Mila researchers, these clusters are to be used for larger experiments having many jobs, multi-node computation and/or multi-GPU jobs as well as long running jobs.</p> <p>Note</p> <p>If you use DRAC resources for your research, please remember to acknowledge their use in your papers</p> <p>Note</p> <p>Compute Canada ceased its operational responsibilities for supporting Canada\u2019s national advanced research computing (ARC) platform on March 31, 2022. The services will be supported by the new Digital Research Alliance of Canada.</p> <p>https://ace-net.ca/compute-canada-operations-move-to-the-digital-research-alliance-of-canada-(the-alliance).html</p> <p>Clusters of the Alliance are shared with researchers across the country, in part through a system of allocations. Allocations are given by the Alliance to selected research groups to ensure a steady availability of computational resources throughout the year. From the Alliance's documentation: <code>An allocation is an amount of resources that a research group can target for use for a period of time, usually a year.</code></p>"},{"location":"Extra_compute/#current-allocation-description","title":"Current Allocation Description","text":"<p>Depending on your supervisor's affiliations, you will have access to different allocations. Almost all students at Mila supervised by \"core\" professors should have access to the <code>rrg-bengioy-ad</code> allocation described below, but it is not the only one; Each supervisor also has a <code>def-profname</code> allocation. Your supervisor is your first point of contact in knowing which allocations you should have access to.</p> <p>The table below provides information on the allocation for <code>rrg-bengioy-ad</code> for the period which spans from July 2025 to Summer 2026.</p> Cluster CPUs account Model RGUs allocated # GPU equiv SLURM type specifier account Fir 193 rrg-bengioy-ad H100-80G 2000 165 <code>h100</code> rrg-bengioy-ad Rorqual 873 rrg-bengioy-ad H100-80G 1500 123 <code>h100</code> rrg-bengioy-ad Nibi 0 rrg-bengioy-ad H100-80G 1000 82 <code>h100</code> rrg-bengioy-ad <p>On DRAC clusters where Mila has no allocated resources under <code>rrg-bengioy-ad</code> this year, users can still use the default allocation associated with their supervisor, so long as the supervisor adds them to it on the DRAC web site. Every university professor in Canada gets a default allocation, and they can add their collaborators to it. The default accounts are of the form <code>def-&lt;yourprofname&gt;-gpu</code> and <code>def-&lt;yourprofname&gt;-cpu</code>. Management of a professor's default DRAC allocation is their personal responsibility, is beyond the control of Mila, and we are unable to provide support for such management.</p> <p>Note</p> <p>An allocation is not a maximal amount of resources that can be used simultaneously, it is a weighting factor of the workload manager to balance jobs. For instance, even though we are allocated 408 GPU-years across all clusters, we can use more or less than 408 GPUs simultaneously depending on the history of usage from our group and other groups using the cluster at a given period of time. Please see the Alliance's documentation for more information on how allocations and resource scheduling are configured for these installations.</p>"},{"location":"Extra_compute/#account-creation","title":"Account Creation","text":"<p>To access the Alliance clusters you have to first create an account at https://ccdb.alliancecan.ca. Use a password with at least 8 characters, mixed case letters, digits and special characters. Later you will be asked to create another password with those rules, and it\u2019s really convenient that the two password are the same.</p> <p>Then, you have to apply for a <code>role</code> at https://ccdb.alliancecan.ca/me/add_role, which basically means telling the Alliance that you are part of the lab so they know which cluster you can have access to, and track your usage.</p> <p>You will be asked for the CCRI (See screenshot below). Please reach out to your sponsor to get the CCRI.</p> <p></p> <p>You will need to wait for your sponsor to accept before being able to login to the Alliance clusters.</p> <p>You should apply to a <code>role</code> using this form for each allocation you can have access to. If, for instance, your supervisor is member of the <code>rrg-bengioy-ad</code> allocation, you should apply using Yoshua Bengio's CCRI, and you should apply separately using your supervisor's CCRI to have access to <code>def-&lt;yoursupervisor&gt;</code>. Ask your supervisor to share these CCRI with you.</p>"},{"location":"Extra_compute/#account-renewal","title":"Account Renewal","text":"<p>All user accounts (Sponsor &amp; Sponsored) have to be renewed annually in order to keep up-to-date information on active accounts and to deactivate unused accounts.</p> <p>To find out how to renew your account or for any other question regarding DRAC's accounts renewal, please head over to their FAQ.</p> <p>If the FAQ is of no help, you can contact DRAC renewal support team at <code>renewals@tech.alliancecan.ca</code> or the general support team at <code>support@tech.alliancecan.ca</code>.</p>"},{"location":"Extra_compute/#clusters","title":"Clusters","text":""},{"location":"Extra_compute/#fir","title":"Fir","text":"<p>Digital Research Alliance of Canada doc</p> <p>The successor to the legacy Cedar cluster. Retains its filesystem. Equipped with H100s.</p>"},{"location":"Extra_compute/#nibi","title":"Nibi","text":"<p>Digital Research Alliance of Canada doc</p> <p>The successor to the legacy Graham cluster. Retains its filesystem. Equipped with H100s.</p>"},{"location":"Extra_compute/#rorqual","title":"Rorqual","text":"<p>Digital Research Alliance of Canada doc</p> <p>The successor to the legacy Beluga cluster. No internet access on compute nodes. Equipped with H100s.</p>"},{"location":"Extra_compute/#trillium","title":"Trillium","text":"<p>Digital Research Alliance of Canada doc</p> <p>The successor to the legacy Niagara cluster. It is principally but not exclusively a CPU cluster. This cluster is not recommended in general. Compute resources in Trillium are not assigned to jobs on a per-CPU, but on a per-node basis. Equipped with H100s.</p>"},{"location":"Extra_compute/#narval","title":"Narval","text":"<p>Digital Research Alliance of Canada doc</p> <p>For some students, Narval might be a good choice if they have already set up there. Narval is the oldest cluster still online, and contains the oldest and smallest GPUs (A100-40GB). The A100 may however be a viable choice for jobs that cannot utilize a full H100.</p>"},{"location":"Extra_compute/#tamia","title":"TamIA","text":"<p>Digital Research Alliance of Canada doc</p> <p>This is a new cluster dedicated to AI jobs for Quebec-area research institutes, within the framework of PAICE (Pan-Canadian AI Compute Environment). Compute resources in TamIA are not assigned to jobs on a per-CPU, but on a per-node basis. Equipped with H100s and H200s.</p> <p>TamIA does not use regular DRAC allocations (<code>--account=rrg-bengioy-ad</code>). Instead, it uses AIP allocations (<code>--account=aip-${PI_NAME}</code>), where <code>${PI_NAME}</code> is the name of your supervising professor. Your professor must add you to their AIP allocation before you will be able to submit jobs.</p>"},{"location":"Extra_compute/#launching-jobs","title":"Launching Jobs","text":"<p>Users must specify the resource allocation Group Name using the flag <code>--account=rrg-bengioy-ad</code>.  To launch a CPU-only job:</p> <pre><code>sbatch --time=1:00:00 --account=rrg-bengioy-ad job.sh\n</code></pre> <p>Note</p> <p>The account name will differ based on your affiliation.</p> <p>To launch a GPU job:</p> <pre><code>sbatch --time=1:00:00 --account=rrg-bengioy-ad --gres=gpu:1 job.sh\n</code></pre> <p>And to get an interactive session, use the <code>salloc</code> command:</p> <pre><code>salloc --time=1:00:00 --account=rrg-bengioy-ad --gres=gpu:1\n</code></pre> <p>The full documentation for jobs launching on Alliance clusters can be found here.</p>"},{"location":"Extra_compute/#drac-storage","title":"DRAC Storage","text":"Storage Path Usage <code>$HOME</code> <code>/home/&lt;user&gt;/</code> Code, specific libraries <code>$HOME/projects</code> <code>/project/rrg-bengioy-ad</code> Compressed raw datasets <code>$SCRATCH</code> <code>/scratch/&lt;user&gt;</code> Processed datasets, experimental results, logs of experiments <code>$SLURM_TMPDIR</code> (on compute node) Temporary job results <p>They are roughly listed in order of increasing performance and optimized for different uses:</p> <ul> <li>The <code>$HOME</code> folder on Lustre is appropriate for code and libraries, which   are small and read once. Do not write experiemental results here!</li> <li>The <code>$HOME/projects</code> folder should only contain compressed raw datasets   (processed datasets should go in <code>$SCRATCH</code>). We have a limit on the   size and number of file in <code>$HOME/projects</code>, so do not put anything else   there.  If you add a new dataset there (make sure it is readable by every   member of the group using <code>chgrp -R rpp-bengioy &lt;dataset&gt;</code>).</li> <li>The <code>$SCRATCH</code> space can be used for short term storage. It has good   performance and large quotas, but is purged regularly (every file that has   not been used in the last 3 months gets deleted, but you receive an email   before this happens).</li> <li><code>$SLURM_TMPDIR</code> points to the local disk of the node on which a job is   running. It should be used to copy the data on the node at the beginning of   the job and write intermediate checkpoints. This folder is cleared after each   job, so results there must be copied to <code>$SCRATCH</code> at the end of a job.</li> </ul> <p>When a series of experiments is finished, results should be transferred back to Mila servers.</p> <p>More details on storage can be found here.</p>"},{"location":"Extra_compute/#modules","title":"Modules","text":"<p>Much software, such as Python or MATLAB, is already compiled and available on DRAC clusters through the <code>module</code> command and its subcommands. Their full documentation can be found here.</p> Command Description <code>module avail</code> Displays all the available modules <code>module load &lt;module&gt;</code> Loads \\&lt;module&gt; <code>module spider &lt;module&gt;</code> Shows specific details about \\&lt;module&gt; <p>In particular, if you with to use <code>Python 3.12</code> you can simply do:</p> <pre><code>module load python/3.12\n</code></pre> <p>Python on the cluster</p> <p>If you wish to use Python on the cluster, we strongly encourage you to read Alliance Python Documentation, and in particular the Pytorch and/or Tensorflow pages.</p> <p>The cluster has many Python packages (or <code>wheels</code>), such already compiled for the cluster. See here for the details. In particular, you can browse the packages by doing:</p> <pre><code>avail_wheels &lt;wheel&gt;\n</code></pre> <p>Such wheels can be installed using pip. Moreover, the most efficient way to use modules on the cluster is to build your environnement inside your job. See the script example below.</p>"},{"location":"Extra_compute/#script-example","title":"Script Example","text":"<p>Here is a <code>sbatch</code> script that follows good practices on Narval and can serve as inspiration for more complicated scripts:</p> <pre><code>#!/bin/bash\n#SBATCH --account=rrg-bengioy-ad         # Yoshua pays for your job\n#SBATCH --cpus-per-task=12               # Ask for 12 CPUs\n#SBATCH --gres=gpu:1                     # Ask for 1 GPU\n#SBATCH --mem=124G                       # Ask for 124 GB of RAM\n#SBATCH --time=03:00:00                  # The job will run for 3 hours\n#SBATCH -o /scratch/&lt;user&gt;/slurm-%j.out  # Write the log in $SCRATCH\n\n# 1. Create your environement locally\nmodule load StdEnv/2023 python/3.12\nvirtualenv --no-download $SLURM_TMPDIR/env\nsource $SLURM_TMPDIR/env/bin/activate\npip install --no-index torch torchvision\n\n# 2. Copy your dataset on the compute node, simultaneously unpacking if\n#    needed (Zip, tar); Alternatively, copy the dataset if it's in an\n#    advanced format like HDF5, or if you can use Zip directly.\nunzip     $SCRATCH/DATASET_CHANGEME.zip    -d $SLURM_TMPDIR\n# tar -xf $SCRATCH/DATASET_CHANGEME.tar.gz -C $SLURM_TMPDIR\n# cp      $SCRATCH/DATASET_CHANGEME.hdf5      $SLURM_TMPDIR\n\n# 3. Launch your job, tell it to save the model in $SLURM_TMPDIR\n#    and look for the dataset into $SLURM_TMPDIR\npython main.py --path $SLURM_TMPDIR --data_path $SLURM_TMPDIR\n\n# 4. Copy whatever you want to save on $SCRATCH\ncp $SLURM_TMPDIR/RESULTS_CHANGEME $SCRATCH\n</code></pre>"},{"location":"Extra_compute/#using-cometml-and-wandb","title":"Using CometML and Wandb","text":"<p>The compute nodes for Narval, Rorqual and Tamia don't have access to the internet, but there is a special module that can be loaded in order to allow training scripts to access some specific servers, which includes the necessary servers for using CometML and Wandb (\"Weights and Biases\").</p> <pre><code>module load httpproxy\n</code></pre> <p>More documentation about this can be found here.</p> <p>Note</p> <p>Be careful when using Wandb with <code>httpproxy</code>. It does not support sending artifacts and wandb's logger will hang in the background when your training is completed, wasting ressources until the job times out. It is recommended to use the offline mode with wandb instead to avoid such waste.</p>"},{"location":"Extra_compute/#faq","title":"FAQ","text":""},{"location":"Extra_compute/#what-are-rgus","title":"What are RGUs?","text":"<p>DRAC uses a concept called <code>RGUs</code> (Reference GPU Units) to measure the allocated GPU resources based on the type of device. This measurement combines the FP32 and FP16 performance of the GPU as well as the memory size. For example, an NVIDIA A100-40G counts has 4.0 RGUs, while a while an H100-80G counts as 12.15 RGUs. This is an improvement over the previous system of counting physical GPU devices and disregarding their actual performance. For example, saying that \"we have 4 GPUs per researcher\" omits which kind of GPUs we're talking about, which is fundamentally important. That proposed RGU measurement can still be improved, but criticisms about it are outside the scope of this document.</p>"},{"location":"Extra_compute/#what-to-do-with-importerror-lib64libmso6-version-glibc_223-not-found","title":"What to do with  <code>ImportError: /lib64/libm.so.6: version GLIBC_2.23 not found</code>?","text":"<p>The structure of the file system is different than a classical Linux, so your code has trouble finding libraries. See how to install binary packages.</p>"},{"location":"Extra_compute/#disk-quota-exceeded-error-on-project-file-systems","title":"Disk quota exceeded error on <code>/project</code> file systems","text":"<p>You have files in <code>/project</code> with the wrong permissions. See how to change permissions.</p>"},{"location":"Extra_compute_drac/","title":"Digital Research Alliance of Canada Clusters","text":"<p>The clusters named <code>Fir</code>, <code>Nibi</code>, <code>Narval</code>, <code>Rorqual</code> and <code>Trillium</code> are clusters provided by the Digital Research Alliance of Canada organisation (the Alliance). For Mila researchers, these clusters are to be used for larger experiments having many jobs, multi-node computation and/or multi-GPU jobs as well as long running jobs.</p> <p>Note</p> <p>If you use DRAC resources for your research, please remember to acknowledge their use in your papers</p> <p>Note</p> <p>Compute Canada ceased its operational responsibilities for supporting Canada\u2019s national advanced research computing (ARC) platform on March 31, 2022. The services will be supported by the new Digital Research Alliance of Canada.</p> <p>https://ace-net.ca/compute-canada-operations-move-to-the-digital-research-alliance-of-canada-(the-alliance).html</p> <p>Clusters of the Alliance are shared with researchers across the country, in part through a system of allocations. Allocations are given by the Alliance to selected research groups to ensure a steady availability of computational resources throughout the year. From the Alliance's documentation: <code>An allocation is an amount of resources that a research group can target for use for a period of time, usually a year.</code></p>"},{"location":"Extra_compute_drac/#current-allocation-description","title":"Current Allocation Description","text":"<p>Depending on your supervisor's affiliations, you will have access to different allocations. Almost all students at Mila supervised by \"core\" professors should have access to the <code>rrg-bengioy-ad</code> allocation described below, but it is not the only one; Each supervisor also has a <code>def-profname</code> allocation. Your supervisor is your first point of contact in knowing which allocations you should have access to.</p> <p>The table below provides information on the allocation for <code>rrg-bengioy-ad</code> for the period which spans from July 2025 to Summer 2026.</p> Cluster CPUs account Model RGUs allocated # GPU equiv SLURM type specifier account Fir 193 rrg-bengioy-ad H100-80G 2000 165 <code>h100</code> rrg-bengioy-ad Rorqual 873 rrg-bengioy-ad H100-80G 1500 123 <code>h100</code> rrg-bengioy-ad Nibi 0 rrg-bengioy-ad H100-80G 1000 82 <code>h100</code> rrg-bengioy-ad <p>On DRAC clusters where Mila has no allocated resources under <code>rrg-bengioy-ad</code> this year, users can still use the default allocation associated with their supervisor, so long as the supervisor adds them to it on the DRAC web site. Every university professor in Canada gets a default allocation, and they can add their collaborators to it. The default accounts are of the form <code>def-&lt;yourprofname&gt;-gpu</code> and <code>def-&lt;yourprofname&gt;-cpu</code>. Management of a professor's default DRAC allocation is their personal responsibility, is beyond the control of Mila, and we are unable to provide support for such management.</p> <p>Note</p> <p>An allocation is not a maximal amount of resources that can be used simultaneously, it is a weighting factor of the workload manager to balance jobs. For instance, even though we are allocated 408 GPU-years across all clusters, we can use more or less than 408 GPUs simultaneously depending on the history of usage from our group and other groups using the cluster at a given period of time. Please see the Alliance's documentation for more information on how allocations and resource scheduling are configured for these installations.</p>"},{"location":"Extra_compute_drac/#account-creation","title":"Account Creation","text":"<p>To access the Alliance clusters you have to first create an account at https://ccdb.alliancecan.ca. Use a password with at least 8 characters, mixed case letters, digits and special characters. Later you will be asked to create another password with those rules, and it\u2019s really convenient that the two password are the same.</p> <p>Then, you have to apply for a <code>role</code> at https://ccdb.alliancecan.ca/me/add_role, which basically means telling the Alliance that you are part of the lab so they know which cluster you can have access to, and track your usage.</p> <p>You will be asked for the CCRI (See screenshot below). Please reach out to your sponsor to get the CCRI.</p> <p></p> <p>You will need to wait for your sponsor to accept before being able to login to the Alliance clusters.</p> <p>You should apply to a <code>role</code> using this form for each allocation you can have access to. If, for instance, your supervisor is member of the <code>rrg-bengioy-ad</code> allocation, you should apply using Yoshua Bengio's CCRI, and you should apply separately using your supervisor's CCRI to have access to <code>def-&lt;yoursupervisor&gt;</code>. Ask your supervisor to share these CCRI with you.</p>"},{"location":"Extra_compute_drac/#account-renewal","title":"Account Renewal","text":"<p>All user accounts (Sponsor &amp; Sponsored) have to be renewed annually in order to keep up-to-date information on active accounts and to deactivate unused accounts.</p> <p>To find out how to renew your account or for any other question regarding DRAC's accounts renewal, please head over to their FAQ.</p> <p>If the FAQ is of no help, you can contact DRAC renewal support team at <code>renewals@tech.alliancecan.ca</code> or the general support team at <code>support@tech.alliancecan.ca</code>.</p>"},{"location":"Extra_compute_drac/#clusters","title":"Clusters","text":""},{"location":"Extra_compute_drac/#fir","title":"Fir","text":"<p>Digital Research Alliance of Canada doc</p> <p>The successor to the legacy Cedar cluster. Retains its filesystem. Equipped with H100s.</p>"},{"location":"Extra_compute_drac/#nibi","title":"Nibi","text":"<p>Digital Research Alliance of Canada doc</p> <p>The successor to the legacy Graham cluster. Retains its filesystem. Equipped with H100s.</p>"},{"location":"Extra_compute_drac/#rorqual","title":"Rorqual","text":"<p>Digital Research Alliance of Canada doc</p> <p>The successor to the legacy Beluga cluster. No internet access on compute nodes. Equipped with H100s.</p>"},{"location":"Extra_compute_drac/#trillium","title":"Trillium","text":"<p>Digital Research Alliance of Canada doc</p> <p>The successor to the legacy Niagara cluster. It is principally but not exclusively a CPU cluster. This cluster is not recommended in general. Compute resources in Trillium are not assigned to jobs on a per-CPU, but on a per-node basis. Equipped with H100s.</p>"},{"location":"Extra_compute_drac/#narval","title":"Narval","text":"<p>Digital Research Alliance of Canada doc</p> <p>For some students, Narval might be a good choice if they have already set up there. Narval is the oldest cluster still online, and contains the oldest and smallest GPUs (A100-40GB). The A100 may however be a viable choice for jobs that cannot utilize a full H100.</p>"},{"location":"Extra_compute_drac/#tamia","title":"TamIA","text":"<p>Digital Research Alliance of Canada doc</p> <p>This is a new cluster dedicated to AI jobs for Quebec-area research institutes, within the framework of PAICE (Pan-Canadian AI Compute Environment). Compute resources in TamIA are not assigned to jobs on a per-CPU, but on a per-node basis. Equipped with H100s and H200s.</p> <p>TamIA does not use regular DRAC allocations (<code>--account=rrg-bengioy-ad</code>). Instead, it uses AIP allocations (<code>--account=aip-${PI_NAME}</code>), where <code>${PI_NAME}</code> is the name of your supervising professor. Your professor must add you to their AIP allocation before you will be able to submit jobs.</p>"},{"location":"Extra_compute_drac/#launching-jobs","title":"Launching Jobs","text":"<p>Users must specify the resource allocation Group Name using the flag <code>--account=rrg-bengioy-ad</code>.  To launch a CPU-only job:</p> <pre><code>sbatch --time=1:00:00 --account=rrg-bengioy-ad job.sh\n</code></pre> <p>Note</p> <p>The account name will differ based on your affiliation.</p> <p>To launch a GPU job:</p> <pre><code>sbatch --time=1:00:00 --account=rrg-bengioy-ad --gres=gpu:1 job.sh\n</code></pre> <p>And to get an interactive session, use the <code>salloc</code> command:</p> <pre><code>salloc --time=1:00:00 --account=rrg-bengioy-ad --gres=gpu:1\n</code></pre> <p>The full documentation for jobs launching on Alliance clusters can be found here.</p>"},{"location":"Extra_compute_drac/#drac-storage","title":"DRAC Storage","text":"Storage Path Usage <code>$HOME</code> <code>/home/&lt;user&gt;/</code> Code, specific libraries <code>$HOME/projects</code> <code>/project/rrg-bengioy-ad</code> Compressed raw datasets <code>$SCRATCH</code> <code>/scratch/&lt;user&gt;</code> Processed datasets, experimental results, logs of experiments <code>$SLURM_TMPDIR</code> (on compute node) Temporary job results <p>They are roughly listed in order of increasing performance and optimized for different uses:</p> <ul> <li>The <code>$HOME</code> folder on Lustre is appropriate for code and libraries, which   are small and read once. Do not write experiemental results here!</li> <li>The <code>$HOME/projects</code> folder should only contain compressed raw datasets   (processed datasets should go in <code>$SCRATCH</code>). We have a limit on the   size and number of file in <code>$HOME/projects</code>, so do not put anything else   there.  If you add a new dataset there (make sure it is readable by every   member of the group using <code>chgrp -R rpp-bengioy &lt;dataset&gt;</code>).</li> <li>The <code>$SCRATCH</code> space can be used for short term storage. It has good   performance and large quotas, but is purged regularly (every file that has   not been used in the last 3 months gets deleted, but you receive an email   before this happens).</li> <li><code>$SLURM_TMPDIR</code> points to the local disk of the node on which a job is   running. It should be used to copy the data on the node at the beginning of   the job and write intermediate checkpoints. This folder is cleared after each   job, so results there must be copied to <code>$SCRATCH</code> at the end of a job.</li> </ul> <p>When a series of experiments is finished, results should be transferred back to Mila servers.</p> <p>More details on storage can be found here.</p>"},{"location":"Extra_compute_drac/#modules","title":"Modules","text":"<p>Much software, such as Python or MATLAB, is already compiled and available on DRAC clusters through the <code>module</code> command and its subcommands. Their full documentation can be found here.</p> Command Description <code>module avail</code> Displays all the available modules <code>module load &lt;module&gt;</code> Loads \\&lt;module&gt; <code>module spider &lt;module&gt;</code> Shows specific details about \\&lt;module&gt; <p>In particular, if you with to use <code>Python 3.12</code> you can simply do:</p> <pre><code>module load python/3.12\n</code></pre> <p>Python on the cluster</p> <p>If you wish to use Python on the cluster, we strongly encourage you to read Alliance Python Documentation, and in particular the Pytorch and/or Tensorflow pages.</p> <p>The cluster has many Python packages (or <code>wheels</code>), such already compiled for the cluster. See here for the details. In particular, you can browse the packages by doing:</p> <pre><code>avail_wheels &lt;wheel&gt;\n</code></pre> <p>Such wheels can be installed using pip. Moreover, the most efficient way to use modules on the cluster is to build your environnement inside your job. See the script example below.</p>"},{"location":"Extra_compute_drac/#script-example","title":"Script Example","text":"<p>Here is a <code>sbatch</code> script that follows good practices on Narval and can serve as inspiration for more complicated scripts:</p> <pre><code>#!/bin/bash\n#SBATCH --account=rrg-bengioy-ad         # Yoshua pays for your job\n#SBATCH --cpus-per-task=12               # Ask for 12 CPUs\n#SBATCH --gres=gpu:1                     # Ask for 1 GPU\n#SBATCH --mem=124G                       # Ask for 124 GB of RAM\n#SBATCH --time=03:00:00                  # The job will run for 3 hours\n#SBATCH -o /scratch/&lt;user&gt;/slurm-%j.out  # Write the log in $SCRATCH\n\n# 1. Create your environement locally\nmodule load StdEnv/2023 python/3.12\nvirtualenv --no-download $SLURM_TMPDIR/env\nsource $SLURM_TMPDIR/env/bin/activate\npip install --no-index torch torchvision\n\n# 2. Copy your dataset on the compute node, simultaneously unpacking if\n#    needed (Zip, tar); Alternatively, copy the dataset if it's in an\n#    advanced format like HDF5, or if you can use Zip directly.\nunzip     $SCRATCH/DATASET_CHANGEME.zip    -d $SLURM_TMPDIR\n# tar -xf $SCRATCH/DATASET_CHANGEME.tar.gz -C $SLURM_TMPDIR\n# cp      $SCRATCH/DATASET_CHANGEME.hdf5      $SLURM_TMPDIR\n\n# 3. Launch your job, tell it to save the model in $SLURM_TMPDIR\n#    and look for the dataset into $SLURM_TMPDIR\npython main.py --path $SLURM_TMPDIR --data_path $SLURM_TMPDIR\n\n# 4. Copy whatever you want to save on $SCRATCH\ncp $SLURM_TMPDIR/RESULTS_CHANGEME $SCRATCH\n</code></pre>"},{"location":"Extra_compute_drac/#using-cometml-and-wandb","title":"Using CometML and Wandb","text":"<p>The compute nodes for Narval, Rorqual and Tamia don't have access to the internet, but there is a special module that can be loaded in order to allow training scripts to access some specific servers, which includes the necessary servers for using CometML and Wandb (\"Weights and Biases\").</p> <pre><code>module load httpproxy\n</code></pre> <p>More documentation about this can be found here.</p> <p>Note</p> <p>Be careful when using Wandb with <code>httpproxy</code>. It does not support sending artifacts and wandb's logger will hang in the background when your training is completed, wasting ressources until the job times out. It is recommended to use the offline mode with wandb instead to avoid such waste.</p>"},{"location":"Extra_compute_drac/#faq","title":"FAQ","text":""},{"location":"Extra_compute_drac/#what-are-rgus","title":"What are RGUs?","text":"<p>DRAC uses a concept called <code>RGUs</code> (Reference GPU Units) to measure the allocated GPU resources based on the type of device. This measurement combines the FP32 and FP16 performance of the GPU as well as the memory size. For example, an NVIDIA A100-40G counts has 4.0 RGUs, while a while an H100-80G counts as 12.15 RGUs. This is an improvement over the previous system of counting physical GPU devices and disregarding their actual performance. For example, saying that \"we have 4 GPUs per researcher\" omits which kind of GPUs we're talking about, which is fundamentally important. That proposed RGU measurement can still be improved, but criticisms about it are outside the scope of this document.</p>"},{"location":"Extra_compute_drac/#what-to-do-with-importerror-lib64libmso6-version-glibc_223-not-found","title":"What to do with  <code>ImportError: /lib64/libm.so.6: version GLIBC_2.23 not found</code>?","text":"<p>The structure of the file system is different than a classical Linux, so your code has trouble finding libraries. See how to install binary packages.</p>"},{"location":"Extra_compute_drac/#disk-quota-exceeded-error-on-project-file-systems","title":"Disk quota exceeded error on <code>/project</code> file systems","text":"<p>You have files in <code>/project</code> with the wrong permissions. See how to change permissions.</p>"},{"location":"Getting_started/","title":"Getting started","text":""},{"location":"Getting_started/#getting-started","title":"Getting started","text":"<p>See userguide.</p>"},{"location":"Handbook/","title":"Handbook","text":""},{"location":"Handbook/#ai-tooling-and-methodology-handbook","title":"AI tooling and methodology handbook","text":"<p>This section seeks to provide researchers with insightful articles pertaining to aspects of methodology in their work.</p>"},{"location":"IDT/","title":"Who, what, where is IDT","text":"<p>This section seeks to help Mila researchers understand the mission and role of the IDT team.</p>"},{"location":"IDT/#idts-mission","title":"IDT's mission","text":""},{"location":"IDT/#the-idt-team","title":"The IDT team","text":"<p>See https://mila.quebec/en/mila/team/?cat_id=143</p>"},{"location":"IDT_mission/","title":"IDT mission","text":""},{"location":"IDT_mission/#idts-mission","title":"IDT's mission","text":""},{"location":"IDT_team/","title":"IDT team","text":""},{"location":"IDT_team/#the-idt-team","title":"The IDT team","text":"<p>See https://mila.quebec/en/mila/team/?cat_id=143</p>"},{"location":"Information/","title":"Computing infrastructure and policies","text":"<p>This section seeks to provide factual information and policies on the Mila cluster computing environments.</p> <ul> <li>Roles and authorizations</li> <li>Node profile description</li> <li>Storage</li> <li>Data sharing policies</li> <li>Data Transmission</li> <li>Monitoring</li> </ul>"},{"location":"Information_data_transmission/","title":"Data Transmission","text":"<p>Multiple methods can be used to transfer data to/from the cluster:</p> <ul> <li><code>rsync --bwlimit=10mb</code>; this is the favored method since the bandwidth can   be limited to prevent impacting the usage of the cluster:   rsync</li> <li>Digital Research Alliance of Canada:   Globus</li> </ul>"},{"location":"Information_monitoring/","title":"Monitoring","text":"<p>Every compute node on the Mila cluster has a Netdata monitoring daemon allowing you to get a sense of the state of the node. This information is exposed in two ways:</p> <ul> <li> <p>For every node, there is a web interface from Netdata itself at <code>&lt;node&gt;.server.mila.quebec:19999</code>.   This is accessible only when using the Mila wifi or through SSH tunnelling.</p> </li> <li> <p>SSH tunnelling: on your local machine, run</p> <ul> <li><code>ssh -L 19999:&lt;node&gt;.server.mila.quebec:19999 -p 2222 login.server.mila.quebec</code></li> <li>or <code>ssh -L 19999:&lt;node&gt;.server.mila.quebec:19999 mila</code> if you have   already setup your SSH Login,</li> <li>then open http://localhost:19999 in your browser.</li> <li>The Mila dashboard at dashboard.server.mila.quebec   exposes aggregated statistics with the use of grafana.   These are collected internally to an instance of prometheus.</li> </ul> </li> </ul> <p>In both cases, those graphs are not editable by individual users, but they provide valuable insight into the state of the whole cluster or the individual nodes. One of the important uses is to collect data about the health of the Mila cluster and to sound the alarm if outages occur (e.g. if the nodes crash or if GPUs mysteriously become unavailable for SLURM).</p>"},{"location":"Information_monitoring/#example-with-netdata-on-cn-c001","title":"Example with Netdata on cn-c001","text":"<p>For example, if we have a job running on <code>cn-c001</code>, we can type <code>cn-c001.server.mila.quebec:19999</code> in a browser address bar and the following page will appear.</p> <p></p>"},{"location":"Information_monitoring/#example-watching-the-cpuramgpu-usage","title":"Example watching the CPU/RAM/GPU usage","text":"<p>Given that compute nodes are generally shared with other users who are also running jobs at the same time and consuming resources, this is not generally a good way to profile your code in fine details. However, it can still be a very useful source of information for getting an idea of whether the machine that you requested is being used in its full capacity.</p> <p>Given how expensive the GPUs are, it generally makes sense to try to make sure that this resources is always kept busy.</p> <ul> <li>CPU<ul> <li>iowait (pink line): High values means your model is waiting on IO a lot (disk or network).</li> </ul> </li> </ul> <p></p> <ul> <li>CPU RAM<ul> <li>You can see how much CPU RAM is being used by your script in practice,   considering the amount that you requested (e.g. <code>sbatch --mem=8G ...</code>).</li> <li>GPU usage is generally more important to monitor than CPU RAM.   You should not cut it so close to the limit that your experiments randomly fail   because they run out of RAM. However, you should not request blindly 32GB of RAM   when you actually require only 8GB.</li> </ul> </li> </ul> <p></p> <ul> <li>GPU<ul> <li>Monitors the GPU usage using an nvidia-smi plugin for Netdata.</li> <li>Under the plugin interface, select the GPU number which was allocated to   you. You can figure this out by running <code>echo $SLURM_JOB_GPUS</code> on the   allocated node or, if you have the job ID,   <code>scontrol show -d job YOUR_JOB_ID | grep 'GRES'</code> and checking <code>IDX</code></li> <li>You should make sure you use the GPUs to their fullest capacity.</li> <li>Select the biggest batch size if possible to increase GPU memory usage and   the GPU computational load.</li> <li>Spawn multiple experiments if you can fit many on a single GPU.   Running 10 independent MNIST experiments on a single GPU will probably take   less than 10x the time to run a single one. This assumes that you have more   experiments to run, because nothing is gained by gratuitously running experiments.</li> <li>You can request a less powerful GPU and leave the more powerful GPUs   to other researchers who have experiments that can make best use of them.   Sometimes you really just need a k80 and not a v100.</li> </ul> </li> </ul> <p></p> <ul> <li>Other users or jobs<ul> <li>If the node seems unresponsive or slow,   it may be useful to check what other tasks are   running at the same time on that node.   This should not be an issue in general,   but in practice it is useful to be able to   inspect this to diagnose certain problems.</li> </ul> </li> </ul> <p></p>"},{"location":"Information_monitoring/#example-with-mila-dashboard","title":"Example with Mila dashboard","text":""},{"location":"Information_nodes/","title":"Node profile description","text":"Name GPU Model Mem # CPUs Sockets Cores/Socket Threads/Core Memory (GB) TmpDisk (TB) Arch Slurm Features GPU Compute Nodes cn-a[001-011] RTX8000 48 8 40 2 20 1 384 3.6 x86_64 turing,48gb cn-b[001-005] V100 32 8 40 2 20 1 384 3.6 x86_64 volta,nvlink,32gb cn-c[001-040] RTX8000 48 8 64 2 32 1 384 3 x86_64 turing,48gb cn-g[001-029] A100 80 4 64 2 32 1 1024 7 x86_64 ampere,nvlink,80gb cn-i001 A100 80 4 64 2 32 1 1024 3.6 x86_64 ampere,80gb cn-j001 A6000 48 8 64 2 32 1 1024 3.6 x86_64 ampere,48gb cn-k[001-004] A100 40 4 48 2 24 1 512 3.6 x86_64 ampere,nvlink,40gb cn-l[001-091] L40S 48 4 48 2 24 1 1024 7 x86_64 lovelace,48gb cn-n[001-002] H100 80 8 192 2 96 1 2048 35 x86_64 hopper,nvlink,80gb DGX Systems cn-d[001-002] A100 40 8 128 2 64 1 1024 14 x86_64 ampere,nvlink,dgx,40gb cn-d[003-004] A100 80 8 128 2 64 1 2048 28 x86_64 ampere,nvlink,dgx,80gb cn-e[002-003] V100 32 8 40 2 20 1 512 7 x86_64 volta,nvlink,dgx,32gb CPU Compute Nodes cn-f[001-004] - - - 32 1 32 1 256 10 x86_64 rome cn-h[001-004] - - - 64 2 32 1 768 7 x86_64 milan cn-m[001-004] - - - 96 2 48 1 1024 7 x86_64 sapphire"},{"location":"Information_nodes/#special-nodes-and-outliers","title":"Special nodes and outliers","text":""},{"location":"Information_nodes/#dgx-a100","title":"DGX A100","text":"<p>DGX A100 nodes are NVIDIA appliances with 8 NVIDIA A100 Tensor Core GPUs. Each GPU has either 40 GB or 80 GB of memory, for a total of 320 GB or 640 GB per appliance. The GPUs are interconnected via 6 NVSwitches which allow for 600 GB/s point-to-point bandwidth (unidirectional) and a full bisection bandwidth of 4.8 TB/s (bidirectional). See the table above for the specifications of each appliance.</p> <p>In order to run jobs on a DGX A100 with 40GB GPUs, add the flags below to your Slurm commands:</p> <pre><code>--gres=gpu:a100:&lt;number&gt; --constraint=\"dgx&amp;ampere\"\n</code></pre> <p>In order to run jobs on a DGX A100 with 80GB GPUs, add the flags below to your Slurm commands:</p> <pre><code>--gres=gpu:a100l:&lt;number&gt; --constraint=\"dgx&amp;ampere\"\n</code></pre>"},{"location":"Information_roles_and_resources/","title":"Roles and authorizations","text":"<p>There are mainly two types of researchers statuses at Mila :</p> <ol> <li>Core researchers</li> <li>Affiliated researchers</li> </ol> <p>This is determined by Mila policy. Core researchers have access to the Mila computing cluster. See your supervisor's Mila status to know what is your own status.</p>"},{"location":"Information_roles_and_resources/#overview-of-available-computing-resources-at-mila","title":"Overview of available computing resources at Mila","text":"<p>The Mila cluster is to be used for regular development and relatively small number of jobs (&lt; 5). It is a heterogeneous cluster. It uses SLURM to schedule jobs.</p>"},{"location":"Information_roles_and_resources/#mila-cluster-versus-digital-research-alliance-of-canada-clusters","title":"Mila cluster versus Digital Research Alliance of Canada clusters","text":"<p>There are a lot of commonalities between the Mila cluster and the clusters from Digital Research Alliance of Canada (the Alliance). At the time being, the Alliance clusters where we have a large allocation of resources are <code>beluga</code>, <code>cedar</code>, <code>graham</code> and <code>narval</code>. We also have comparable computational resources in the Mila cluster, with more to come.</p> <p>The main distinguishing factor is that we have more control over our own cluster than we have over the ones at the Alliance. Notably, also, the compute nodes in the Mila cluster all have unrestricted access to the Internet, which is not the case in general for the Alliance clusters (although <code>cedar</code> does allow it).</p> <p>At the current time of this writing (June 2021), Mila students are advised to use a healthy diet of a mix of Mila and Alliance clusters. This is especially true in times when your favorite cluster is oversubscribed, because you can easily switch over to a different one if you are used to it.</p>"},{"location":"Information_roles_and_resources/#guarantees-about-one-gpu-as-absolute-minimum","title":"Guarantees about one GPU as absolute minimum","text":"<p>There are certain guarantees that the Mila cluster tries to honor when it comes to giving at minimum one GPU per student, all the time, to be used in interactive mode. This is strictly better than \"one GPU per student on average\" because it's a floor meaning that, at any time, you should be able to ask for your GPU, right now, and get it (although it might take a minute for the request to be processed by SLURM).</p> <p>Interactive sessions are possible on the Alliance clusters, and there are generally special rules that allow you to get resources more easily if you request them for a very short duration (for testing code before queueing long jobs). You do not get the same guarantee as on the Mila cluster, however.</p>"},{"location":"Information_sharing_policies/","title":"Data sharing policies","text":"<p>Note</p> <p><code>/network/scratch</code> aims to support Access Control Lists (ACLs) to allow collaborative work on rapidly changing data, e.g. work in process datasets, model checkpoints, etc.</p> <p><code>/network/projects</code> aims to offer a collaborative space for long-term projects. Data that should be kept for a longer period than 90 days can be stored in that location but first a request to Mila's helpdesk has to be made to create the project directory.</p>"},{"location":"Information_storage/","title":"Storage","text":"Path Performance Usage Quota (Space/Files) Backup Auto-cleanup <code>/network/datasets/</code> High Curated raw datasets (read only) \u2014 \u2014 \u2014 <code>/network/weights/</code> High Curated models weights (read only) \u2014 \u2014 \u2014 <code>$HOME</code> or <code>/home/mila/&lt;u&gt;/&lt;username&gt;/</code> Low Personal user space; specific libraries, code, binaries 100GB/1000K Daily no <code>$SCRATCH</code> or <code>/network/scratch/&lt;u&gt;/&lt;username&gt;/</code> High Temporary job results; processed datasets; optimized for small files 5TB/no no 90 days <code>$SLURM_TMPDIR</code> Highest High speed disk for temporary job results no/no no at job end <code>/network/projects/&lt;groupname&gt;/</code> Fair Shared space for collaboration; long-term project storage 1TB/1000K Daily no <code>$ARCHIVE</code> or <code>/network/archive/&lt;u&gt;/&lt;username&gt;/</code> Low Long-term personal storage 5TB no no <p>Note</p> <p>The <code>$HOME</code> file system is backed up once a day. For any file restoration request, file a request to Mila's IT support with the path to the file or directory to restore, with the required date.</p>"},{"location":"Information_storage/#home","title":"$HOME","text":"<p><code>$HOME</code> is appropriate for codes and libraries which are small and read once, as well as the experimental results that would be needed at a later time (e.g. the weights of a network referenced in a paper).</p> <p>Quotas are enabled on <code>$HOME</code> for both disk capacity (blocks) and number of files (inodes). The limits for blocks and inodes are respectively 100GiB and 1 million per user. The command to check the quota usage from a login node is:</p> <pre><code>disk-quota\n</code></pre>"},{"location":"Information_storage/#scratch","title":"$SCRATCH","text":"<p><code>$SCRATCH</code> can be used to store processed datasets, work in progress datasets or temporary job results. Its block size is optimized for small files which minimizes the performance hit of working on extracted datasets.</p> <p>Auto-cleanup</p> <p>This file system is cleared on a daily basis; files not used for more than 90 days will be deleted. This period can be shortened when the file system usage is above 90%.</p> <p>Quotas are enabled on <code>$SCRATCH</code> for disk capacity (blocks). The limit is 5TiB. There is no limit in the number of files (inodes). The command to check the quota usage from a login node is:</p> <pre><code>disk-quota\n</code></pre>"},{"location":"Information_storage/#slurm_tmpdir","title":"$SLURM_TMPDIR","text":"<p><code>$SLURM_TMPDIR</code> points to the local disk of the node on which a job is running. It should be used to copy the data on the node at the beginning of the job and write intermediate checkpoints. This folder is cleared after each job.</p>"},{"location":"Information_storage/#projects","title":"projects","text":"<p><code>projects</code> can be used for collaborative projects. It aims to ease the sharing of data between users working on a long-term project.</p> <p>Quotas are enabled on <code>projects</code> for both disk capacity (blocks) and number of files (inodes). The limits for blocks and inodes are respectively 1TiB and 1 million per group.</p> <p>Note</p> <p>It is possible to request higher quota limits if the project requires it. File a request to Mila's IT support.</p>"},{"location":"Information_storage/#archive","title":"$ARCHIVE","text":"<p><code>$ARCHIVE</code> purpose is to store data other than datasets that has to be kept long-term (e.g.  generated samples, logs, data relevant for paper submission).</p> <p><code>$ARCHIVE</code> is only available on the login nodes and CPU-only nodes. Because this file system is tuned for large files, it is recommended to archive your directories. For example, to archive the results of an experiment in <code>$SCRATCH/my_experiment_results/</code>, run the commands below from a login node:</p> <pre><code>cd $SCRATCH\ntar cJf $ARCHIVE/my_experiment_results.tar.xz --xattrs my_experiment_results\n</code></pre> <p>Disk capacity quotas are enabled on <code>$ARCHIVE</code>. The soft limit per user is 5TB, the hard limit is 5.1TB. The grace time is 7 days. This means that one can use more than 5TB for 7 days before the file system enforces quota. However, it is not possible to use more than 5.1TB. The command to check the quota usage from a login node is <code>df</code>:</p> <pre><code>df -h $ARCHIVE\n</code></pre> <p>Note</p> <p>There is NO backup of this file system.</p>"},{"location":"Information_storage/#datasets","title":"datasets","text":"<p><code>datasets</code> contains curated datasets to the benefit of the Mila community. To request the addition of a dataset or a preprocessed dataset you think could benefit the research of others, you can fill the datasets form. Datasets can also be browsed from the web : Mila Datasets</p> <p>Datasets in <code>datasets/restricted</code> are restricted and require an explicit request to gain access. Please submit a support ticket mentioning the dataset's access group (ex.: <code>scannet_users</code>), your cluster's username and the approbation of the group owner. You can find the dataset's access group by listing the content of <code>/network/datasets/restricted</code> with the ls command.</p> <p>Those datasets are mirrored to the Alliance clusters in <code>~/projects/rrg-bengioy-ad/data/curated/</code> if they follow Digital Research Alliance of Canada's good practices on data. To list the local datasets on an Alliance cluster, you can execute the following command:</p> <pre><code>ssh [CLUSTER_LOGIN] -C \"projects/rrg-bengioy-ad/data/curated/list_datasets_cc.sh\"\n</code></pre>"},{"location":"Information_storage/#weights","title":"weights","text":"<p><code>weights</code> contains curated models weights to the benefit of the Mila community.  To request the addition of a weight you think could benefit the research of others, you can fill the weights form.</p> <p>Weights in <code>weights/restricted</code> are restricted and require an explicit request to gain access. Please submit a support ticket mentioning the weights's access group (ex.: <code>NAME_OF_A_RESTRICTED_MODEL_WEIGHTS_users</code>), your cluster's username and the approbation of the group owner. You can find the weights's access group by listing the content of <code>/network/weights/restricted</code> with the ls command.</p>"},{"location":"Purpose/","title":"Purpose of this documentation","text":"<p>This documentation aims to cover the information required to run scientific and data-intensive computing tasks at Mila and the available resources for its members.</p> <p>It also aims to be an outlet for sharing know-how, tips and tricks and examples from the IDT team to the Mila researcher community.</p>"},{"location":"Purpose/#intended-audience","title":"Intended audience","text":"<p>This documentation is mainly intended for Mila researchers having access to the Mila cluster. This access is determined by your researcher status. See Roles and authorizations for more information. The core of the information with this purpose can be found in the following section: Computing infrastructure and policies.</p> <p>However, we also aim to provide more general information which can be useful outside the scope of using the Mila cluster. For instance, more general theory on computational considerations and such. In this perspective, we hope the documentation can be of use for all AI researchers.</p>"},{"location":"Purpose_audience/","title":"Purpose audience","text":""},{"location":"Purpose_audience/#intended-audience","title":"Intended audience","text":"<p>This documentation is mainly intended for Mila researchers having access to the Mila cluster. This access is determined by your researcher status. See Roles and authorizations for more information. The core of the information with this purpose can be found in the following section: Computing infrastructure and policies.</p> <p>However, we also aim to provide more general information which can be useful outside the scope of using the Mila cluster. For instance, more general theory on computational considerations and such. In this perspective, we hope the documentation can be of use for all AI researchers.</p>"},{"location":"Theory_cluster/","title":"Theory cluster","text":"<p>..    Keep it THEORETICAL folks. If you are starting to write an opiniated HOWTO,    it might be better to set that kind of information aside for the handbook.</p>"},{"location":"Theory_cluster_batch_scheduling/","title":"The workload manager","text":""},{"location":"Theory_cluster_batch_scheduling/#the-workload-manager","title":"The workload manager","text":"<p>On a cluster, users don't have direct access to the compute nodes but instead connect to a login node and add jobs to the workload manager queue. Whenever there are resources available to execute these jobs they will be allocated to a compute node and run, which can be immediately or after a wait of up to several days.</p>"},{"location":"Theory_cluster_batch_scheduling/#anatomy-of-a-job","title":"Anatomy of a job","text":"<p>A job is comprised of a number of steps that will run one after the other. This is done so that you can schedule a sequence of processes that can use the results of the previous steps without having to manually interact with the scheduler.</p> <p>Each step can have any number of tasks which are groups of processes that can be scheduled independently on the cluster but can run in parallel if there are resources available. The distinction between steps and tasks is that multiple tasks, if they are part of the same step, cannot depend on results of other tasks because there are no guarantees on the order in which they will be executed.</p> <p>Finally each process group is the basic unit that is scheduled in the cluster. It comprises of a set of processes (or threads) that can run on a number of resources (CPU, GPU, RAM, ...) and are scheduled together as a unit on one or more machines.</p> <p>Each of these concepts lends itself to a particular use. For multi-gpu training in AI workloads you would use one task per GPU for data paralellism or one process group if you are doing model parallelism. Hyperparameter optimisation can be done using a combination of tasks and steps but is probably better left to a framework outside of the scope of the workload manager.</p> <p>If this all seems complicated, you should know that all these things do not need to always be used. It is perfectly acceptable to sumbit jobs with a single step, a single task and a single process.</p>"},{"location":"Theory_cluster_batch_scheduling/#understanding-the-queue","title":"Understanding the queue","text":"<p>The available resources on the cluster are not infinite and it is the workload manager's job to allocate them. Whenever a job request comes in and there are not enough resources available to start it immediately, it will go in the queue.</p> <p>Once a job is in the queue, it will stay there until another job finishes and then the workload manager will try to use the newly freed resources with jobs from the queue. The exact order in which the jobs will start is not fixed, because it depends on the local policies which can take into account the user priority, the time since the job was requested, the amount of resources requested and possibly other things. There should be a tool that comes with the manager where you can see the status of your queued jobs and why they remain in the queue.</p>"},{"location":"Theory_cluster_batch_scheduling/#about-partitions","title":"About partitions","text":"<p>The workload manager will divide the cluster into partitions according to the configuration set by the admins. A partition is a set of machines typically reserved for a particular purpose. An example might be CPU-only machines for preprocessing setup as a separate partition. It is possible for multiple partitions to share resources.</p> <p>There will always be at least one partition that is the default partition in which jobs without a specific request will go. Other partitions can be requested, but might be restricted to a group of users, depending on policy.</p> <p>Partitions are useful for a policy standpoint to ensure efficient use of the cluster resources and avoid using up too much of one resource type blocking use of another. They are also useful for heterogenous clusters where different hardware is mixed in and not all software is compatible with all of it (for example x86 and POWER cpus).</p>"},{"location":"Theory_cluster_batch_scheduling/#exceding-limits-preemption-and-grace-periods","title":"Exceding limits (preemption and grace periods)","text":"<p>To ensure a fair share of the computing resources for all, the workload manager establishes limits on the amount of resources that a single user can use at once. These can be hard limits which prevent running jobs when you go over or soft limits which will let you run jobs, but only until some other job needs the resources.</p> <p>Admin policy will determine what those exact limits are for a particular cluster or user and whether they are hard or soft limits.</p> <p>The way soft limits are enforced is using preemption, which means that when another job with higher priority needs the resources that your job is using, your job will receive a signal that it needs to save its state and exit. It will be given a certain amount of time to do this (the grace period, which may be 0s) and then forcefully terminated if it is still running.</p> <p>Depending on the workload manager in use and the cluster configuration a job that is preempted like this may be automatically rescheduled to have a chance to finish or it may be up to the job to reschedule itself.</p> <p>The other limit you can encounter with a job that goes over its declared limits. When you schedule a job, you declare how much resources it will need (RAM, CPUs, GPUs, ...). Some of those may have default values and not be explicitely defined. For certain types of devices, like GPUs, access to units over your job limit is made unavailable. For others, like RAM, usage is monitored and your job will be terminated if it goes too much over. This makes it important to ensure you estimate resource usage accurately.</p>"},{"location":"Theory_cluster_batch_scheduling/#mila-information","title":"Mila information","text":"<p>Mila as well as  Digital Research Alliance of Canada  use the workload manager Slurm to schedule and allocate resources on their infrastructure.</p> <p>Slurm client commands are available on the login nodes for you to submit jobs to the main controller and add your job to the queue. Jobs are of 2 types: batch jobs and interactive jobs.</p> <p>For practical examples of Slurm commands on the Mila cluster, see Running your code.</p>"},{"location":"Theory_cluster_data/","title":"Processing data","text":""},{"location":"Theory_cluster_data/#processing-data","title":"Processing data","text":"<p>For processing large amounts of data common for deep learning, either for dataset preprocessing or training, several techniques exist. Each has typical uses and limitations.</p>"},{"location":"Theory_cluster_data/#data-parallelism","title":"Data parallelism","text":"<p>The first technique is called data parallelism (aka task parallelism in formal computer science). You simply run lots of processes each handling a portion of the data you want to process. This is by far the easiest technique to use and should be favored whenever possible. A common example of this is hyperparameter optimisation.</p> <p>For really small computations the time to setup multiple processes might be longer than the processing time and lead to waste. This can be addressed by bunching up some of the processes together by doing sequential processing of sub-partitions of the data.</p> <p>For the cluster systems it is also inadvisable to launch thousands of jobs and even if each job would run for a reasonable amount of time (several minutes at minimum), it would be best to make larger groups until the amount of jobs is in the low hundreds at most.</p> <p>Finally another thing to keep in mind is that the transfer bandwidth is limited between the filesystems (see Filesystem concerns) and the compute nodes and if you run too many jobs using too much data at once they may end up not being any faster because they will spend their time waiting for data to arrive.</p>"},{"location":"Theory_cluster_data/#model-parallelism","title":"Model parallelism","text":"<p>The second technique is called model parallelism (which doesn't have a single equivalent in formal computer science). It is used mostly when a single instance of a model will not fit in a computing resource (such as the GPU memory being too small for all the parameters).</p> <p>In this case, the model is split into its constituent parts, each processed independently and their intermediate results communicated with each other to arrive at a final result.</p> <p>This is generally harder but necessary to work with larger, more powerful models like GPT.</p>"},{"location":"Theory_cluster_data/#communication-concerns","title":"Communication concerns","text":"<p>The main difference of these two approaches is the need for communication between the multiple processes. Some common training methods, like stochastic gradient descent sit somewhere between the two, because they require some communication, but not a lot. Most people classify it as data parallelism since it sits closer to that end.</p> <p>In general for data parallelism tasks or tasks that communicate infrequently it doesn't make a lot of difference where the processes sit because the communication bandwidth and latency will not have a lot of impact on the time it takes to complete the job.  The individual tasks can generally be scheduled independently.</p> <p>On the contrary for model parallelism you need to pay more attention to where your tasks are.  In this case it is usually required to use the facilities of the workload manager to group the tasks so that they are on the same machine or machines that are closely linked to ensure optimal communication.  What is the best allocation depends on the specific cluster architecture available and the technologies it support (such as InfiniBand , RDMA , NVLink  or others)</p>"},{"location":"Theory_cluster_data/#filesystem-concerns","title":"Filesystem concerns","text":"<p>When working on a cluster, you will generally encounter several different filesystems.  Usually there will be names such as 'home', 'scratch', 'datasets', 'projects', 'tmp'.</p> <p>The reason for having different filesystems available instead of a single giant one is to provide for different use cases. For example, the 'datasets' filesystem would be optimized for fast reads but have slow write performance. This is because datasets are usually written once and then read very often for training.</p> <p>Different filesystems have different performance levels. For instance, backed up filesystems (such as <code>$PROJECT</code> in Digital Research Alliance of Canada clusters) provide more space and can handle large files but cannot sustain highly parallel accesses typically required for high speed model training.</p> <p>The set of filesystems provided by the cluster you are using should be detailed in the documentation for that cluster and the names can differ from those above. You should pay attention to their recommended use case in the documentation and use the appropriate filesystem for the appropriate job. There are cases where a job ran hundreds of times slower because it tried to use a filesystem that wasn't a good fit for the job.</p> <p>One last thing to pay attention to is the data retention policy for the filesystems. This has two subpoints: how long is the data kept for, and are there backups.</p> <p>Some filesystems will have a limit on how long they keep their files. Typically the limit is some number of days (like 90 days) but can also be 'as long as the job runs' for some.</p> <p>As for backups, some filesystems will not have a limit for data, but will also not have backups. For those it is important to maintain a copy of any crucial data somewhere else. The data will not be purposefully deleted, but the filesystem may fail and lose all or part of its data. If you have any data that is crucial for a paper or your thesis keep an additional copy of it somewhere else.</p>"},{"location":"Theory_cluster_parts/","title":"What is a computer cluster?","text":""},{"location":"Theory_cluster_parts/#what-is-a-computer-cluster","title":"What is a computer cluster?","text":"<p>A computer cluster is a set of loosely or tightly connected computers that work together so that, in many respects, they can be viewed as a single system.</p>"},{"location":"Theory_cluster_parts/#parts-of-a-computing-cluster","title":"Parts of a computing cluster","text":"<p>To provide high performance computation capabilities, clusters can combine hundreds to thousands of computers, called nodes, which are all inter-connected with a high-performance communication network. Most nodes are designed for high-performance computations, but clusters can also use specialized nodes to offer parallel file systems, databases, login nodes and even the cluster scheduling functionality as pictured in the image below.</p> <p></p> <p>We will overview the different types of nodes which you can encounter on a typical cluster.</p>"},{"location":"Theory_cluster_parts/#the-login-nodes","title":"The login nodes","text":"<p>To execute computing processes on a cluster, you must first connect to a cluster and this is accomplished through a login node. These so-called login nodes are the entry point to most clusters.</p> <p>Another entry point to some clusters such as the Mila cluster is the JupyterHub web interface, but we'll read about that later. For now let's return to the subject of this section: Login nodes. To connect to these, you would typically use a remote shell connection. The most usual tool to do so is SSH. You'll hear and read a lot about this tool. Imagine it as a very long (and somewhat magical) extension cord which connects the computer you are using now, such as your laptop, to a remote computer's terminal shell. You might already know what a terminal shell is if you ever used the command line.</p>"},{"location":"Theory_cluster_parts/#the-compute-nodes","title":"The compute nodes","text":"<p>In the field of artificial intelligence, you will usually be on the hunt for GPUs. In most clusters, the compute nodes are the ones with GPU capacity.</p> <p>While there is a general paradigm to tend towards a homogeneous configuration for nodes, this is not always possible in the field of artificial intelligence as the hardware evolve rapidly as is being complemented by new hardware and so on. Hence, you will often read about computational node classes. Some of which might have different GPU models or even no GPU at all. For the Mila cluster you will find this information in the Node profile description section. For now, you should note that is important to keep in mind that you should be aware of which nodes your code is running on.  More on that later.</p>"},{"location":"Theory_cluster_parts/#the-storage-nodes","title":"The storage nodes","text":"<p>Some computers on a cluster function to only store and serve files.  While the name of these computers might matter to some, as a user, you'll only be concerned about the path to the data. More on that in the Processing data section.</p>"},{"location":"Theory_cluster_parts/#different-nodes-for-different-uses","title":"Different nodes for different uses","text":"<p>It is important to note here the difference in intended uses between the compute nodes and the login nodes. While the compute nodes are meant for heavy computation, the login nodes are not.</p> <p>The login nodes however are used by everyone who uses the cluster and care must be taken not to overburden these nodes. Consequently, only very short and light processes should be run on these otherwise the cluster may become inaccessible. In other words, please refrain from executing long or compute intensive processes on login nodes because it affects all other users. In some cases, you will also find that doing so might get you into trouble.</p>"},{"location":"Theory_cluster_software_deps/","title":"Software on the cluster","text":""},{"location":"Theory_cluster_software_deps/#software-on-the-cluster","title":"Software on the cluster","text":"<p>This section aims to raise awareness to problems one can encounter when trying to run a software on different computers and how this is dealt with on typical computation clusters.</p> <p>The Mila cluster and the Digital Research Alliance of Canada clusters both provide various useful software and computing environments, which can be activated through the module system. Alternatively, you may build containers with your desired software and run them on compute nodes.</p> <p>Regarding Python development, we recommend using virtual environments to install Python packages in isolation.</p>"},{"location":"Theory_cluster_software_deps/#cluster-software-modules","title":"Cluster software modules","text":"<p>Modules are small files which modify your environment variables to point to specific versions of various software and libraries. For instance, a module might provide the <code>python</code> command to point to Python 3.7, another might activate CUDA version 11.0, another might provide the <code>torch</code> package, and so on.</p> <p>For more information, see The module command.</p>"},{"location":"Theory_cluster_software_deps/#containers","title":"Containers","text":"<p>Containers are a special form of isolation of software and its dependencies. A container is essentially a lightweight virtual machine: it encapsulates a virtual file system for a full OS installation, as well as a separate network and execution environment.</p> <p>For example, you can create an Ubuntu container in which you install various packages using <code>apt</code>, modify settings as you would as a root user, and so on, but without interfering with your main installation. Once built, a container can be run on any compatible system.</p> <p>For more information, see Using containers.</p>"},{"location":"Theory_cluster_software_deps/#python-virtual-environments","title":"Python Virtual environments","text":"<p>A virtual environment in Python is a local, isolated environment in which you can install or uninstall Python packages without interfering with the global environment (or other virtual environments). In order to use a virtual environment, you first have to activate it.</p> <p>For more information, see Virtual environments.</p>"},{"location":"Theory_cluster_unix/","title":"Unix","text":""},{"location":"Theory_cluster_unix/#unix","title":"UNIX","text":"<p>All clusters typically run on GNU/Linux distributions. Hence a minimum knowledge of GNU/Linux and BASH is usually required to use them. See the following tutorial  for a rough guide on getting started with Linux.</p>"},{"location":"Userguide_HPO/","title":"Userguide HPO","text":""},{"location":"Userguide_HPO/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<p>Hyperparameter optimization is very easy to parallelize as each trial (unique set of hyperparameters) are independant of each other. The easiest way is to launch as many jobs as possible each trying a different set of hyperparameters and reporting their results back to a synchronized location (database).</p> <p>You will need to estimate how much resources your training requires and update the provided example to fit. In the example below we use 100 tasks with each 4 CPU cores and one GPU. Each task will run 4 trainings in parallel on the same GPU to maximize its utilization.</p> <p>This means there could be 400 set of hyperparameters being worked on in parallel across 100 GPUs.</p> <p>The easiest way to run an hyperparameter search on the cluster is simply to use a job array, which will launch the same job n times. Your HPO library will generate different parameters to try for each instances.</p> <pre><code>sbatch --array=1-100 --gres=gpu:1 --cpus-per-gpu=2 --mem-per-gpu=16G scripts/hpo_launcher.sh train.py\n</code></pre>"},{"location":"Userguide_HPO/#configure-orion","title":"Configure Orion","text":"<p>Orion  is an asynchronous framework for black-box function optimization developped at Mila.</p> <p>Its purpose is to serve as a meta-optimizer for machine learning models and training, as well as a flexible experimentation platform for large scale asynchronous optimization procedures.</p> <p>Orion saves all the results of its optimization process in a database, by default it is using a local database on a shared filesystem named <code>pickleddb</code>. You will need to specify its location and the name of your experiment. Optionally you can configure workers which will run in parallel to maximize resource usage.</p> <p>.. code-block:: bash</p> <p>cat &gt; $ORION_CONFIG &lt;&lt;- EOM        experiment:            name: ${EXPERIMENT_NAME}            algorithms:                hyperband:                    seed: null            max_broken: 10</p> <pre><code>   worker:\n       n_workers: $SBATCH_CPUS_PER_GPU\n       pool_size: 0\n       executor: joblib\n       heartbeat: 120\n       max_broken: 10\n       idle_timeout: 60\n\n   database:\n       host: $SCRATCH/${EXPERIMENT_NAME}_orion.pkl\n       type: pickleddb\n</code></pre> <p>EOM</p>"},{"location":"Userguide_HPO/#define-the-search-space","title":"Define the search space","text":"<p>We now need to define a search space that Orion will go through. The search space is going to be dependent on your model, optimizer and others. You find below an example of a common set of hyperparameters.</p> <p>.. code-block:: bash</p> <p># Define your hyperparameter search space    cat &gt; $SPACE_CONFIG &lt;&lt;- EOM       {          \"epochs\": \"orion~fidelity(1, 100, base=2)\",          \"lr\": \"orion~loguniform(1e-5, 1.0)\",          \"weight_decay\": \"orion~loguniform(1e-10, 1e-3)\",          \"momentum\": \"orion~loguniform(0.9, 1.0)\"       }    EOM</p>"},{"location":"Userguide_HPO/#run-orion-hunt","title":"Run Orion hunt","text":"<p>Now that Orion is configured and has a search space we can execute orion-hunt which will start training with a different set of hyperparameters for each worker.</p> <p>.. code-block:: bash</p> <p>orion hunt --config $ORION_CONFIG python train.py --config $SPACE_CONFIG --arg1 value1</p> <p>NOTE Do not forget to move orion database out of <code>scratch</code> once the experiment is done.</p>"},{"location":"Userguide_HPO/#full-example","title":"Full Example","text":"<p>.. code-block:: bash</p> <p>sbatch --array=1-100 --gres=gpu:1 --cpus-per-gpu=2 --mem-per-gpu=16G hpo_launcher.sh train.py</p> <p>.. literalinclude:: /examples/hpo_launcher.sh    :language: bash    :linenos:</p> <p>NOTE Network Architecture Search (NAS) is a subset of hyperparameter optimization where hyperparameters are used to generate a network. You can use the same technique for both NAS and HPO.</p>"},{"location":"Userguide_cluster_access/","title":"Userguide cluster access","text":"<p>To access the Mila Cluster clusters, you will need a Mila account. Please contact Mila systems administrators if you don't have it already. Our IT support service is available here: https://it-support.mila.quebec/</p> <p>You will also need to complete and return an IT Onboarding Training to get access to the cluster.  Please refer to the Mila Intranet for more informations: https://sites.google.com/mila.quebec/mila-intranet/it-infrastructure/it-onboarding-training</p> <p>Important</p> <p>Your access to the Cluster is granted based on your status at Mila (for students, your status is the same as your main supervisor' status), and on the duration of your stay, set during the creation of your account.  The following have access to the cluster : Current Students of Core Professors - Core Professors - Staff</p>"},{"location":"Userguide_comet/","title":"Comet","text":"<p>Students supervised by core professors are elligible to the Mila organization on Comet. To request access, write to it-support@mila.quebec. Then please follow the guidelines below to get your account created within Mila's organization. This account will be independant from your personal account if you already have one.</p>"},{"location":"Userguide_comet/#logging-in-for-the-first-time","title":"Logging in for the first time","text":"<p>To access mila-org, you need to login using the url https://comet.mila.quebec/. On first login, one of the following will apply:</p> <ol> <li> <p>If you have no Comet account or another account with an email address other than    @mila.quebec: Comet will create a new account.</p> </li> <li> <p>If you have an account with our email address @mila.quebec. Comet will link    your account to mila-org.</p> </li> </ol>"},{"location":"Userguide_containers/","title":"Using containers","text":"<p>Podman containers are now available as tech preview on the Mila cluster without root privileges using podman.</p> <p>Generally any command-line argument accepted by docker will work with podman. This means that you can mostly use the docker examples you find on the web by replacing <code>docker</code> with <code>podman</code> in the command line.</p> <p>Note</p> <p>Complete Podman Documentation: https://docs.podman.io/en/stable/</p>"},{"location":"Userguide_containers/#using-in-slurm","title":"Using in SLURM","text":"<p>To use podman you can just use the <code>podman</code> command in either a batch script or an interactive job.</p> <p>One difference in configuration is that for certain technical reasons all the storage for podman (images, containers, ...) is on a job-specific location and will be lost after the job is complete or preempted. If you have data that must be preseved across jobs, you can mount a local folder inside the container, such as <code>$SCRATCH</code> or your home to save data.</p> <pre><code>$ podman run --mount type=bind,source=$SCRATCH/exp,destination=/data/exp bash touch /data/exp/file\n$ ls $SCRATCH/exp\nfile\n</code></pre> <p>You can use multiple containers in a single job, but you have to be careful about the memory and CPU limits of the job.</p> <p>Note</p> <p>Due to the cluster environment you may see warning messages like one of these: <pre><code>WARN[0000] \"/\" is not a shared mount, this could cause issues or missing mounts with rootless containers\nERRO[0000] cannot find UID/GID for user &lt;user&gt;: no subuid ranges found for user \"&lt;user&gt;\" in /etc/subuid - check rootless mode in man pages.\nWARN[0000] Using rootless single mapping into the namespace. This might break some images. Check /etc/subuid and /etc/subgid for adding sub*ids if not using a network user\nWARN[0005] Failed to add pause process to systemd sandbox cgroup: dbus: couldn't determine address of session bus\n</code></pre> but as far as we can see those can be safely ignored and should not have an impact on your images.</p>"},{"location":"Userguide_containers/#gpu","title":"GPU","text":"<p>To use a GPU in a container, you need a GPU job and then use <code>--device nvidia.com/gpu=all</code> to make all GPUs allocated available in the container or <code>--device nvidia.com/gpu=N</code> where <code>N</code> is the gpu index you want in the container, starting at 0.</p> <pre><code>$ nvidia-smi\nFri Dec 13 12:47:34 2024\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L40S                    On  |   00000000:4A:00.0 Off |                    0 |\n| N/A   25C    P8             36W /  350W |       1MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA L40S                    On  |   00000000:61:00.0 Off |                    0 |\n| N/A   26C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n$ podman run --device nvidia.com/gpu=all nvidia/cuda:11.6.1-base-ubuntu20.04 nvidia-smi\nFri Dec 13 17:48:21 2024\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L40S                    On  |   00000000:4A:00.0 Off |                    0 |\n| N/A   25C    P8             36W /  350W |       1MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA L40S                    On  |   00000000:61:00.0 Off |                    0 |\n| N/A   25C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n$ podman run --device nvidia.com/gpu=0 nvidia/cuda:11.6.1-base-ubuntu20.04 nvidia-smi\nFri Dec 13 17:48:33 2024\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L40S                    On  |   00000000:4A:00.0 Off |                    0 |\n| N/A   25C    P8             36W /  350W |       1MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n$ podman run --device nvidia.com/gpu=1 nvidia/cuda:11.6.1-base-ubuntu20.04 nvidia-smi\nFri Dec 13 17:48:40 2024\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA L40S                    On  |   00000000:61:00.0 Off |                    0 |\n| N/A   25C    P8             35W /  350W |       1MiB /  46068MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre> <p>You can pass <code>--device</code> multiple times to add more than one gpus to the container.</p> <p>Note</p> <p>CDI (GPU) support documentation: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/cdi-support.html#running-a-workload-with-cdi</p>"},{"location":"Userguide_data_transfer/","title":"Data Transmission using Globus Connect Personal","text":"<p>Mila doesn't own a Globus license but if the source or destination provides a Globus account, like Digital Research Alliance of Canada for example, it's possible to setup Globus Connect Personal to create a personal endpoint on the Mila cluster by following the Globus guide to Install, Configure, and Uninstall Globus Connect Personal for Linux.</p> <p>This endpoint can then be used to transfer data to and from the Mila cluster.</p>"},{"location":"Userguide_datasets/","title":"Contributing datasets","text":"<p>If a dataset could help the research of others at Mila, this form can be filled to request its addition to /network/datasets.</p>"},{"location":"Userguide_datasets/#publicly-share-a-mila-dataset","title":"Publicly share a Mila dataset","text":"<p>Mila offers two ways to publicly share a Mila dataset:</p> <ul> <li>Academic Torrent</li> <li>Google Drive</li> </ul> <p>Note that these options are not mutually exclusive and both can be used.</p>"},{"location":"Userguide_datasets/#academic-torrent","title":"Academic Torrent","text":"<p>Mila hosts/seeds some datasets created by the Mila community through Academic Torrent. The first step is to create an account and a torrent file.</p> <p>Then drop the dataset in <code>/network/scratch/.transit_datasets</code> and send the Academic Torrent URL to Mila's helpdesk . If the dataset does not reside on the Mila cluster, only the Academic Torrent URL would be needed to proceed with the initial download. Then you can delete / stop sharing your copy.</p> <p>Note</p> <ul> <li>Avoid mentioning dataset in the name of the dataset</li> <li>Avoid capital letters, special charaters (including spaces) in files and   directories names. Spaces can be replaced by hyphens (<code>-</code>).</li> <li>Multiple archives can be provided to spread the data (e.g. dataset splits,   raw data, extra data, ...)</li> </ul>"},{"location":"Userguide_datasets/#generate-a-torrent-file-to-be-uploaded-to-academic-torrent","title":"Generate a .torrent file to be uploaded to Academic Torrent","text":"<p>The command line / Python utility torrentool can be used to create a <code>DATASET_NAME.torrent</code> file:</p> <pre><code># Install torrentool\npython3 -m pip install torrentool click\n# Change Directory to the location of the dataset to be hosted by Mila\ncd /network/scratch/.transit_datasets\ntorrent create --tracker https://academictorrents.com/announce.php DATASET_NAME\n</code></pre> <p>The resulting <code>DATASET_NAME.torrent</code> can then be used to register a new dataset on Academic Torrent.</p> <p>Warning</p> <p>The creation of a <code>DATASET_NAME.torrent</code> file requires the computation of checksums for the dataset content which can quickly become CPU-heavy. This process should not be executed on a login node</p>"},{"location":"Userguide_datasets/#download-a-dataset-from-academic-torrent","title":"Download a dataset from Academic Torrent","text":"<p>Academic Torrent provides a Python API to easily download a dataset from it's registered list:</p> <pre><code># Install the Python API with:\n# python3 -m pip install academictorrents\nimport academictorrents as at\nmnist_path = at.get(\"323a0048d87ca79b68f12a6350a57776b6a3b7fb\", datastore=\"~/scratch/.academictorrents-datastore\") # Download the mnist dataset\n</code></pre> <p>Note</p> <p>Current needs have been evaluated to be for a download speed of about 10 MB/s. This speed can be higher if more users also seeds the dataset.</p>"},{"location":"Userguide_datasets/#google-drive","title":"Google Drive","text":"<p>Only a member of the staff team can upload to Mila's Google Drive which requires to first drop the dataset in <code>/network/scratch/.transit_datasets</code>. Then, contact Mila's helpdesk and provide the following informations:</p> <ul> <li>directory containing the archived dataset (zip is favored) in   <code>/network/scratch/.transit_datasets</code></li> <li>the name of the dataset</li> <li>a licence in <code>.txt</code> format. One of the the creative common   licenses can be used. It is recommended to at least have the Attribution option. The No Derivatives option is discouraged unless the dataset should not be modified by others.</li> <li>MD5 checksum of the archive</li> <li>the arXiv and GitHub URLs (those can be sent later if the article is still in   the submission process)</li> <li>instructions to know if the dataset needs to be <code>unzip</code>\\ed, <code>untar</code>\\ed or   else before uploading to Google Drive</li> </ul> <p>Note</p> <ul> <li>Avoid mentioning dataset in the name of the dataset</li> <li>Avoid capital letters, special charaters (including spaces) in files and   directories names. Spaces can be replaced by hyphens (<code>-</code>).</li> <li>Multiple archives can be provided to spread the data (e.g. dataset splits,   raw data, extra data, ...)</li> </ul>"},{"location":"Userguide_datasets/#download-a-dataset-from-milas-google-drive-with-gdown","title":"Download a dataset from Mila's Google Drive with  <code>gdown</code>","text":"<p>The utility gdown  is a simple utility to download data from Google Drive from the command line shell or in a Python script and requires no setup.</p> <p>Warning</p> <p>A limitation however is that it uses a shared client id which can cause a quota block when too many users uses it in the same day. It is described in a GitHub issue</p>"},{"location":"Userguide_datasets/#download-a-dataset-from-milas-google-drive-with-rclone","title":"Download a dataset from Mila's Google Drive with <code>rclone</code>","text":"<p>Rclone is a command line program to manage files on cloud storage. In the context of a Google Drive remote, it allows to specify a client id to avoid sharing with other users which avoid quota limits. Rclone describes the creation of a client id in its documentaton. Once this is done, a remote for Mila's Google Drive can be configured from the command line:</p> <pre><code>rclone config create mila-gdrive drive client_id XXXXXXXXXXXX-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.apps.googleusercontent.com \\\n    client_secret XXXXXXXXXXXXX-XXXXXXXXXX \\\n    scope 'drive.readonly' \\\n    root_folder_id 1peJ6VF9wQ-LeETgcdGxu1e4fo28JbtUt \\\n    config_is_local false \\\n    config_refresh_token false\n</code></pre> <p>The remote can then be used to download a dataset:</p> <pre><code>rclone copy --progress mila-gdrive:DATASET_NAME/ ~/scratch/datasets/DATASET_NAME/\n</code></pre> <p>Rclone is available from the conda channel conda-forge.</p>"},{"location":"Userguide_datasets/#digital-object-identifier-doi","title":"Digital Object Identifier (DOI)","text":"<p>It is recommended to get a DOI to reference the dataset. A DOI is a permanent id/URL which prevents losing references of online scientific data. https://figshare.com can be used to create a DOI:</p> <ul> <li>Go in <code>My Data</code></li> <li>Create an item by clicking <code>Create new item</code></li> <li>Check <code>Metadata record only</code> at the top</li> <li>Fill the metadata fields</li> </ul> <p>Then reference the dataset using https://doi.org like this: https://doi.org/10.6084/m9.figshare.2066037</p>"},{"location":"Userguide_faq/","title":"Frequently asked questions (FAQs)","text":""},{"location":"Userguide_faq/#connectionssh-issues","title":"Connection/SSH issues","text":""},{"location":"Userguide_faq/#im-getting-connection-refused-while-trying-to-connect-to-a-login-node","title":"I'm getting <code>connection refused</code> while trying to connect to a login node","text":"<p>Login nodes are protected against brute force attacks and might ban your IP if it detects too many connections/failures. You will be automatically unbanned after 1 hour. For any further problem, please submit a support ticket.</p>"},{"location":"Userguide_faq/#shell-issues","title":"Shell issues","text":""},{"location":"Userguide_faq/#how-do-i-change-my-shell","title":"How do I change my shell ?","text":"<p>By default you will be assigned <code>/bin/bash</code> as a shell. If you would like to change for another one, please submit a support ticket.</p>"},{"location":"Userguide_faq/#slurm-issues","title":"SLURM issues","text":""},{"location":"Userguide_faq/#how-can-i-get-an-interactive-shell-on-the-cluster","title":"How can I get an interactive shell on the cluster ?","text":"<p>Use <code>salloc [--slurm_options]</code> without any executable at the end of the command, this will launch your default shell on an interactive session. Remember that an interactive session is bound to the login node where you start it so you could risk losing your job if the login node becomes unreachable.</p>"},{"location":"Userguide_faq/#how-can-i-reset-my-cluster-password","title":"How can I reset my cluster password ?","text":"<p>To reset your password, please submit a support ticket.</p> <p>Warning</p> <p>your cluster password is the same as your Google Workspace account. So, after reset, you must use the new password for all your Google services.</p>"},{"location":"Userguide_faq/#srun-error-mem-and-mem-per-cpu-are-mutually-exclusive","title":"srun: error: --mem and --mem-per-cpu are mutually exclusive","text":"<p>You can safely ignore this, <code>salloc</code> has a default memory flag in case you don't provide one.</p>"},{"location":"Userguide_faq/#how-can-i-see-where-and-if-my-jobs-are-running","title":"How can I see where and if my jobs are running ?","text":"<p>Use <code>squeue -u YOUR_USERNAME</code> to see all your job status and locations. To get more info on a running job, try <code>scontrol show job #JOBID</code></p>"},{"location":"Userguide_faq/#unable-to-allocate-resources-invalid-account-or-accountpartition-combination-specified","title":"Unable to allocate resources: Invalid account or account/partition combination specified","text":"<p>Chances are your account is not setup properly. You should submit a support ticket.</p>"},{"location":"Userguide_faq/#how-do-i-cancel-a-job","title":"How do I cancel a job?","text":"<ul> <li>To cancel a specific job, use <code>scancel #JOBID</code></li> <li>To cancel all your jobs (running and pending), use <code>scancel -u YOUR_USERNAME</code></li> <li>To cancel all your pending jobs only, use <code>scancel -t PD</code></li> </ul>"},{"location":"Userguide_faq/#how-can-i-access-a-node-on-which-one-of-my-jobs-is-running","title":"How can I access a node on which one of my jobs is running ?","text":"<p>You can ssh into a node on which you have a job running, your ssh connection will be adopted by your job, i.e.  if your job finishes your ssh connection will be automatically terminated. In order to connect to a node, you need to have password-less ssh either with a key present in your home or with an <code>ssh-agent</code>. You can generate a key on the login node like this:</p> <pre><code>ssh-keygen (3xENTER)\ncat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/authorized_keys\nchmod 700 ~/.ssh\n</code></pre> <p>The ECDSA, RSA and ED25519 fingerprints for Mila's compute nodes are:</p> <pre><code>SHA256:hGH64v72h/c0SfngAWB8WSyMj8WSAf5um3lqVsa7Cfk (ECDSA)\nSHA256:4Es56W5ANNMQza2sW2O056ifkl8QBvjjNjfMqpB7/1U (RSA)\nSHA256:gUQJw6l1lKjM1cCyennetPoQ6ST0jMhQAs/57LhfakA (ED25519)\n</code></pre>"},{"location":"Userguide_faq/#im-getting-permission-denied-publickey-while-trying-to-connect-to-a-node","title":"I'm getting <code>Permission denied (publickey)</code> while trying to connect to a node","text":"<p>See previous question.</p>"},{"location":"Userguide_faq/#where-do-i-put-my-data-during-a-job","title":"Where do I put my data during a job ?","text":"<p>Your <code>/home</code> as well as the datasets are on shared file-systems, it is recommended to copy them to the <code>$SLURM_TMPDIR</code> to better process them and leverage higher-speed local drives. If you run a low priority job subject to preemption, it's better to save any output you want to keep on the shared file systems, because the <code>$SLURM_TMPDIR</code> is deleted at the end of each job.</p>"},{"location":"Userguide_faq/#slurmstepd-error-detected-1-oom-kill-events-in-step-batch-cgroup","title":"slurmstepd: error: Detected 1 oom-kill event(s) in step <code>###.batch cgroup</code>","text":"<p>You exceeded the amount of memory allocated to your job, either you did not request enough memory or you have a memory leak in your process. Try increasing the amount of memory requested with <code>--mem=</code> or <code>--mem-per-cpu=</code>.</p>"},{"location":"Userguide_faq/#fork-retry-resource-temporarily-unavailable","title":"fork: retry: Resource temporarily unavailable","text":"<p>You exceeded the limit of 2000 tasks/PIDs in your job, it probably means there is an issue with a sub-process spawning too many processes in your script. For any help with your software, please submit a support ticket.</p>"},{"location":"Userguide_faq/#pytorch-issues","title":"PyTorch issues","text":""},{"location":"Userguide_faq/#i-randomly-get-internal-assert-failed-at-atensrcatenmapallocatorcpp263","title":"I randomly get <code>INTERNAL ASSERT FAILED at \"../aten/src/ATen/MapAllocator.cpp\":263</code>","text":"<p>You are using PyTorch 1.10.x and hitting #67864, for which the solution is PR #72232 merged in PyTorch 1.11.x. For an immediate fix, consider the following compilable Gist: hack.cpp. Compile the patch to <code>hack.so</code> and then <code>export LD_PRELOAD=/absolute/path/to/hack.so</code> before executing the Python process that <code>import torch</code> a broken PyTorch 1.10.</p> <p>For Hydra users who are using the submitit launcher plug-in, the <code>env_set</code> key cannot be used to set <code>LD_PRELOAD</code> in the environment as it does so too late at runtime. The dynamic loader reads <code>LD_PRELOAD</code> only once and very early during the startup of any process, before the variable can be set from inside the process. The hack must therefore be injected using the <code>setup</code> key in Hydra YAML config file:: <pre><code>hydra:\n  launcher:\n    setup:\n      - export LD_PRELOAD=/absolute/path/to/hack.so\n</code></pre></p>"},{"location":"Userguide_faq/#on-mig-gpus-i-get-torchcudadevice_count-0-despite-torchcudais_available","title":"On MIG GPUs, I get <code>torch.cuda.device_count() == 0</code> despite <code>torch.cuda.is_available()</code>","text":"<p>You are using PyTorch 1.13.x and hitting #90543, for which the solution is PR #92315 merged in PyTorch 2.0.</p> <p>To avoid thus problem, update to PyTorch 2.0. If PyTorch 1.13.x is required, a workaround is to add the following to your script:</p> <pre><code>unset CUDA_VISIBLE_DEVICES\n</code></pre> <p>But this is no longer necessary with PyTorch &gt;= 2.0.</p>"},{"location":"Userguide_faq/#i-am-told-my-pytorch-job-abuses-the-filesystem-with-extreme-amounts-of-iops","title":"I am told my PyTorch job abuses the filesystem with extreme amounts of IOPS","text":"<p>A fairly common issue in PyTorch is:</p> <pre><code>RuntimeError: one of the variables needed for gradient computation has been\nmodified by an inplace operation: [torch.cuda.FloatTensor [1, 50, 300]],\nwhich is output 0 of SplitBackward, is at version 2; expected version 0\ninstead. Hint: enable anomaly detection to find the operation that failed to\ncompute its gradient, with torch.autograd.set_detect_anomaly(True).\n</code></pre> <p>PyTorch's autograd engine contains an \"anomaly detection mode\", which detects such things as NaN/infinities being created, and helps debugging in-place Tensor modifications. It is activated with</p> <pre><code>torch.autograd.set_detect_anomaly(True)\n</code></pre> <p>PyTorch's implementation of the anomaly-detection mode tracks where every Tensor was created in the program. This involves the collection of the backtrace at the point the Tensor was created.</p> <p>Unfortunately, the collection of a backtrace involves a <code>stat()</code> system call to every source file in the backtrace. This is considered a metadata access to <code>$HOME</code> and results in intolerably heavy traffic to the shared filesystem containing the source code, usually <code>$HOME</code>, whatever the location of the dataset, and even if it is on <code>$SLURM_TMPDIR</code>. It is the source-code files being polled, not the dataset. As there can be hundreds of PyTorch tensors created per iteration and thousands of iterations per second, this mode results in extreme amounts of IOPS to the filesystem.</p> <p>Warning</p> <ul> <li> <p>Do not use <code>torch.autograd.set_detect_anomaly(True)</code> except for   debugging an individual job interactively, and switch it off as soon as   done using it.</p> </li> <li> <p>Do not set <code>torch.autograd.set_detect_anomaly(True)</code> enabled   unconditionally in all your jobs. It is not a consequence-free aid.   Due to heavy use of filesystem calls, it has a performance impact and   slows down your code, on top of abusing the filesystem.</p> </li> <li> <p>You will be contacted if you violate these guidelines due to the   severity of its impact on shared filesystems.</p> </li> </ul>"},{"location":"Userguide_faq/#conda-refuses-to-create-an-environment-with-your-installed-cuda-driver-is-not-available","title":"Conda refuses to create an environment with <code>Your installed CUDA driver is: not available</code>","text":"<p>Anaconda attempts to auto-detect the NVIDIA driver version of the system and thus the maximum CUDA toolkit supported, in an attempt at choosing an appropriate CUDA Toolkit version.</p> <p>However, on login and CPU nodes, there is no NVIDIA GPU and thus no need for NVIDIA drivers. But that means <code>conda</code>'s auto-detection will not work on those nodes, and packages declaring a minimum requirement on the drivers will fail to install.</p> <p>The solution in such a situation is to set the environment variable <code>CONDA_OVERRIDE_CUDA</code> to the desired CUDA Toolkit version; For example,</p> <pre><code>CONDA_OVERRIDE_CUDA=11.8 conda create -n ENVNAME python=3.10 pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n</code></pre> <p>This and other <code>CONDA_OVERRIDE_*</code> variables are documented in the conda manual.</p>"},{"location":"Userguide_jupyterhub/","title":"JupyterHub","text":"<p>JupyterHub is a platform connected to Slurm to start a JupyterLab session as a batch job then connects it when the allocation has been granted. It does not require any ssh tunnel or port redirection, the hub acts as a proxy server that will redirect you to a session as soon as it is available.</p> <p>It is currently available for Mila clusters and some Digital Research Alliance of Canada (Alliance) clusters.</p> Cluster Address Login type Mila Local https://jupyterhub.server.mila.quebec Google Oauth Alliance https://docs.alliancecan.ca/wiki/JupyterHub DRAC login <p>Warning</p> <p>Do not forget to close the JupyterLab session! Closing the window leaves running the session and the SLURM job it is linked to.</p> <p>To close it, use the <code>hub</code> menu and then <code>Control Panel &gt; Stop my server</code></p> <p>Note</p> <p>mila.quebec account credentials should be used to login and start a JupyterLab session.</p>"},{"location":"Userguide_jupyterhub/#access-mila-storage-in-jupyterlab","title":"Access Mila Storage in JupyterLab","text":"<p>Unfortunately, JupyterLab does not allow the navigation to parent directories of <code>$HOME</code>. This makes some file systems like <code>/network/datasets</code> or <code>$SLURM_TMPDIR</code> unavailable through their absolute path in the interface. It is however possible to create symbolic links to those resources. To do so, you can use the <code>ln -s</code> command:</p> <pre><code>ln -s /network/datasets $HOME\n</code></pre> <p>Note that <code>$SLURM_TMPDIR</code> is a directory that is dynamically created for each job so you would need to recreate the symbolic link every time you start a JupyterHub session:</p> <pre><code>ln -sf $SLURM_TMPDIR $HOME\n</code></pre>"},{"location":"Userguide_login/","title":"Logging in to the cluster","text":"<p>To access the Mila Cluster clusters, you will need a Mila account. Please contact Mila systems administrators if you don't have it already. Our IT support service is available here: https://it-support.mila.quebec/</p> <p>You will also need to complete and return an IT Onboarding Training to get access to the cluster.  Please refer to the Mila Intranet for more informations: https://sites.google.com/mila.quebec/mila-intranet/it-infrastructure/it-onboarding-training</p> <p>Important</p> <p>Your access to the Cluster is granted based on your status at Mila (for students, your status is the same as your main supervisor' status), and on the duration of your stay, set during the creation of your account.  The following have access to the cluster : Current Students of Core Professors - Core Professors - Staff</p>"},{"location":"Userguide_login/#ssh-secure-shell","title":"SSH (Secure Shell)","text":"<p>All access to the Mila cluster is via SSH using public-key authentication. As of March 31, 2025, this will become the only means of authentication, and password-based authentication will no longer work.</p> <p>SSH uses a configuration file <code>~/.ssh/config</code> (right next to the SSH keys)  to indicate which connection settings to use for each SSH server one can connect to.</p> <p>The Mila login nodes require:</p> <ul> <li><code>Hostname</code>: <code>login.server.mila.quebec</code></li> <li><code>Port</code>: <code>2222</code></li> <li><code>User</code>: Your Mila account username</li> <li><code>PreferredAuthentications</code>: <code>publickey,keyboard-interactive</code></li> </ul> <p>A simple SSH configuration is automatically created and added for you to <code>~/.ssh/config</code> by mila init.</p> <p>Alternatively, more advanced users can edit the SSH <code>.config</code> file manually.</p>"},{"location":"Userguide_login/#mila-init","title":"mila init","text":"<p>To make it easier to set up a productive environment, Mila publishes the milatools package, which defines a <code>mila init</code> command which will automatically perform some of the below steps for you. You can install it with <code>pip</code> and use it, provided your Python version is at least 3.9:</p> <p>pip install milatools  mila init</p> <p>Note</p> <p>This guide is current for <code>milatools &gt;= 0.0.17</code>. If you have installed an older version previously, run <code>uv tool upgrade milatools</code> or <code>pip install -U milatools</code> to upgrade and re-run <code>mila init</code> in order to apply new features or bug fixes.</p>"},{"location":"Userguide_login/#manual-ssh-configuration","title":"Manual SSH configuration","text":"<p>If you would like to set entries in your <code>~/.ssh/config</code> file manually for advanced use-cases, you may use the following as inspiration:</p> <pre><code>#   Mila\nHost mila             login.server.mila.quebec\n    Hostname          login.server.mila.quebec\nHost mila1    login-1.login.server.mila.quebec\n    Hostname  login-1.login.server.mila.quebec\nHost mila2    login-2.login.server.mila.quebec\n    Hostname  login-2.login.server.mila.quebec\nHost mila3    login-3.login.server.mila.quebec\n    Hostname  login-3.login.server.mila.quebec\nHost mila4    login-4.login.server.mila.quebec\n    Hostname  login-4.login.server.mila.quebec\nHost mila5    login-5.login.server.mila.quebec\n    Hostname  login-5.login.server.mila.quebec\nHost cn-????\n    Hostname             %h.server.mila.quebec\nMatch host !*login.server.mila.quebec,*.server.mila.quebec\n    Hostname                 %h\n    ProxyJump                mila\nMatch host           *login.server.mila.quebec\n    Port                     2222\n    ServerAliveInterval      120\n    ServerAliveCountMax      5\nMatch host *.server.mila.quebec\n    PreferredAuthentications publickey,keyboard-interactive\n    AddKeysToAgent           yes\n    ## Consider uncommenting:\n    # ForwardAgent             yes\n    ## Delete if on Linux, uncomment if on Mac:\n    # UseKeychain              yes\n    User                     CHANGEME_YOUR_MILA_USERNAME\n</code></pre> <p>Important</p> <p>Please make the required edits to the template above, especially regarding <code>CHANGEME_YOUR_MILA_USERNAME</code>!</p>"},{"location":"Userguide_login/#logging-in-with-ssh","title":"Logging in with SSH","text":"<p>Login to the Mila cluster is with <code>ssh</code> through four Internet-facing login nodes and a load-balancer. At each connection through the load-balancer, you will be redirected to the least loaded login node.</p> <pre><code># Generic login, will send you to one of the 4 login nodes to spread the load\nssh -p 2222 &lt;user&gt;@login.server.mila.quebec\n\n# To connect to a specific login node, X in [1, 2, 3, 4]\nssh -p 2222 &lt;user&gt;@login-X.login.server.mila.quebec\n</code></pre> <p>This is a significant amount of typing. You are strongly encouraged to add a <code>mila</code> \"alias\" to your SSH configuration file (see below for how). With a correctly-configured SSH you can now simply run</p> <pre><code># Login with SSH configuration in place\nssh mila\n\n# Can also scp...        vvvv\nscp  file-to-upload.zip  mila:scratch/uploaded.zip\n\n#          vvvv  ... and rsync!\nrsync -avz mila:my/remote/sourcecode/  downloaded-source/\n</code></pre> <p>to connect to a login node. The <code>mila</code> alias will be available to <code>ssh</code>, <code>scp</code>, <code>rsync</code> and all other programs that consult the SSH configuration file.</p> <p>Upon first login, you may be asked to enter your SSH key passphrase. Use the passphrase you used to create your SSH key below.</p> <p>Upon first login, you may also be asked whether you trust the Mila login servers' own SSH keys. The ECDSA, RSA and ED25519 fingerprints for Mila's login nodes are:</p> <pre><code>SHA256:baEGIa311fhnxBWsIZJ/zYhq2WfCttwyHRKzAb8zlp8 (ECDSA)\nSHA256:Xr0/JqV/+5DNguPfiN5hb8rSG+nBAcfVCJoSyrR0W0o (RSA)\nSHA256:gfXZzaPiaYHcrPqzHvBi6v+BWRS/lXOS/zAjOKeoBJg (ED25519)\n</code></pre> <p>If the fingerprints presented to you do not match one of the above, do not trust them!</p> <p>TIP</p> <p>You can run commands on the login node with <code>ssh</code> directly, for example <code>ssh mila squeue -u '$USER'</code> (remember to put single quotes around any <code>$VARIABLE</code> you want to evaluate on the remote side, otherwise it will be evaluated locally before ssh is even executed).</p> <p>Important</p> <p>Login nodes are merely entry points to the cluster. They give you access to the compute nodes and to the filesystem, but they are not meant to run anything heavy. Do not run compute-heavy programs on these nodes, because in doing so you could bring them down, impeding cluster access for everyone.</p> <p>This means no training scripts or experiments and no compilation of software unless it is small or ends quickly. Do not run anything that demands a sustained large amount of computation or a large amount of memory.</p> <p>Rule of thumb: Never run a program that takes more than a few seconds on a login node, unless it mostly sleeps or mostly moves data.</p> <p>Examples: A non-exhaustive list of use-cases, to give a sense of what is and is not allowed on the login nodes:</p> <ul> <li>A Python training script is unacceptable on the login nodes.   (Too computationally- and memory-intensive)</li> <li>A Python or shell script that downloads a dataset and exits immediately after may be acceptable on the login nodes. (Mostly moves data)</li> <li>A Python hyperparameter search script that uses <code>submitit</code>  to launch jobs and only sleeps waiting for them to end and run other jobs is acceptable on the login nodes. (Mostly sleeps; The actual jobs run on the compute nodes)</li> <li><code>pip install</code> of <code>vllm</code> or <code>flash-attn</code> from source code on the login nodes is unacceptable (and is likely to fail anyways). (Takes far too much RAM to compile the CUDA kernels)</li> <li>Editing code with <code>nano</code>, <code>vim</code> or <code>emacs</code> is acceptable. (Editors mostly sleep awaiting user keystrokes)</li> <li>Copying/moving files with <code>cp</code>, <code>mv</code>, ... is acceptable. (Mostly moves data)</li> <li>Connecting to compute nodes with <code>ssh</code> is acceptable. (Mostly sleeps, forwarding keystrokes and ports to/from the node)</li> <li>Using <code>tmux</code> is acceptable. (Mostly sleeps, managing the processes under its control)</li> </ul> <p>Note</p> <p>In a similar vein, you should not run VSCode remote SSH instances directly on login nodes, because even though they are typically not very computationally expensive, when many people do it, they add up! See Visual Studio Code for specific instructions.</p>"},{"location":"Userguide_login/#connecting-to-compute-nodes","title":"Connecting to compute nodes","text":"<p>If (and only if) you have a job running on compute node <code>cnode</code>, you are allowed to SSH to it, if for some reason you need a second terminal. That session will be automatically ended when your job ends.</p> <p>First, however, you need to add your public key (the one you provided to IT-support) to the ~/.ssh/authorized_keys file on the cluster, or configure an <code>ssh-agent</code> that will forward that key when connecting to the compute node.</p> <pre><code># ON A LOGIN NODE\nmkdir -p ~/.ssh\necho \"THE SSH PUBLIC KEY THAT YOU GAVE TO IT-SUPPORT\" &gt;&gt; ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/authorized_keys\nchmod 700 ~/.ssh\nchmod go-w ~   # in case you accidentally gave too many permissions for $HOME in the past.\n</code></pre> <p>Then from the login node you can write <code>ssh cnode</code>. From your local machine, you can use <code>ssh -J mila USERNAME@cnode</code> (<code>-J</code> represents a \"jump\" through the login node, necessary because the compute nodes are behind a firewall).</p> <p>If you wish, you may also add the following wildcard rule in your <code>.ssh/config</code>:</p> <pre><code>Host *.server.mila.quebec !*login.server.mila.quebec\n    HostName %h\n    User YOUR-USERNAME\n    ProxyJump mila\n</code></pre> <p>This will let you connect to a compute node with <code>ssh &lt;node&gt;.server.mila.quebec</code>.</p>"},{"location":"Userguide_login/#auto-allocation-with-mila-cpu","title":"Auto-allocation with mila-cpu","text":"<p>If you install milatools_ and run <code>mila init</code>, then you can automatically allocate a CPU on a compute node and connect to it by running:</p> <pre><code>ssh mila-cpu\n</code></pre> <p>And that's it! Multiple connections to <code>mila-cpu</code> will all reuse the same job, so you can use it liberally. It also works transparently with VSCode's Remote SSH feature.</p> <p>We recommend using this for light work that is too heavy for a login node but does not require a lot of resources: editing via VSCode, building conda environments, tests, etc.</p> <p>The <code>mila-cpu</code> entry should be in your <code>.ssh/config</code>. Changes are at your own risk.</p>"},{"location":"Userguide_login/#more-about-ssh","title":"More About SSH","text":"<p>SSH key authentication is a technique using pairs of closely-linked keys: A private key, and a corresponding public key. The public key should be distributed to everyone, while the private key is known to only one person. The public key can be used by anyone to challenge a person to prove their identity. If they have the corresponding private key, that person can perform an electronic signature that everyone can validate but that no one else could have done themselves. The challenge is thus answered by demonstrating possession of the private key (and therefore their identity), without ever revealing the private key itself.</p> <p>Mila asks you to generate a pair of SSH keys, to provide Mila only with your public key, which has no confidentiality implications, and to keep the private key for yourself. The private key must remain secret and solely known to you, because anyone who possesses it is capable of impersonating you by performing your electronic signature.</p> <p>During the IT Onboarding Training, you will be asked to submit that SSH public key.</p> <ul> <li>If you do not know what SSH keys are, or are not familiar with them, you can   read the informative material below, then proceed to   generate them.</li> <li>If you do not already have SSH keys, or are not sure if you have them, skip   to the instructions on how to generate them here.</li> <li>If you do have SSH keys, you can skip to configuring SSH for access to Mila.</li> </ul>"},{"location":"Userguide_login/#ssh-private-keys","title":"SSH Private Keys","text":"<p>A private SSH key commonly takes the form of an obscure text file. It encodes the digital secret of how to make an electronic signature \u2014 specifically, yours. The content of a private SSH key might resemble</p> <pre><code>-----BEGIN OPENSSH PRIVATE KEY-----\nb3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn\nNhAAAAAwEAAQAAAYEAl5dD/UU2CvauaVS2/4/iWoUyO1Hey+m8KojCFMvIywL6PPdYRqVa\nFOidmOw/E9V2HVzHz/z/2Dj6TO5xNX1qJFk7A/ACGGc1+KguIDQWdjR6AZb5Tat+aAMYro\n\u2026\naSeJOS59knbQJeBwPm0g5G+iFz6R17446dXk5jn3/29AutF5MPnKwqE0mjywxCLYxVX3He\nYSOCZfE80P/z4sImW82BYxAzKtI8kKagLmHS4gXJEmE13Dfyq0xcB3q5OMuQ2fZwvukTx3\nxdWgyqFrMyC4wHAAAAAAEC\n-----END OPENSSH PRIVATE KEY-----\n</code></pre> <p>In the real world, a handwritten signature is useless for authenticating you if it can be easily reproduced by others. In the virtual world, the same is true. Anyone who has your private key is capable of reproducing your electronic signature. It is therefore essential that only one person \u2014 you \u2014 holds this private key. The secrecy of the private key is the guarantor of your online identity.</p> <p>Mila will never ask you for your private SSH key, and any pretense of request for a private key constitutes an attempt at phishing and identity theft. Keep your private keys safe and do not share them with anyone. Do not put them in the cloud, your emails, Slack messages, or Git repos. Protect them with a passphrase.</p>"},{"location":"Userguide_login/#ssh-public-keys","title":"SSH Public Keys","text":"<p>A public SSH key is a simple line of text, albeit sometimes very long, commonly found in a file with the <code>.pub</code> extension. It encodes the digital knowledge required to recognize and validate your electronic signature, without however making it possible to reproduce it elsewhere. Here are three examples of public SSH keys:</p> <pre><code>ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDMYpSndal/\u2026mPL+NXs=\nssh-ed25519 AAAA\u2026d/ca2h  user@server\necdsa-sha2-nistp256 AAAA\u2026hWQcQg8=  mylaptop\n</code></pre> <p>You are requested to submit just such a public SSH key to Mila, which will allow Mila to recognize you when you connect to the Mila cluster, but without revealing the secret of how to perform your signature.</p>"},{"location":"Userguide_login/#checking-if-you-already-have-ssh-private-keys","title":"Checking If You Already Have SSH (Private) Keys","text":"<p>Usually, a private SSH key is found in the hidden directory <code>~/.ssh/</code> and is named <code>id_rsa</code>, <code>id_ed25519</code>, or <code>id_ecdsa</code>. Its corresponding public SSH key is usually in the same directory and shares the same filename, except with a <code>.pub</code> suffix.</p>"},{"location":"Userguide_login/#generating-an-ssh-private-key","title":"Generating an SSH Private Key","text":"<p>If no private SSH key already exists, you can create one with the <code>ssh-keygen</code> utility:</p> RSA Ed25519 Integer factorization Elliptic curve <ul><li>Classic</li><li>Ultra-compatible, standardized, the reference</li><li>Large key size, but configurable</li><li>Slow or even very slow</li></ul> <ul><li>New</li><li>Less compatible</li><li>Fixed, small key size</li><li>Fast</li></ul> <code>$ ssh-keygen -t rsa -b 3072</code> <code>$ ssh-keygen -t ed25519</code> <p>Tip</p> <p>The pass-phrase protects the SSH private key on-disk. The passphrase is not the same thing as the pass-word used to log into your personal computer account. However, choosing them to be equal may allow for automatic unlocking of encrypted SSH private keys at login, in combination with software such as <code>pam_ssh(8)</code> (Linux) or Keychain (Mac OS X/macOS). This makes the good practice of using encrypted keys convenient as well.</p>"},{"location":"Userguide_login/#generating-an-ssh-public-key-from-a-private-key","title":"Generating an SSH Public Key from a Private Key","text":"<p>If a private SSH key exists, but not its corresponding SSH public key, it can be recalculated with the <code>ssh-keygen</code> utility as well:</p> RSA Ed25519 SSH public key: SSH public key: <ul><li>380 bytes @ 2048 bits (not rec.)    </li></ul> ~82 bytes <ul><li>550 bytes @ 3072 bits (recommended) </li></ul> <ul><li>725 bytes @ 4096 bits (slower)      </li></ul> <ul><li>1400 bytes @ 8192 bits (much slower)</li></ul> <code>$ ssh-keygen -y -f ~/.ssh/id_rsa</code> <code>$ ssh-keygen -y -f ~/.ssh/id_ed25519</code> <p>It is this SSH public key that you should submit in the IT Onboarding Training form.</p>"},{"location":"Userguide_login/#using-a-non-bash-unix-shell","title":"Using a non-Bash Unix shell","text":"<p>While Mila does not provide support in debugging your shell setup, Bash is the standard shell to be used on the cluster and the cluster is designed to support both Bash and Zsh shells. If you think things should work with Zsh and they don't, please contact Mila's IT support .</p>"},{"location":"Userguide_multigpu/","title":"Advanced SLURM usage and Multiple GPU jobs","text":""},{"location":"Userguide_multigpu/#handling-preemption","title":"Handling preemption","text":"<p>On the Mila cluster, jobs can preempt one-another depending on their priority (unkillable&gt;high&gt;low) (See the Slurm documentation)</p> <p>The default preemption mechanism is to kill and re-queue the job automatically without any notice. To allow a different preemption mechanism, every partition have been duplicated (i.e. have the same characteristics as their counterparts) allowing a 120sec grace period before killing your job but don't requeue it automatically: those partitions are referred by the suffix: <code>-grace</code> (<code>main-grace, long-grace, main-cpu-grace, long-cpu-grace</code>).</p> <p>When using a partition with a grace period, a series of signals consisting of first <code>SIGCONT</code> and <code>SIGTERM</code> then <code>SIGKILL</code> will be sent to the SLURM job.  It's good practice to catch those signals using the Linux <code>trap</code> command to properly terminate a job and save what's necessary to restart the job.  On each cluster, you'll be allowed a grace period before SLURM actually kills your job (<code>SIGKILL</code>).</p> <p>The easiest way to handle preemption is by trapping the <code>SIGTERM</code> signal</p> <pre><code>#SBATCH --ntasks=1\n#SBATCH ....\n\nexit_script() {\n      echo \"Preemption signal, saving myself\"\n      trap - SIGTERM # clear the trap\n      # Optional: sends SIGTERM to child/sub processes\n      kill -- -$$\n}\n\ntrap exit_script SIGTERM\n\n# The main script part\npython3 my_script\n</code></pre> <p>Requeuing</p> <p>The Slurm scheduler on the cluster does not allow a grace period before preempting a job while requeuing it automatically, therefore your job will be cancelled at the end of the grace period. To automatically requeue it, you can just add the <code>sbatch</code> command inside your <code>exit_script</code> function.</p>"},{"location":"Userguide_multigpu/#packing-jobs","title":"Packing jobs","text":""},{"location":"Userguide_multigpu/#sharing-a-gpu-between-processes","title":"Sharing a GPU between processes","text":"<p><code>srun</code>, when used in a batch job is responsible for starting tasks on the allocated resources (see srun) SLURM batch script</p> <pre><code>#SBATCH --ntasks-per-node=2\n#SBATCH --output=myjob_output_wrapper.out\n#SBATCH --ntasks=2\n#SBATCH --gres=gpu:1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=18G\nsrun --label --output=myjob_output_%t.out python script args\n</code></pre> <p>This will run Python 2 times, each process with 4 CPUs with the same arguments <code>--output=myjob_output_%t.out</code> will create 2 output files appending the task id (<code>%t</code>) to the filename and 1 global log file for things happening outside the <code>srun</code> command.</p> <p>Knowing that, if you want to have 2 different arguments to the Python program, you can use a multi-prog configuration file: <code>srun --label --multi-prog silly.conf</code></p> <pre><code>0  python script firstarg\n1  python script secondarg\n</code></pre> <p>Or by specifying a range of tasks</p> <pre><code>0-1  python script %t\n</code></pre> <p>%t being the taskid that your Python script will parse.  Note the <code>-l</code> on the <code>srun</code> command: this will prepend each line with the taskid (0:, 1:)</p>"},{"location":"Userguide_multigpu/#sharing-a-node-with-multiple-gpu-1processgpu","title":"Sharing a node with multiple GPU 1process/GPU","text":"<p>On Digital Research Alliance of Canada, several nodes, especially nodes with <code>largeGPU</code> (P100) are reserved for jobs requesting the whole node, therefore packing multiple processes in a single job can leverage faster GPU.</p> <p>If you want different tasks to access different GPUs in a single allocation you need to create an allocation requesting a whole node and using <code>srun</code> with a subset of those resources (1 GPU).</p> <p>Keep in mind that every resource not specified on the <code>srun</code> command while inherit the global allocation specification so you need to split each resource in a subset (except --cpu-per-task which is a per-task requirement)</p> <p>Each <code>srun</code> represents a job step (<code>%s</code>).</p> <p>Example for a GPU node with 24 cores and 4 GPUs and 128G of RAM Requesting 1 task per GPU</p> <pre><code>#!/bin/bash\n#SBATCH --nodes=1-1\n#SBATCH --ntasks-per-node=4\n#SBATCH --output=myjob_output_wrapper.out\n#SBATCH --gres=gpu:4\n#SBATCH --cpus-per-task=6\nsrun --gres=gpu:1 --ntasks=1 --mem=30G --label --output=%j-step-%s.out --exclusive --multi-prog python script args1 &amp;\nsrun --gres=gpu:1 --ntasks=1 --mem=30G --label --output=%j-step-%s.out --exclusive --multi-prog python script args2 &amp;\nsrun --gres=gpu:1 --ntasks=1 --mem=30G --label --output=%j-step-%s.out --exclusive --multi-prog python script args3 &amp;\nsrun --gres=gpu:1 --ntasks=1 --mem=30G --label --output=%j-step-%s.out --exclusive --multi-prog python script args4 &amp;\nwait\n</code></pre> <p>This will create 4 output files:</p> <ul> <li>JOBID-step-0.out</li> <li>JOBID-step-1.out</li> <li>JOBID-step-2.out</li> <li>JOBID-step-3.out</li> </ul>"},{"location":"Userguide_multigpu/#sharing-a-node-with-multiple-gpu-multiple-processesgpu","title":"Sharing a node with multiple GPU &amp; multiple processes/GPU","text":"<p>Combining both previous sections, we can create a script requesting a whole node with four GPUs, allocating 1 GPU per <code>srun</code> and sharing each GPU with multiple processes</p> <p>Example still with a 24 cores/4 GPUs/128G RAM Requesting 2 tasks per GPU</p> <p><pre><code>#!/bin/bash\n#SBATCH --nodes=1-1\n#SBATCH --ntasks-per-node=8\n#SBATCH --output=myjob_output_wrapper.out\n#SBATCH --gres=gpu:4\n#SBATCH --cpus-per-task=3\nsrun --gres=gpu:1 --ntasks=2 --mem=30G --label --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;\nsrun --gres=gpu:1 --ntasks=2 --mem=30G --label --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;\nsrun --gres=gpu:1 --ntasks=2 --mem=30G --label --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;\nsrun --gres=gpu:1 --ntasks=2 --mem=30G --label --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;\nwait\n</code></pre> <code>--exclusive</code> is important to specify subsequent step/srun to bind to different cpus.</p> <p>This will produce 8 output files, 2 for each step:</p> <ul> <li>JOBID-step-0-task-0.out</li> <li>JOBID-step-0-task-1.out</li> <li>JOBID-step-1-task-0.out</li> <li>JOBID-step-1-task-1.out</li> <li>JOBID-step-2-task-0.out</li> <li>JOBID-step-2-task-1.out</li> <li>JOBID-step-3-task-0.out</li> <li>JOBID-step-3-task-1.out</li> </ul> <p>Running <code>nvidia-smi</code> in silly.conf, while parsing the output, we can see 4 GPUs allocated and 2 tasks per GPU <pre><code>$ cat JOBID-step-* | grep Tesla\n0: |   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\n1: |   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |\n0: |   0  Tesla P100-PCIE...  On   | 00000000:83:00.0 Off |                    0 |\n1: |   0  Tesla P100-PCIE...  On   | 00000000:83:00.0 Off |                    0 |\n0: |   0  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |\n1: |   0  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |\n0: |   0  Tesla P100-PCIE...  On   | 00000000:03:00.0 Off |                    0 |\n1: |   0  Tesla P100-PCIE...  On   | 00000000:03:00.0 Off |                    0 |\n</code></pre></p>"},{"location":"Userguide_multinode/","title":"Multiple Nodes","text":""},{"location":"Userguide_multinode/#data-parallel","title":"Data Parallel","text":"<p>Request 3 nodes with at least 4 GPUs each.</p> <pre><code>#!/bin/bash\n# Number of Nodes\n#SBATCH --nodes=3\n\n# Number of tasks. 3 (1 per node)\n#SBATCH --ntasks=3\n\n# Number of GPU per node\n#SBATCH --gres=gpu:4\n#SBATCH --gpus-per-node=4\n\n# 16 CPUs per node\n#SBATCH --cpus-per-gpu=4\n\n# 16Go per nodes (4Go per GPU)\n#SBATCH --mem=16G\n\n# we need all nodes to be ready at the same time\n#SBATCH --wait-all-nodes=1\n\n# Total resources:\n#   CPU: 16 * 3 = 48\n#   RAM: 16 * 3 = 48 Go\n#   GPU:  4 * 3 = 12\n\n# Setup our rendez-vous point\nRDV_ADDR=$(hostname)\nWORLD_SIZE=$SLURM_JOB_NUM_NODES\n# -----\n\nsrun --label torchrun \\\n   --nproc_per_node=$SLURM_GPUS_PER_NODE\\\n   --nnodes=$WORLD_SIZE\\\n   --rdzv_id=$SLURM_JOB_ID\\\n   --rdzv_backend=c10d\\\n   --rdzv_endpoint=$RDV_ADDR\\\n   training_script.py\n</code></pre> <p>You can find below a pytorch script outline on what a multi-node trainer could look like.</p> Training script outline for multi node training<pre><code>import os\nimport torch.distributed as dist\n\nclass Trainer:\n   def __init__(self):\n      self.local_rank = None\n      self.chk_path = ...\n      self.model = ...\n\n   @property\n   def device_id(self):\n      return self.local_rank\n\n   def load_checkpoint(self, path):\n      self.chk_path = path\n      # ...\n\n   def should_checkpoint(self):\n      # Note: only one worker saves its weights\n      return self.global_rank == 0 and self.local_rank == 0\n\n   def save_checkpoint(self):\n      if self.chk_path is None:\n            return\n\n      # Save your states here\n      # Note: you should save the weights of self.model not ddp_model\n      # ...\n\n   def initialize(self):\n      self.global_rank = int(os.environ.get(\"RANK\", -1))\n      self.local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n\n      assert self.global_rank &gt;= 0, 'Global rank should be set (Only Rank 0 can save checkpoints)'\n      assert self.local_rank &gt;= 0, 'Local rank should be set'\n\n      dist.init_process_group(backend=\"gloo|nccl\")\n\n   def sync_weights(self, resuming=False):\n      if resuming:\n            # in the case of resuming all workers need to load the same checkpoint\n            self.load_checkpoint()\n\n            # Wait for everybody to finish loading the checkpoint\n            dist.barrier()\n            return\n\n      # Make sure all workers have the same initial weights\n      # This makes the leader save his weights\n      if self.should_checkpoint():\n            self.save_checkpoint()\n\n      # All workers wait for the leader to finish\n      dist.barrier()\n\n      # All followers load the leader's weights\n      if not self.should_checkpoint():\n            self.load_checkpoint()\n\n      # Leader waits for the follower to load the weights\n      dist.barrier()\n\n   def dataloader(self, dataset, batch_size):\n      train_sampler = ElasticDistributedSampler(dataset)\n      train_loader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            num_workers=4,\n            pin_memory=True,\n            sampler=train_sampler,\n      )\n      return train_loader\n\n   def train_step(self):\n      # Your batch processing step here\n      # ...\n      pass\n\n   def train(self, dataset, batch_size):\n      self.sync_weights()\n\n      ddp_model = torch.nn.parallel.DistributedDataParallel(\n            self.model,\n            device_ids=[self.device_id],\n            output_device=self.device_id\n      )\n\n      loader = self.dataloader(dataset, batch_size)\n\n      for epoch in range(100):\n            for batch in iter(loader):\n               self.train_step(batch)\n\n               if self.should_checkpoint():\n                  self.save_checkpoint()\n\ndef main():\n   trainer = Trainer()\n   trainer.load_checkpoint(path)\n   tainer.initialize()\n\n   trainer.train(dataset, batch_size)\n</code></pre> <p>Note</p> <p>To bypass Python GIL (Global interpreter lock) pytorch spawn one process for each GPU. In the example above this means at least 12 processes are spawn, at least 4 on each node.</p>"},{"location":"Userguide_portability/","title":"Portability concerns and solutions","text":"<p>When working on a software project, it is important to be aware of all the software and libraries the project relies on and to list them explicitly and under a version control system in such a way that they can easily be installed and made available on different systems. The upsides are significant:</p> <ul> <li>Easily install and run on the cluster</li> <li>Ease of collaboration</li> <li>Better reproducibility</li> </ul> <p>To achieve this, try to always keep in mind the following aspects:</p> <ul> <li>Versions: For each dependency, make sure you have some record of the   specific version you are using during development. That way, in the future, you   will be able to reproduce the original environment which you know to be   compatible. Indeed, the more time passes, the more likely it is that newer   versions of some dependency have breaking changes. The <code>pip freeze</code> command can create   such a record for Python dependencies.</li> <li>Isolation: Ideally, each of your software projects should be isolated from   the others. What this means is that updating the environment for project A   should not update the environment for project B. That way, you can freely   install and upgrade software and libraries for the former without worrying about   breaking the latter (which you might not notice until weeks later, the next time   you work on project B!) Isolation can be made easy using using uv, as well as   Python Virtual environments   and, as a last resort, containers.</li> </ul>"},{"location":"Userguide_portability/#virtual-environments","title":"Virtual environments","text":"<p>A virtual environment in Python is a local, isolated environment in which you can install or uninstall Python packages without interfering with the global environment (or other virtual environments). It usually lives in a directory (location varies depending on whether you use venv, conda or poetry). In order to use a virtual environment, you have to activate it. Activating an environment essentially sets environment variables in your shell so that:</p> <ul> <li><code>python</code> points to the right Python version for that environment (different   virtual environments can use different versions of Python!)</li> <li><code>python</code> looks for packages in the virtual environment</li> <li><code>pip install</code> installs packages into the virtual environment</li> <li>Any shell commands installed via <code>pip install</code> are made available</li> </ul> <p>To run experiments within a virtual environment, you can simply activate it in the script given to <code>sbatch</code>.</p>"},{"location":"Userguide_portability/#uv","title":"UV","text":"<p>In many cases, where your dependencies are Python packages, we highly recommend using <code>UV &lt;https://docs.astral.sh/uv&gt;</code>__, a modern package manager for Python.</p> <p>In addition to all the same features as pip, it also manages Python installations, virtual environments, and makes your environments easier to reproduce and reuse across compute clusters.</p> <p>Note</p> <p>UV is not currently available as a module on the Mila or DRAC clusters at the time of writing. To use it, you first need to install it using this command on a cluster login node: <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre></p> Pip/virtualenv command UV pip equivalent UV <code>project</code> command (recommended) Create your virtualenv <code>module load python/3.10</code>then <code>python -m venv</code> <code>uv venv</code>_ <code>uv init</code> and <code>uv sync</code> Activate the virtualenv <code>. .venv/bin/activate</code> (same) (same, but often unnecessary) Install a package activate venv then <code>pip install</code> <code>uv pip install</code>_ <code>uv add</code>_ Run a command     (ex. <code>python main.py</code>) <code>module load python</code>, then<code>. &lt;venv&gt;/bin/activate</code>, then  <code>python main.py</code> <code>. &lt;venv&gt;/bin/activate</code>,then <code>python main.py</code> <code>uv run python main.py</code> Where are dependencies declared? Maybe in a <code>requirements.txt</code>, <code>setup.py</code> or <code>pyproject.toml</code> Maybe in a <code>requirements.txt</code>,  <code>setup.py</code> or <code>pyproject.toml</code> always in <code>pyproject.toml</code> Easy to change Python   versions? No somewhat Yes: <code>uv python pin &lt;version&gt;</code> or <code>uv sync --python &lt;version&gt;</code> <p>While you can use UV as a drop-in replacement for pip, we recommend adopting a project-based workflow:</p> <ul> <li> <p>Use <code>uv init</code> to create a new project. A <code>pyproject.toml</code> file will be created. This is where your dependencies are listed.</p> <pre><code>uv init --python=3.12\n</code></pre> </li> <li> <p>Use <code>uv add</code> to add (and <code>uv remove</code> to remove) dependencies to your project. This will update the <code>pyproject.toml</code> file and update the virtual environment.</p> <pre><code>uv add torch\n</code></pre> </li> <li> <p>Use <code>uv run</code> to run commands, for example <code>uv run python train.py</code>.   This will automatically do the following:</p> <ol> <li>Create or update the virtualenv (with the correct Python version) if necessary, based the dependencies in <code>pyproject.toml</code>.</li> <li>Activates the virtualenv.</li> <li>Runs the command you provided, e.g. <code>python train.py</code>.</li> </ol> <pre><code>uv run python main.py\n</code></pre> </li> </ul>"},{"location":"Userguide_portability/#pipvirtualenv","title":"Pip/Virtualenv","text":"<p>Pip is the most widely used package manager for Python and each cluster provides several Python versions through the associated module which comes with pip. In order to install new packages, you will first have to create a personal space for them to be stored.  The usual solution (as it is the recommended solution on Digital Research Alliance of Canada clusters) is to use <code>virtual environments &lt;https://virtualenv.pypa.io/en/stable/&gt;</code>_, although using_uv is now the recommended way to manage Python installations, virtual environments and dependencies.</p> <p>Note</p> <p>We recommend you use <code>UV &lt;https://docs.astral.sh/uv&gt;</code>_ to manage your Python virtual environments instead of doing it manually. The previous section gives an overview of how to install it and use it.</p> <p>First, load the Python module you want to use: <pre><code>module load python/3.10\n</code></pre></p> <p>Then, create a virtual environment in your <code>home</code> directory: <pre><code>python -m venv $HOME/&lt;env&gt;\n</code></pre></p> <p>Where <code>&lt;env&gt;</code> is the name of your environment. Finally, activate the environment:</p> <pre><code>source $HOME/&lt;env&gt;/bin/activate\n</code></pre> <p>You can now install any Python package you wish using the <code>pip</code> command, e.g. pytorch :</p> <pre><code>pip install torch torchvision\n</code></pre> <p>Or Tensorflow : <pre><code>pip install tensorflow-gpu\n</code></pre></p>"},{"location":"Userguide_portability/#conda","title":"Conda","text":"<p>Another solution for Python is to use miniconda or anaconda which are also available through the <code>module</code> command: (the use of Conda is not recommended for Digital Research Alliance of Canada clusters due to the availability of custom-built packages for pip)</p> <pre><code>$ module load miniconda/3\n[=== Module miniconda/3 loaded ===]\n</code></pre> <p>To create an environment (see here for details) using a specific Python version, you may write: <pre><code>conda create -n &lt;env&gt; python=3.9\n</code></pre> Where <code>&lt;env&gt;</code> is the name of your environment. You can now activate it by doing: <pre><code>conda activate &lt;env&gt;\n</code></pre> You are now ready to install any Python package you want in this environment. For instance, to install PyTorch, you can find the Conda command of any version you want on pytorch's website, e.g:</p> <pre><code>conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n</code></pre> <p>If you make a lot of environments and install/uninstall a lot of packages, it can be good to periodically clean up Conda's cache: <pre><code>conda clean -it\n</code></pre></p>"},{"location":"Userguide_portability/#mamba","title":"Mamba","text":"<p>When installing new packages with <code>conda install</code>, conda uses a built-in dependency solver for solving the dependency graph of all packages (and their versions) requested such that package dependency conflicts are avoided.</p> <p>In some cases, especially when there are many packages already installed in a conda environment, conda's built-in dependency solver can struggle to solve the dependency graph, taking several to tens of minutes, and sometimes never solving. In these cases, it is recommended to try libmamba.</p> <p>To install and set the <code>libmamba</code> solver, run the following commands:</p> <pre><code># Install miniconda\n# (you can not use the preinstalled anaconda/miniconda as installing libmamba\n#  requires ownership over the anaconda/miniconda install directory)\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-py310_22.11.1-1-Linux-x86_64.sh\n$ bash Miniconda3-py310_22.11.1-1-Linux-x86_64.sh\n\n# Install libmamba\n$ conda install -n base conda-libmamba-solver\n</code></pre> <p>By default, conda uses the built-in solver when installing packages, even after installing other solvers. To try <code>libmamba</code> once, add <code>--solver=libmamba</code> in your <code>conda install</code> command. For example: <pre><code>conda install tensorflow --solver=libmamba\n</code></pre></p> <p>You can set <code>libmamba</code> as the default solver by adding <code>solver: libmamba</code> to your <code>.condarc</code> configuration file located under your <code>$HOME</code> directory. You can create it if it doesn't exist. You can also run: <pre><code>conda config --set solver libmamba\n</code></pre></p>"},{"location":"Userguide_portability/#using-modules","title":"Using Modules","text":"<p>A lot of software, such as Python and Conda, is already compiled and available on the cluster through the <code>module</code> command and its sub-commands. In particular, if you wish to use <code>Python 3.10</code> you can simply do: <pre><code>module load python/3.10\n</code></pre></p>"},{"location":"Userguide_portability/#the-module-command","title":"The module command","text":"<p>For a list of available modules, simply use:</p> <pre><code>$ module avail\n</code></pre> <p>Modules can be loaded using the <code>load</code> command: <pre><code>module load &lt;module&gt;\n</code></pre> To search for a module or a software, use the command <code>spider</code>: <pre><code>module spider search_term\n</code></pre> E.g.: by default, <code>python2</code> will refer to the os-shipped installation of <code>python2.7</code> and <code>python3</code> to <code>python3.10</code>. If you want to use <code>python3.7</code> you can type: <pre><code>module load python/3.7\n</code></pre></p>"},{"location":"Userguide_portability/#available-software","title":"Available Software","text":"<p>Modules are divided in 5 main sections:</p> Section Description Core Base interpreter and software (Python, go, etc...) Compiler Interpreter-dependent software (see the note below) Cuda Toolkits, cudnn and related libraries Pytorch/Tensorflow Pytorch/TF built with a specific Cuda/Cudnn version for Mila's GPUs (see the related paragraph) <p>Note</p> <p>Modules which are nested (../../..) usually depend on other software/module loaded alongside the main module.  No need to load the dependent software, the complex naming scheme allows an automatic detection of the dependent module(s): i.e.: Loading <code>cudnn/7.6/cuda/9.0/tensorrt/7.0</code> will load <code>cudnn/7.6</code> and <code>cuda/9.0</code> alongside <code>python/3.X</code> is a particular dependency which can be served through <code>python/3.X</code> or <code>anaconda/3</code> and is not automatically loaded to let the user pick his favorite flavor.</p>"},{"location":"Userguide_portability/#default-package-location","title":"Default package location","text":"<p>Python by default uses the user site package first and packages provided by <code>module</code> last to not interfere with your installation.  If you want to skip packages installed in your site-packages folder (in your /home directory), you have to start Python with the <code>-s</code> flag.</p> <p>To check which package is loaded at import, you can print <code>package.__file__</code> to get the full path of the package.</p> <p>Example: <pre><code>$ module load pytorch/1.5.0\n$ python -c 'import torch;print(torch.__file__)'\n/home/mila/my_home/.local/lib/python3.7/site-packages/torch/__init__.py   &lt;== package from your own site-package\n</code></pre> Now with the <code>-s</code> flag:</p> <pre><code>$ module load pytorch/1.5.0\n$ python -s -c 'import torch;print(torch.__file__)'\n/cvmfs/ai.mila.quebec/apps/x86_64/debian/pytorch/python3.7-cuda10.1-cudnn7.6-v1.5.0/lib/python3.7/site-packages/torch/__init__.py'\n</code></pre>"},{"location":"Userguide_portability/#on-using-containers","title":"On using containers","text":"<p>Another option for creating portable code is Using containers.</p> <p>Containers are a popular approach at deploying applications by packaging a lot of the required dependencies together. The most popular tool for this is Docker, but Docker cannot be used on the Mila cluster (nor the other clusters from Digital Research Alliance of Canada).</p> <p>One popular mechanism for containerisation on a computational cluster is called Podman.  This is the recommended approach for running containers on the Mila cluster. See the Using containers section for more details.</p>"},{"location":"Userguide_portability_modules/","title":"Userguide portability modules","text":""},{"location":"Userguide_portability_modules/#the-module-command","title":"The module command","text":"<p>For a list of available modules, simply use:</p> <pre><code>$ module avail\n</code></pre> <p>Modules can be loaded using the <code>load</code> command: <pre><code>module load &lt;module&gt;\n</code></pre> To search for a module or a software, use the command <code>spider</code>: <pre><code>module spider search_term\n</code></pre> E.g.: by default, <code>python2</code> will refer to the os-shipped installation of <code>python2.7</code> and <code>python3</code> to <code>python3.10</code>. If you want to use <code>python3.7</code> you can type: <pre><code>module load python/3.7\n</code></pre></p>"},{"location":"Userguide_portability_modules/#available-software","title":"Available Software","text":"<p>Modules are divided in 5 main sections:</p> Section Description Core Base interpreter and software (Python, go, etc...) Compiler Interpreter-dependent software (see the note below) Cuda Toolkits, cudnn and related libraries Pytorch/Tensorflow Pytorch/TF built with a specific Cuda/Cudnn version for Mila's GPUs (see the related paragraph) <p>Note</p> <p>Modules which are nested (../../..) usually depend on other software/module loaded alongside the main module.  No need to load the dependent software, the complex naming scheme allows an automatic detection of the dependent module(s): i.e.: Loading <code>cudnn/7.6/cuda/9.0/tensorrt/7.0</code> will load <code>cudnn/7.6</code> and <code>cuda/9.0</code> alongside <code>python/3.X</code> is a particular dependency which can be served through <code>python/3.X</code> or <code>anaconda/3</code> and is not automatically loaded to let the user pick his favorite flavor.</p>"},{"location":"Userguide_portability_modules/#default-package-location","title":"Default package location","text":"<p>Python by default uses the user site package first and packages provided by <code>module</code> last to not interfere with your installation.  If you want to skip packages installed in your site-packages folder (in your /home directory), you have to start Python with the <code>-s</code> flag.</p> <p>To check which package is loaded at import, you can print <code>package.__file__</code> to get the full path of the package.</p> <p>Example: <pre><code>$ module load pytorch/1.5.0\n$ python -c 'import torch;print(torch.__file__)'\n/home/mila/my_home/.local/lib/python3.7/site-packages/torch/__init__.py   &lt;== package from your own site-package\n</code></pre> Now with the <code>-s</code> flag:</p> <pre><code>$ module load pytorch/1.5.0\n$ python -s -c 'import torch;print(torch.__file__)'\n/cvmfs/ai.mila.quebec/apps/x86_64/debian/pytorch/python3.7-cuda10.1-cudnn7.6-v1.5.0/lib/python3.7/site-packages/torch/__init__.py'\n</code></pre>"},{"location":"Userguide_python/","title":"Userguide python","text":""},{"location":"Userguide_python/#virtual-environments","title":"Virtual environments","text":"<p>A virtual environment in Python is a local, isolated environment in which you can install or uninstall Python packages without interfering with the global environment (or other virtual environments). It usually lives in a directory (location varies depending on whether you use venv, conda or poetry). In order to use a virtual environment, you have to activate it. Activating an environment essentially sets environment variables in your shell so that:</p> <ul> <li><code>python</code> points to the right Python version for that environment (different   virtual environments can use different versions of Python!)</li> <li><code>python</code> looks for packages in the virtual environment</li> <li><code>pip install</code> installs packages into the virtual environment</li> <li>Any shell commands installed via <code>pip install</code> are made available</li> </ul> <p>To run experiments within a virtual environment, you can simply activate it in the script given to <code>sbatch</code>.</p>"},{"location":"Userguide_python/#uv","title":"UV","text":"<p>In many cases, where your dependencies are Python packages, we highly recommend using <code>UV &lt;https://docs.astral.sh/uv&gt;</code>__, a modern package manager for Python.</p> <p>In addition to all the same features as pip, it also manages Python installations, virtual environments, and makes your environments easier to reproduce and reuse across compute clusters.</p> <p>Note</p> <p>UV is not currently available as a module on the Mila or DRAC clusters at the time of writing. To use it, you first need to install it using this command on a cluster login node:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> Pip/virtualenv command UV pip equivalent UV <code>project</code> command (recommended) Create your virtualenv <code>module load python/3.10</code>then <code>python -m venv</code> <code>uv venv</code>_ <code>uv init</code> and <code>uv sync</code> Activate the virtualenv <code>. .venv/bin/activate</code> (same) (same, but often unnecessary) Install a package activate venv then <code>pip install</code> <code>uv pip install</code>_ <code>uv add</code>_ Run a command     (ex. <code>python main.py</code>) <code>module load python</code>, then<code>. &lt;venv&gt;/bin/activate</code>, then  <code>python main.py</code> <code>. &lt;venv&gt;/bin/activate</code>,then <code>python main.py</code> <code>uv run python main.py</code> Where are dependencies declared? Maybe in a <code>requirements.txt</code>, <code>setup.py</code> or <code>pyproject.toml</code> Maybe in a <code>requirements.txt</code>,  <code>setup.py</code> or <code>pyproject.toml</code> always in <code>pyproject.toml</code> Easy to change Python   versions? No somewhat Yes: <code>uv python pin &lt;version&gt;</code> or <code>uv sync --python &lt;version&gt;</code> <p>While you can use UV as a drop-in replacement for pip, we recommend adopting a project-based workflow:</p> <ul> <li> <p>Use <code>uv init</code> to create a new project. A <code>pyproject.toml</code> file will be created. This is where your dependencies are listed.</p> <pre><code>uv init --python=3.12\n</code></pre> </li> <li> <p>Use <code>uv add</code> to add (and <code>uv remove</code> to remove) dependencies to your project. This will update the <code>pyproject.toml</code> file and update the virtual environment.</p> <pre><code>uv add torch\n</code></pre> </li> <li> <p>Use <code>uv run</code> to run commands, for example <code>uv run python train.py</code>.   This will automatically do the following:</p> <ol> <li>Create or update the virtualenv (with the correct Python version) if necessary, based the dependencies in <code>pyproject.toml</code>.</li> <li>Activates the virtualenv.</li> <li>Runs the command you provided, e.g. <code>python train.py</code>.</li> </ol> <pre><code>uv run python main.py\n</code></pre> </li> </ul>"},{"location":"Userguide_python/#pipvirtualenv","title":"Pip/Virtualenv","text":"<p>Pip is the most widely used package manager for Python and each cluster provides several Python versions through the associated module which comes with pip. In order to install new packages, you will first have to create a personal space for them to be stored.  The usual solution (as it is the recommended solution on Digital Research Alliance of Canada clusters) is to use <code>virtual environments &lt;https://virtualenv.pypa.io/en/stable/&gt;</code>_, although using_uv is now the recommended way to manage Python installations, virtual environments and dependencies.</p> <p>Note</p> <p>We recommend you use <code>UV &lt;https://docs.astral.sh/uv&gt;</code>_ to manage your Python virtual environments instead of doing it manually. The previous section gives an overview of how to install it and use it.</p> <p>First, load the Python module you want to use: <pre><code>module load python/3.10\n</code></pre></p> <p>Then, create a virtual environment in your <code>home</code> directory: <pre><code>python -m venv $HOME/&lt;env&gt;\n</code></pre></p> <p>Where <code>&lt;env&gt;</code> is the name of your environment. Finally, activate the environment:</p> <pre><code>source $HOME/&lt;env&gt;/bin/activate\n</code></pre> <p>You can now install any Python package you wish using the <code>pip</code> command, e.g. pytorch :</p> <pre><code>pip install torch torchvision\n</code></pre> <p>Or Tensorflow : <pre><code>pip install tensorflow-gpu\n</code></pre></p>"},{"location":"Userguide_python/#conda","title":"Conda","text":"<p>Another solution for Python is to use miniconda or anaconda which are also available through the <code>module</code> command: (the use of Conda is not recommended for Digital Research Alliance of Canada clusters due to the availability of custom-built packages for pip)</p> <pre><code>$ module load miniconda/3\n[=== Module miniconda/3 loaded ===]\n</code></pre> <p>To create an environment (see here for details) using a specific Python version, you may write: <pre><code>conda create -n &lt;env&gt; python=3.9\n</code></pre> Where <code>&lt;env&gt;</code> is the name of your environment. You can now activate it by doing: <pre><code>conda activate &lt;env&gt;\n</code></pre> You are now ready to install any Python package you want in this environment. For instance, to install PyTorch, you can find the Conda command of any version you want on pytorch's website, e.g:</p> <pre><code>conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n</code></pre> <p>If you make a lot of environments and install/uninstall a lot of packages, it can be good to periodically clean up Conda's cache: <pre><code>conda clean -it\n</code></pre></p>"},{"location":"Userguide_python/#mamba","title":"Mamba","text":"<p>When installing new packages with <code>conda install</code>, conda uses a built-in dependency solver for solving the dependency graph of all packages (and their versions) requested such that package dependency conflicts are avoided.</p> <p>In some cases, especially when there are many packages already installed in a conda environment, conda's built-in dependency solver can struggle to solve the dependency graph, taking several to tens of minutes, and sometimes never solving. In these cases, it is recommended to try libmamba.</p> <p>To install and set the <code>libmamba</code> solver, run the following commands:</p> <pre><code># Install miniconda\n# (you can not use the preinstalled anaconda/miniconda as installing libmamba\n#  requires ownership over the anaconda/miniconda install directory)\n$ wget https://repo.anaconda.com/miniconda/Miniconda3-py310_22.11.1-1-Linux-x86_64.sh\n$ bash Miniconda3-py310_22.11.1-1-Linux-x86_64.sh\n\n# Install libmamba\n$ conda install -n base conda-libmamba-solver\n</code></pre> <p>By default, conda uses the built-in solver when installing packages, even after installing other solvers. To try <code>libmamba</code> once, add <code>--solver=libmamba</code> in your <code>conda install</code> command. For example: <pre><code>conda install tensorflow --solver=libmamba\n</code></pre></p> <p>You can set <code>libmamba</code> as the default solver by adding <code>solver: libmamba</code> to your <code>.condarc</code> configuration file located under your <code>$HOME</code> directory. You can create it if it doesn't exist. You can also run: <pre><code>conda config --set solver libmamba\n</code></pre></p>"},{"location":"Userguide_quick_start/","title":"Quick Start","text":""},{"location":"Userguide_quick_start/#cluster-access","title":"Cluster access","text":"<p>To access the Mila Cluster clusters, you will need a Mila account. Please contact Mila systems administrators if you don't have it already. Our IT support service is available here: https://it-support.mila.quebec/</p> <p>You will also need to complete and return an IT Onboarding Training to get access to the cluster.  Please refer to the Mila Intranet for more informations: https://sites.google.com/mila.quebec/mila-intranet/it-infrastructure/it-onboarding-training</p> <p>Important</p> <p>Your access to the Cluster is granted based on your status at Mila (for students, your status is the same as your main supervisor' status), and on the duration of your stay, set during the creation of your account.  The following have access to the cluster : Current Students of Core Professors - Core Professors - Staff</p>"},{"location":"Userguide_quick_start/#mila-code","title":"mila code","text":"<p>It is recommended to install milatools which will help in the set up of the SSH configuration needed to securely and easily connect to the cluster. <code>milatools</code> also makes it easy to run and debug code on the Mila cluster.</p> <p>First you need to set up your SSH configuration using <code>mila init</code>. The initialization of the SSH configuration is explained on the milatools README.</p> <p>Once that is done, you may run VSCode on the cluster simply by using the Remote-SSH extension and selecting <code>mila-cpu</code> as the host (in step 2).</p> <p><code>mila-cpu</code> allocates a single CPU and 8 GB of RAM. If you need more resources from within VSCode (e.g. to run an ML model in a notebook), then you can use <code>mila code</code>. For example, if you want a GPU, 32G of RAM and 4 cores, run this command in the terminal:</p> <pre><code>mila code path/on/cluster --alloc --gres=gpu:1 --mem=32G -c 4\n</code></pre> <p>The details of the command can be found in the milatools README. Remember that you need to first set up your SSH configuration using <code>mila init</code> before the <code>mila code</code> command can be used.</p>"},{"location":"Userguide_quick_start/#using-a-terminal","title":"Using a Terminal","text":"<p>While VSCode provides a graphical interface for writing and debugging code on the cluster, working on the cluster will require using a terminal to navigate the filesystem, run commands, and manage jobs.</p> <p>To open a terminal session on the cluster, connect using:</p> <pre><code>ssh mila\n</code></pre> <p>This will connect you to a login node where you can run commands, submit jobs, and navigate the cluster filesystem.</p>"},{"location":"Userguide_quick_start/#next-steps","title":"Next Steps","text":"<p>Once you have access to the cluster, you may want to:</p> <ul> <li> <p>Set up a framework: For a quick example of setting up PyTorch on the     cluster, see PyTorch Setup.</p> </li> <li> <p>Keep these references handy:</p> <pre><code>* The [Cheat Sheet](Cheatsheet.md) provides a quick reference for common\n    commands and information about the Mila and DRAC clusters.\n</code></pre> <ul> <li>For a comprehensive reference of common terminal commands, see the     command line cheat sheet.</li> </ul> </li> </ul> <p>Note</p> <p>Before running a minimal example, make sure to read Running your code, which explains how to submit jobs using Slurm and provides essential information about job submission arguments, partitions, and useful commands.</p>"},{"location":"Userguide_running_code/","title":"Running your code","text":""},{"location":"Userguide_running_code/#slurm-commands-guide","title":"SLURM commands guide","text":""},{"location":"Userguide_running_code/#basic-usage","title":"Basic Usage","text":"<p>The SLURM documentation provides extensive information on the available commands to query the cluster status or submit jobs.</p> <p>Below are some basic examples of how to use SLURM.</p>"},{"location":"Userguide_running_code/#submitting-jobs","title":"Submitting jobs","text":""},{"location":"Userguide_running_code/#batch-job","title":"Batch job","text":"<p>In order to submit a batch job, you have to create a script containing the main command(s) you would like to execute on the allocated resources/nodes.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --output=job_output.txt\n#SBATCH --error=job_error.txt\n#SBATCH --ntasks=1\n#SBATCH --time=10:00\n#SBATCH --mem=100Gb\n\nmodule load python/3.10\npython my_script.py\n</code></pre> <p>Your job script is then submitted to SLURM with <code>sbatch</code>.</p> <pre><code>$ sbatch job_script\nsbatch: Submitted batch job 4323674\n</code></pre> <p>The working directory of the job will be the one where your executed <code>sbatch</code>.</p> <p>Tip</p> <p>Slurm directives can be specified on the command line alongside <code>sbatch</code> or inside the job script with a line starting with <code>#SBATCH</code>.</p>"},{"location":"Userguide_running_code/#interactive-job","title":"Interactive job","text":"<p>Workload managers usually run batch jobs to avoid having to watch its progression and let the scheduler run it as soon as resources are available. If you want to get access to a shell while leveraging cluster resources, you can submit an interactive jobs where the main executable is a shell with the srun or salloc commands.</p> <pre><code>salloc\n</code></pre> <p>Will start an interactive job on the first node available with the default resources set in SLURM (1 task/1 CPU).  <code>srun</code> accepts the same arguments as <code>sbatch</code> with the exception that the environment is not passed.</p> <p>Tip</p> <p>To pass your current environment to an interactive job, add <code>--preserve-env</code> to <code>srun</code>.</p> <p><code>salloc</code> can also be used and is mostly a wrapper around <code>srun</code> if provided without more info but it gives more flexibility if for example you want to get an allocation on multiple nodes.</p>"},{"location":"Userguide_running_code/#job-submission-arguments","title":"Job submission arguments","text":"<p>In order to accurately select the resources for your job, several arguments are available. The most important ones are:</p> Argument Description <code>-n</code>, <code>--ntasks=&lt;number&gt;</code> The number of task in your script, usually =1 <code>-c</code>, <code>--cpus-per-task=&lt;ncpus&gt;</code> The number of cores for each task <code>-t</code>, <code>--time=&lt;time&gt;</code> Time requested for your job <code>--mem=&lt;size[units]&gt;</code> Memory requested for all your tasks <code>--gres=&lt;list&gt;</code> Select generic resources such as GPUs for your job: <code>--gres=gpu:GPU_MODEL</code> <p>Tip</p> <p>Always consider requesting the adequate amount of resources to improve the scheduling of your job (small jobs always run first).</p>"},{"location":"Userguide_running_code/#checking-job-status","title":"Checking job status","text":"<p>To display jobs currently in queue, use <code>squeue</code> and to get only your jobs type</p> <pre><code>$ squeue -u $USER\n JOBID   USER          NAME    ST  START_TIME         TIME NODES CPUS TRES_PER_NMIN_MEM NODELIST (REASON) COMMENT\n 133     my_username   myjob   R   2019-03-28T18:33   0:50     1    2        N/A  7000M node1 (None) (null)\n</code></pre> <p>Note</p> <p>The maximum number of jobs able to be submitted to the system per user is 1000 (MaxSubmitJobs=1000) at any given time from the given association. If this limit is reached, new submission requests will be denied until existing jobs in this association complete.</p>"},{"location":"Userguide_running_code/#removing-a-job","title":"Removing a job","text":"<p>To cancel your job simply use <code>scancel</code></p> <pre><code>scancel 4323674\n</code></pre>"},{"location":"Userguide_running_code/#partitioning","title":"Partitioning","text":"<p>Since we don't have many GPUs on the cluster, resources must be shared as fairly as possible.  The <code>--partition=/-p</code> flag of SLURM allows you to set the priority you need for a job.  Each job assigned with a priority can preempt jobs with a lower priority: <code>unkillable &gt; main &gt; long</code>. Once preempted, your job is killed without notice and is automatically re-queued on the same partition until resources are available. (To leverage a different preemption mechanism, see the Handling preemption )</p> Flag Max Resource Usage Max Time Note <code>--partition=unkillable</code> 6  CPUs, mem=32G,  1 GPU 2 days <code>--partition=unkillable-cpu</code> 2  CPUs, mem=16G 2 days CPU-only jobs <code>--partition=short-unkillable</code> mem=1000G, 4 GPUs 3 hours (!) Large but short jobs <code>--partition=main</code> 8  CPUs, mem=48G,  2 GPUs 5 days <code>--partition=main-cpu</code> 8  CPUs, mem=64G 5 days CPU-only jobs <code>--partition=long</code> no limit of resources 7 days <code>--partition=long-cpu</code> no limit of resources 7 days CPU-only jobs About outdated partitions (<code>cpu_jobs</code>, <code>cpu_jobs_low</code>, etc.) <p>Historically, before the 2022 introduction of CPU-only nodes (e.g. the <code>cn-f</code>  series), CPU jobs ran side-by-side with the GPU jobs on GPU nodes. To prevent them obstructing any GPU job, they were always lowest-priority and preemptible. This was implemented by automatically assigning them to one of the now-obsolete partitions <code>cpu_jobs</code>, <code>cpu_jobs_low</code> or <code>cpu_jobs_low-grace</code>. Do not use these partition names anymore. Prefer the <code>*-cpu</code> partition names defined above.</p> <p>For backwards-compatibility purposes, the legacy partition names are translated to their effective equivalent <code>long-cpu</code>, but they will eventually be removed entirely.</p> <p>Note</p> <p>As a convenience, should you request the <code>unkillable</code>, <code>main</code> or <code>long</code> partition for a CPU-only job, the partition will be translated to its <code>-cpu</code> equivalent automatically.</p> <p>For instance, to request an unkillable job with 1 GPU, 4 CPUs, 10G of RAM and 12h of computation do:</p> <pre><code>sbatch --gres=gpu:1 -c 4 --mem=10G -t 12:00:00 --partition=unkillable &lt;job.sh&gt;\n</code></pre> <p>You can also make it an interactive job using <code>salloc</code>:</p> <pre><code>salloc --gres=gpu:1 -c 4 --mem=10G -t 12:00:00 --partition=unkillable\n</code></pre> <p>The Mila cluster has many different types of nodes/GPUs. To request a specific type of node/GPU, you can add specific feature requirements to your job submission command.</p> <p>To access those special nodes you need to request them explicitly by adding the flag <code>--constraint=&lt;name&gt;</code>.  The full list of nodes in the Mila Cluster can be accessed Node profile description.</p> <p>Examples:</p> <p>To request a machine with 2 GPUs using NVLink, you can use</p> <pre><code>sbatch -c 4 --gres=gpu:2 --constraint=nvlink\n</code></pre> <p>To request a DGX system with 8 A100 GPUs, you can use</p> <pre><code>sbatch -c 16 --gres=gpu:8 --constraint=\"dgx&amp;ampere\"\n</code></pre> Feature Particularities 12gb/32gb/40gb/48gb/80gb Request a specific amount of GPU memory volta/turing/ampere Request a specific GPU architecture nvlink Machine with GPUs using the NVLink interconnect technology dgx NVIDIA DGX system with DGX OS"},{"location":"Userguide_running_code/#information-on-partitionsnodes","title":"Information on partitions/nodes","text":"<p><code>sinfo</code> provides most of the information about available nodes and partitions/queues to submit jobs to.</p> <p>Partitions are a group of nodes usually sharing similar features. On a partition, some job limits can be applied which will override those asked for a job (i.e. max time, max CPUs, etc...)</p> <p>To display available partitions, simply use</p> <pre><code>$ sinfo\nPARTITION AVAIL TIMELIMIT NODES STATE  NODELIST\nbatch     up     infinite     2 alloc  node[1,3,5-9]\nbatch     up     infinite     6 idle   node[10-15]\ncpu       up     infinite     6 idle   cpu_node[1-15]\ngpu       up     infinite     6 idle   gpu_node[1-15]\n</code></pre> <p>To display available nodes and their status, you can use</p> <pre><code>$ sinfo -N -l\nNODELIST    NODES PARTITION STATE  CPUS MEMORY TMP_DISK WEIGHT FEATURES REASON\nnode[1,3,5-9]   2 batch     allocated 2    246    16000     0  (null)   (null)\nnode[2,4]       2 batch     drain     2    246    16000     0  (null)   (null)\nnode[10-15]     6 batch     idle      2    246    16000     0  (null)   (null)\n...\n</code></pre> <p>And to get statistics on a job running or terminated, use <code>sacct</code> with some of the fields you want to display</p> <pre><code>$ sacct --format=User,JobID,Jobname,partition,state,time,start,end,elapsed,nnodes,ncpus,nodelist,workdir -u $USER\n     User        JobID    JobName  Partition      State  Timelimit               Start                 End    Elapsed   NNodes      NCPUS        NodeList              WorkDir\n--------- ------------ ---------- ---------- ---------- ---------- ------------------- ------------------- ---------- -------- ---------- --------------- --------------------\nmy_usern+ 2398         run_extra+      batch    RUNNING 130-05:00+ 2019-03-27T18:33:43             Unknown 1-01:07:54        1         16 node9           /home/mila/my_usern+\nmy_usern+ 2399         run_extra+      batch    RUNNING 130-05:00+ 2019-03-26T08:51:38             Unknown 2-10:49:59        1         16 node9           /home/mila/my_usern+\n</code></pre> <p>Or to get the list of all your previous jobs, use the <code>--start=YYYY-MM-DD</code> flag. You can check <code>sacct(1)</code> for further information about additional time formats.</p> <pre><code>sacct -u $USER --start=2019-01-01\n</code></pre> <p><code>scontrol</code> can be used to provide specific information on a job (currently running or recently terminated)</p> <pre><code>$ scontrol show job 43123\nJobId=43123 JobName=python_script.py\nUserId=my_username(1500000111) GroupId=student(1500000000) MCS_label=N/A\nPriority=645895 Nice=0 Account=my_username QOS=normal\nJobState=RUNNING Reason=None Dependency=(null)\nRequeue=1 Restarts=3 BatchFlag=1 Reboot=0 ExitCode=0:0\nRunTime=2-10:41:57 TimeLimit=130-05:00:00 TimeMin=N/A\nSubmitTime=2019-03-26T08:47:17 EligibleTime=2019-03-26T08:49:18\nAccrueTime=2019-03-26T08:49:18\nStartTime=2019-03-26T08:51:38 EndTime=2019-08-03T13:51:38 Deadline=N/A\nPreemptTime=None SuspendTime=None SecsPreSuspend=0\nLastSchedEval=2019-03-26T08:49:18\nPartition=slurm_partition AllocNode:Sid=login-node-1:14586\nReqNodeList=(null) ExcNodeList=(null)\nNodeList=node2\nBatchHost=node2\nNumNodes=1 NumCPUs=16 NumTasks=1 CPUs/Task=16 ReqB:S:C:T=0:0:*:*\nTRES=cpu=16,mem=32000M,node=1,billing=3\nSocks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*\nMinCPUsNode=16 MinMemoryNode=32000M MinTmpDiskNode=0\nFeatures=(null) DelayBoot=00:00:00\nOverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\nWorkDir=/home/mila/my_username\nStdErr=/home/mila/my_username/slurm-43123.out\nStdIn=/dev/null\nStdOut=/home/mila/my_username/slurm-43123.out\nPower=\n</code></pre> <p>Or more info on a node and its resources</p> <pre><code>$ scontrol show node node9\nNodeName=node9 Arch=x86_64 CoresPerSocket=4\nCPUAlloc=16 CPUTot=16 CPULoad=1.38\nAvailableFeatures=(null)\nActiveFeatures=(null)\nGres=(null)\nNodeAddr=10.252.232.4 NodeHostName=mila20684000000 Port=0 Version=18.08\nOS=Linux 4.15.0-1036 #38-Ubuntu SMP Fri Dec 7 02:47:47 UTC 2018\nRealMemory=32000 AllocMem=32000 FreeMem=23262 Sockets=2 Boards=1\nState=ALLOCATED+CLOUD ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A\nPartitions=slurm_partition\nBootTime=2019-03-26T08:50:01 SlurmdStartTime=2019-03-26T08:51:15\nCfgTRES=cpu=16,mem=32000M,billing=3\nAllocTRES=cpu=16,mem=32000M\nCapWatts=n/a\nCurrentWatts=0 LowestJoules=0 ConsumedJoules=0\nExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s\n</code></pre>"},{"location":"Userguide_running_code/#useful-commands","title":"Useful Commands","text":"Get an interactive job and give you a shell. (ssh like) CPU only<pre><code>salloc\n</code></pre> Get an interactive job with one GPU, 2 CPUs and 12000 MB RAM<pre><code>salloc --gres=gpu:1 -c 2 --mem=12000\n</code></pre> <p>start a batch job (same options as salloc)<pre><code>sbatch\n</code></pre> Re-attach a dropped interactive job<pre><code>sattach \\--pty &lt;jobid&gt;.0\n</code></pre> status of all nodes<pre><code>sinfo\n</code></pre> List GPU type and FEATURES that you can request<pre><code>sinfo -Ogres:27,nodelist,features -tidle,mix,alloc\n</code></pre> (Custom) List available gpus<pre><code>savail\n</code></pre> Cancel a job<pre><code>scancel &lt;jobid&gt;\n</code></pre> summary status of all active jobs<pre><code>squeue\n</code></pre> summary status of all YOUR active jobs<pre><code>squeue -u $USER\n</code></pre> summary status of a specific job<pre><code>squeue -j &lt;jobid&gt;\n</code></pre> status of all jobs including requested resources (see the SLURM squeue doc for all output options)<pre><code>squeue -Ojobid,name,username,partition,state,timeused,nodelist,gres,tres\n</code></pre> Detailed status of a running job<pre><code>scontrol show job &lt;jobid&gt;\n</code></pre> Get the node where a finished job ran<pre><code>sacct -j &lt;job_id&gt; -o NodeList\n</code></pre> Find info about old jobs<pre><code>sacct -u $USER -S &lt;start_time&gt; -E &lt;stop_time&gt;\n</code></pre> List of current and recent jobs<pre><code>sacct -oJobID,JobName,User,Partition,Node,State\n</code></pre></p>"},{"location":"Userguide_running_code/#special-gpu-requirements","title":"Special GPU requirements","text":"<p>Specific GPU architecture and memory can be easily requested through the <code>--gres</code> flag by using either</p> <ul> <li><code>--gres=gpu:architecture:number</code></li> <li><code>--gres=gpu:memory:number</code></li> <li><code>--gres=gpu:model:number</code></li> </ul> <p>Example:</p> <p>To request 1 GPU with at least 48GB of memory use</p> <pre><code>sbatch -c 4 --gres=gpu:48gb:1\n</code></pre> <p>The full list of GPU and their features can be accessed here .</p>"},{"location":"Userguide_running_code/#example-script","title":"Example script","text":"<p>Here is a <code>sbatch</code> script that follows good practices on the Mila cluster:</p> <p>Note</p> <p>This example is a bit outdated and uses Conda. In practice, we now recommend that you use uv to manage your Python environments. See the Minimal Examples Section for more information.</p> <pre><code>#!/bin/bash\n\n#SBATCH --partition=unkillable                           # Ask for unkillable job\n#SBATCH --cpus-per-task=2                                # Ask for 2 CPUs\n#SBATCH --gres=gpu:1                                     # Ask for 1 GPU\n#SBATCH --mem=10G                                        # Ask for 10 GB of RAM\n#SBATCH --time=3:00:00                                   # The job will run for 3 hours\n#SBATCH -o /network/scratch/&lt;u&gt;/&lt;username&gt;/slurm-%j.out  # Write the log on scratch\n\n# 1. Load the required modules\nmodule --quiet load anaconda/3\n\n# 2. Load your environment\nconda activate \"&lt;env_name&gt;\"\n\n# 3. Copy your dataset on the compute node\ncp /network/datasets/&lt;dataset&gt; $SLURM_TMPDIR\n\n# 4. Launch your job, tell it to save the model in $SLURM_TMPDIR\n#    and look for the dataset into $SLURM_TMPDIR\npython main.py --path $SLURM_TMPDIR --data_path $SLURM_TMPDIR\n\n# 5. Copy whatever you want to save on $SCRATCH\ncp $SLURM_TMPDIR/&lt;to_save&gt; /network/scratch/&lt;u&gt;/&lt;username&gt;/\n</code></pre>"},{"location":"Userguide_sharing_data/","title":"Sharing Data with ACLs","text":"<p>Regular permissions bits are extremely blunt tools: They control access through only three sets of bits owning user, owning group and all others. Therefore, access is either too narrow (<code>0700</code> allows access only by oneself) or too wide (<code>770</code> gives all permissions to everyone in the same group, and <code>777</code> to literally everyone).</p>"},{"location":"Userguide_sharing_data/#using-acls","title":"Using ACLs","text":"<p>ACLs (Access Control Lists) are an expansion of the permissions bits that allow more fine-grained, granular control of accesses to a file. They can be used to permit specific users access to files and folders even if conservative default permissions would have denied them such access.</p> <p>As an illustrative example, to use ACLs to allow <code>$USER</code> (oneself) to share with <code>$USER2</code> (another person) a \"playground\" folder hierarchy in Mila's scratch filesystem at a location</p> <p><code>$SCRATCH/X/Y/Z/...</code></p> <p>in a safe and secure fashion that allows both users to read, write, execute, search and delete each others' files:</p> <p>1. Grant oneself permissions to access any future files/folders created by the other (or oneself) (<code>-d</code> renders this permission a \"default\" / inheritable one)</p> <pre><code>setfacl -Rdm user:${USER}:rwx  $SCRATCH/X/Y/Z/\n</code></pre> <p>Note</p> <p>The importance of doing this seemingly-redundant step first is that files and folders are always owned by only one person, almost always their creator (the UID will be the creator's, the GID typically as well). If that user is not yourself, you will not have access to those files unless the other person specifically gives them to you -- or these files inherited a default ACL allowing you full access.</p> <p>This is the inherited, default ACL serving that purpose.</p> <p>2. Grant the other permission to access any future files/folders created   by the other (or oneself) (<code>-d</code> renders this permission a \"default\" / inheritable one)</p> <pre><code>setfacl -Rdm user:${USER2:?defineme}:rwx $SCRATCH/X/Y/Z/\n</code></pre> <p>3. Grant the other permission to access any existing files/folders created   by oneself. Such files and folders were created before the new default ACLs were added above and thus did not inherit them from their parent folder at the moment of their creation.</p> <pre><code>setfacl -Rm  user:${USER2:?defineme}:rwx $SCRATCH/X/Y/Z/\n</code></pre> <p>4. Grant another permission to search through one's hierarchy down to the shared location in question.</p> <ul> <li>Non-recursive (!!!!)</li> <li>May also grant <code>:rx</code> in unlikely event others listing your folders on the   path is not troublesome or desirable.</li> </ul> <pre><code>setfacl -m   user:${USER2:?defineme}:x   $SCRATCH/X/Y/\nsetfacl -m   user:${USER2:?defineme}:x   $SCRATCH/X/\nsetfacl -m   user:${USER2:?defineme}:x   $SCRATCH\n</code></pre> <p>Note</p> <p>The purpose of granting permissions first for future files and then for existing files is to prevent a race condition whereby after the first <code>setfacl</code> command the other person could create files to which the second <code>setfacl</code> command does not apply.</p> <p>Note</p> <p>In order to access a file, all folders from the root (<code>/</code>) down to the parent folder in question must be searchable (<code>+x</code>) by the concerned user. This is already the case for all users for folders such as <code>/</code>, <code>/network</code> and <code>/network/scratch</code>, but users must explicitly grant access to some or all users either through base permissions or by adding ACLs, for at least <code>/network/scratch/${USER:0:1}/$USER</code> (= <code>$SCRATCH</code>), <code>$HOME</code> and subfolders.</p> <p>To bluntly allow all users to search through a folder (think twice!), the following command can be used:</p> <pre><code>chmod a+X $SCRATCH\n</code></pre> <p>Note</p> <p>For more information on <code>setfacl</code> and path resolution/access checking, consider the following documentation viewing commands:</p> <ul> <li><code>man setfacl</code></li> <li><code>man path_resolution</code></li> </ul>"},{"location":"Userguide_sharing_data/#viewing-and-verifying-acls","title":"Viewing and Verifying ACLs","text":"<pre><code>getfacl /path/to/folder/or/file\n           1:  # file: somedir/\n           2:  # owner: lisa\n           3:  # group: staff\n           4:  # flags: -s-\n           5:  user::rwx\n           6:  user:joe:rwx               #effective:r-x\n           7:  group::rwx                 #effective:r-x\n           8:  group:cool:r-x\n           9:  mask::r-x\n          10:  other::r-x\n          11:  default:user::rwx\n          12:  default:user:joe:rwx       #effective:r-x\n          13:  default:group::r-x\n          14:  default:mask::r-x\n          15:  default:other::---\n</code></pre> <p>Note</p> <ul> <li><code>man getfacl</code></li> </ul>"},{"location":"Userguide_singularity/","title":"Singularity","text":""},{"location":"Userguide_singularity/#overview","title":"Overview","text":""},{"location":"Userguide_singularity/#what-is-singularity","title":"What is Singularity?","text":"<p>Running Docker on SLURM is a security problem (e.g. running as root, being able to mount any directory).  The alternative is to use Singularity, which is a popular solution in the world of HPC.</p> <p>There is a good level of compatibility between Docker and Singularity, and we can find many exaggerated claims about able to convert containers from Docker to Singularity without any friction. Oftentimes, Docker images from DockerHub are 100% compatible with Singularity, and they can indeed be used without friction, but things get messy when we try to convert our own Docker build files to Singularity recipes.</p>"},{"location":"Userguide_singularity/#links-to-official-documentation","title":"Links to official documentation","text":"<ul> <li>official Singularity user guide   (this is the one you will use most often)</li> <li>official Singularity admin guide</li> </ul>"},{"location":"Userguide_singularity/#overview-of-the-steps-used-in-practice","title":"Overview of the steps used in practice","text":"<p>Most often, the process to create and use a Singularity container is:</p> <ul> <li> <p>on your Linux computer (at home or work)</p> </li> <li> <p>select a Docker image from DockerHub (e.g. pytorch/pytorch)</p> </li> <li>make a recipe file for Singularity that starts with that DockerHub image</li> <li>build the recipe file, thus creating the image file (e.g. <code>my-pytorch-image.sif</code>)</li> <li>test your singularity container before send it over to the cluster</li> <li> <p><code>rsync -av my-pytorch-image.sif &lt;login-node&gt;:Documents/my-singularity-images</code></p> </li> <li> <p>on the login node for that cluster</p> </li> <li> <p>queue your jobs with <code>sbatch ...</code></p> </li> <li>(note that your jobs will copy over the <code>my-pytorch-image.sif</code> to $SLURM_TMPDIR     and will then launch Singularity with that image)</li> <li>do something else while you wait for them to finish</li> <li>queue more jobs with the same <code>my-pytorch-image.sif</code>,     reusing it many times over</li> </ul> <p>In the following sections you will find specific examples or tips to accomplish in practice the steps highlighted above.</p>"},{"location":"Userguide_singularity/#nope-not-on-macos","title":"Nope, not on MacOS","text":"<p>Singularity does not work on MacOS, as of the time of this writing in 2021. Docker does not actually run on MacOS, but there Docker silently installs a virtual machine running Linux, which makes it a pleasant experience, and the user does not need to care about the details of how Docker does it.</p> <p>Given its origins in HPC, Singularity does not provide that kind of seamless experience on MacOS, even though it's technically possible to run it inside a Linux virtual machine on MacOS.</p>"},{"location":"Userguide_singularity/#where-to-build-images","title":"Where to build images","text":"<p>Building Singularity images is a rather heavy task, which can take 20 minutes if you have a lot of steps in your recipe. This makes it a bad task to run on the login nodes of our clusters, especially if it needs to be run regularly.</p> <p>On the Mila cluster, we are lucky to have unrestricted internet access on the compute nodes, which means that anyone can request an interactive CPU node (no need for GPU) and build their images there without problem.</p> <p>Warning</p> <p>Do not build Singularity images from scratch every time your run a job in a large batch.  This will be a colossal waste of GPU time as well as internet bandwidth. If you setup your workflow properly (e.g. using bind paths for your code and data), you can spend months reusing the same Singularity image <code>my-pytorch-image.sif</code>.</p>"},{"location":"Userguide_singularity/#building-the-containers","title":"Building the containers","text":"<p>Building a container is like creating a new environment except that containers are much more powerful since they are self-contained systems.  With singularity, there are two ways to build containers.</p> <p>The first one is by yourself, it's like when you got a new Linux laptop and you don't really know what you need, if you see that something is missing, you install it. Here you can get a vanilla container with Ubuntu called a sandbox, you log in and you install each packages by yourself.  This procedure can take time but will allow you to understand how things work and what you need. This is recommended if you need to figure out how things will be compiled or if you want to install packages on the fly. We'll refer to this procedure as singularity sandboxes.</p> <p>The second way is more like you know what you want, so you write a list of everything you need, you send it to singularity and it will install everything for you. Those lists are called singularity recipes.</p>"},{"location":"Userguide_singularity/#first-way-build-and-use-a-sandbox","title":"First way: Build and use a sandbox","text":"<p>You might ask yourself: On which machine should I build a container?</p> <p>First of all, you need to choose where you'll build your container. This operation requires memory and high cpu usage.</p> <p>Do NOT build containers on any login nodes!</p> <ul> <li> <p>(Recommended for beginner) If you need to use apt-get, you should build   the container on your laptop with sudo privileges. You'll only need to   install singularity on your laptop. Windows/Mac users can look <code>there</code>_ and   Ubuntu/Debian users can use directly:   <pre><code>sudo apt-get install singularity-container\n</code></pre></p> </li> <li> <p>If you can't install singularity on your laptop and you don't need     apt-get, you can reserve a cpu node on the Mila cluster to build your     container.</p> <p>In this case, in order to avoid too much I/O over the network, you should define the singularity cache locally:</p> <pre><code>export SINGULARITY_CACHEDIR=$SLURM_TMPDIR\n</code></pre> </li> <li> <p>If you can't install singularity on your laptop and you want to use   apt-get, you can use <code>singularity-hub</code> to build your containers and read   Recipe_section.</p> </li> </ul>"},{"location":"Userguide_singularity/#download-containers-from-the-web","title":"Download containers from the web","text":"<p>Hopefully, you may not need to create containers from scratch as many have been already built for the most common deep learning software. You can find most of them on dockerhub.</p> <p>Go on dockerhub and select the container you want to pull.</p> <p>For example, if you want to get the latest PyTorch version with GPU support (Replace runtime by devel if you need the full Cuda toolkit):</p> <pre><code>singularity pull docker://pytorch/pytorch:1.0.1-cuda10.0-cudnn7-runtime\n</code></pre> <p>Or the latest TensorFlow:</p> <pre><code>singularity pull docker://tensorflow/tensorflow:latest-gpu-py3\n</code></pre> <p>Currently the pulled image <code>pytorch.simg</code> or <code>tensorflow.simg</code> is read-only meaning that you won't be able to install anything on it.  Starting now, PyTorch will be taken as example. If you use TensorFlow, simply replace every pytorch occurrences by tensorflow.</p>"},{"location":"Userguide_singularity/#how-to-add-or-install-stuff-in-a-container","title":"How to add or install stuff in a container","text":"<p>The first step is to transform your read only container <code>pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</code> in a writable version that will allow you to add packages.</p> <p>Warning</p> <p>Depending on the version of singularity you are using, singularity will build a container with the extension .simg or .sif. If you're using .sif files, replace every occurences of .simg by .sif.</p> <p>Tip</p> <p>If you want to use apt-get you have to put sudo ahead of the  following commands</p> <p>This command will create a writable image in the folder <code>pytorch</code>. <pre><code>singularity build --sandbox pytorch pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg\n</code></pre> Then you'll need the following command to log inside the container. <pre><code>singularity shell --writable -H $HOME:/home pytorch\n</code></pre> Once you get into the container, you can use pip and install anything you need (Or with <code>apt-get</code> if you built the container with sudo).</p> <p>Warning</p> <p>Singularity mounts your home folder, so if you install things into the <code>$HOME</code> of your container, they will be installed in your real <code>$HOME</code>!</p> <p>You should install your stuff in /usr/local instead.</p>"},{"location":"Userguide_singularity/#creating-useful-directories","title":"Creating useful directories","text":"<p>One of the benefits of containers is that you'll be able to use them across different clusters. However for each cluster the datasets and experiments folder location can be different. In order to be invariant to those locations, we will create some useful mount points inside the container:</p> <pre><code>&lt;Singularity_container&gt;$ mkdir /dataset\n&lt;Singularity_container&gt;$ mkdir /tmp_log\n&lt;Singularity_container&gt;$ mkdir /final_log\n</code></pre> <p>From now, you won't need to worry anymore when you write your code to specify where to pick up your dataset. Your dataset will always be in <code>/dataset</code> independently of the cluster you are using.</p>"},{"location":"Userguide_singularity/#testing","title":"Testing","text":"<p>If you have some code that you want to test before finalizing your container, you have two choices.  You can either log into your container and run Python code inside it with: <pre><code>singularity shell --nv pytorch\n</code></pre> Or you can execute your command directly with <pre><code>singularity exec --nv pytorch Python YOUR_CODE.py\n</code></pre></p> <p>Tip</p> <p>---nv allows the container to use gpus. You don't need this if you don't plan to use a gpu.</p> <p>Warning</p> <p>Don't forget to clear the cache of the packages you installed in the containers.</p>"},{"location":"Userguide_singularity/#creating-a-new-image-from-the-sandbox","title":"Creating a new image from the sandbox","text":"<p>Once everything you need is installed inside the container, you need to convert it back to a read-only singularity image with:</p> <pre><code>singularity build pytorch_final.simg pytorch\n</code></pre>"},{"location":"Userguide_singularity/#second-way-use-recipes","title":"Second way: Use recipes","text":"<p>A singularity recipe is a file including specifics about installation software, environment variables, files to add, and container metadata.  It is a starting point for designing any custom container. Instead of pulling a container and installing your packages manually, you can specify in this file the packages you want and then build your container from this file.</p> <p>Here is a toy example of a singularity recipe installing some stuff:</p> <pre><code>################# Header: Define the base system you want to use ################\n# Reference of the kind of base you want to use (e.g., docker, debootstrap, shub).\nBootstrap: docker\n# Select the docker image you want to use (Here we choose tensorflow)\nFrom: tensorflow/tensorflow:latest-gpu-py3\n\n################# Section: Defining the system #################################\n# Commands in the %post section are executed within the container.\n%post\n        echo \"Installing Tools with apt-get\"\n        apt-get update\n        apt-get install -y cmake libcupti-dev libyaml-dev wget unzip\n        apt-get clean\n        echo \"Installing things with pip\"\n        pip install tqdm\n        echo \"Creating mount points\"\n        mkdir /dataset\n        mkdir /tmp_log\n        mkdir /final_log\n\n# Environment variables that should be sourced at runtime.\n%environment\n        # use bash as default shell\n        SHELL=/bin/bash\n        export SHELL\n</code></pre> <p>A recipe file contains two parts: the <code>header</code> and <code>sections</code>. In the <code>header</code> you specify which base system you want to use, it can be any docker or singularity container. In <code>sections</code>, you can list the things you want to install in the subsection <code>post</code> or list the environment's variable you need to source at each runtime in the subsection <code>environment</code>. For a more detailed description, please look at the singularity documentation.</p> <p>In order to build a singularity container from a singularity recipe file, you should use: <pre><code>sudo singularity build &lt;NAME_CONTAINER&gt; &lt;YOUR_RECIPE_FILES&gt;\n</code></pre></p> <p>Warning</p> <p>You always need to use sudo when you build a container from a    recipe. As there is no access to sudo on the cluster, a personal computer or    the use singularity hub is needed to build a container</p>"},{"location":"Userguide_singularity/#build-recipe-on-singularity-hub","title":"Build recipe on singularity hub","text":"<p>Singularity hub allows users to build containers from recipes directly on singularity-hub's cloud meaning that you don't need to build containers by yourself.  You need to register on singularity-hub and link your singularity-hub account to your GitHub account, then:</p> <ol> <li>Create a new github repository.</li> <li>Add a collection on singularity-hub and select the github repository your created.</li> <li> <p>Clone the github repository on your computer.     <pre><code>$ git clone &lt;url&gt;\n</code></pre></p> </li> <li> <p>Write the singularity recipe and save it as a file named Singularity.</p> </li> <li> <p>Git add Singularity, commit and push on the master branch</p> <pre><code>$ git add Singularity\n$ git commit\n$ git push origin master\n</code></pre> </li> </ol> <p>At this point, robots from singularity-hub will build the container for you, you will be able to download your container from the website or directly with: <pre><code>singularity pull shub://&lt;github_username&gt;/&lt;repository_name&gt;\n</code></pre></p>"},{"location":"Userguide_singularity/#example-recipe-with-openai-gym-mujoco-and-miniworld","title":"Example: Recipe with OpenAI gym, MuJoCo and Miniworld","text":"<p>Here is an example on how you can use a singularity recipe to install complex environment such as OpenAI gym, MuJoCo and Miniworld on a PyTorch based container. In order to use MuJoCo, you'll need to copy the key stored on the Mila cluster in <code>/ai/apps/mujoco/license/mjkey.txt</code> to your current directory.</p> <pre><code>#This is a dockerfile that sets up a full Gym install with test dependencies\nBootstrap: docker\n\n# Here we ll build our container upon the pytorch container\nFrom: pytorch/pytorch:1.0-cuda10.0-cudnn7-runtime\n\n# Now we'll copy the mjkey file located in the current directory inside the container's root\n# directory\n%files\n        mjkey.txt\n\n# Then we put everything we need to install\n%post\n        export PATH=$PATH:/opt/conda/bin\n        apt -y update &amp;&amp; \\\n        apt install -y keyboard-configuration &amp;&amp; \\\n        apt install -y \\\n        python3-dev \\\n        python-pyglet \\\n        python3-opengl \\\n        libhdf5-dev \\\n        libjpeg-dev \\\n        libboost-all-dev \\\n        libsdl2-dev \\\n        libosmesa6-dev \\\n        patchelf \\\n        ffmpeg \\\n        xvfb \\\n        libhdf5-dev \\\n        openjdk-8-jdk \\\n        wget \\\n        git \\\n        unzip &amp;&amp; \\\n        apt clean &amp;&amp; \\\n        rm -rf /var/lib/apt/lists/*\n        pip install h5py\n\n        # Download Gym and MuJoCo\n        mkdir /Gym &amp;&amp; cd /Gym\n        git clone https://github.com/openai/gym.git || true &amp;&amp; \\\n        mkdir /Gym/.mujoco &amp;&amp; cd /Gym/.mujoco\n        wget https://www.roboti.us/download/mjpro150_linux.zip  &amp;&amp; \\\n        unzip mjpro150_linux.zip &amp;&amp; \\\n        wget https://www.roboti.us/download/mujoco200_linux.zip &amp;&amp; \\\n        unzip mujoco200_linux.zip &amp;&amp; \\\n        mv mujoco200_linux mujoco200\n\n        # Export global environment variables\n        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n        cp /mjkey.txt /Gym/.mujoco/mjkey.txt\n        # Install Python dependencies\n        wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt\n        pip install -r requirements.txt\n        # Install Gym and MuJoCo\n        cd /Gym/gym\n        pip install -e '.[all]'\n        # Change permission to use mujoco_py as non sudoer user\n        chmod -R 777 /opt/conda/lib/python3.6/site-packages/mujoco_py/\n        pip install --upgrade minerl\n\n# Export global environment variables\n%environment\n        export SHELL=/bin/sh\n        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n        export PATH=/Gym/gym/.tox/py3/bin:$PATH\n\n%runscript\n        exec /bin/sh \"$@\"\n</code></pre> <p>Here is the same recipe but written for TensorFlow:</p> <pre><code>#This is a dockerfile that sets up a full Gym install with test dependencies\nBootstrap: docker\n\n# Here we ll build our container upon the tensorflow container\nFrom: tensorflow/tensorflow:latest-gpu-py3\n\n# Now we'll copy the mjkey file located in the current directory inside the container's root\n# directory\n%files\n        mjkey.txt\n\n# Then we put everything we need to install\n%post\n        apt -y update &amp;&amp; \\\n        apt install -y keyboard-configuration &amp;&amp; \\\n        apt install -y \\\n        python3-setuptools \\\n        python3-dev \\\n        python-pyglet \\\n        python3-opengl \\\n        libjpeg-dev \\\n        libboost-all-dev \\\n        libsdl2-dev \\\n        libosmesa6-dev \\\n        patchelf \\\n        ffmpeg \\\n        xvfb \\\n        wget \\\n        git \\\n        unzip &amp;&amp; \\\n        apt clean &amp;&amp; \\\n        rm -rf /var/lib/apt/lists/*\n\n        # Download Gym and MuJoCo\n        mkdir /Gym &amp;&amp; cd /Gym\n        git clone https://github.com/openai/gym.git || true &amp;&amp; \\\n        mkdir /Gym/.mujoco &amp;&amp; cd /Gym/.mujoco\n        wget https://www.roboti.us/download/mjpro150_linux.zip  &amp;&amp; \\\n        unzip mjpro150_linux.zip &amp;&amp; \\\n        wget https://www.roboti.us/download/mujoco200_linux.zip &amp;&amp; \\\n        unzip mujoco200_linux.zip &amp;&amp; \\\n        mv mujoco200_linux mujoco200\n\n        # Export global environment variables\n        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n        cp /mjkey.txt /Gym/.mujoco/mjkey.txt\n\n        # Install Python dependencies\n        wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt\n        pip install -r requirements.txt\n        # Install Gym and MuJoCo\n        cd /Gym/gym\n        pip install -e '.[all]'\n        # Change permission to use mujoco_py as non sudoer user\n        chmod -R 777 /usr/local/lib/python3.5/dist-packages/mujoco_py/\n\n        # Then install miniworld\n        cd /usr/local/\n        git clone https://github.com/maximecb/gym-miniworld.git\n        cd gym-miniworld\n        pip install -e .\n\n# Export global environment variables\n%environment\n        export SHELL=/bin/bash\n        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n        export PATH=/Gym/gym/.tox/py3/bin:$PATH\n\n%runscript\n        exec /bin/bash \"$@\"\n</code></pre> <p>Keep in mind that those environment variables are sourced at runtime and not at build time. This is why, you should also define them in the <code>%post</code> section since they are required to install MuJoCo.</p>"},{"location":"Userguide_singularity/#using-containers-on-clusters","title":"Using containers on clusters","text":""},{"location":"Userguide_singularity/#how-to-use-containers-on-clusters","title":"How to use containers on clusters","text":"<p>On every cluster with Slurm, datasets and intermediate results should go in <code>$SLURM_TMPDIR</code> while the final experiment results should go in <code>$SCRATCH</code>. In order to use the container you built, you need to copy it on the cluster you want to use.</p> <p>You should always store your container in $SCRATCH!</p> <p>Then reserve a node with srun/sbatch, copy the container and your dataset on the node given by SLURM (i.e in <code>$SLURM_TMPDIR</code>) and execute the code <code>&lt;YOUR_CODE&gt;</code> within the container <code>&lt;YOUR_CONTAINER&gt;</code> with:</p> <pre><code>singularity exec --nv -H $HOME:/home -B $SLURM_TMPDIR:/dataset/ \\\n  -B $SLURM_TMPDIR:/tmp_log/ -B $SCRATCH:/final_log/ \\\n  $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; python &lt;YOUR_CODE&gt;\n</code></pre> <p>Remember that <code>/dataset</code>, <code>/tmp_log</code> and <code>/final_log</code> were created in the previous section. Now each time, we'll use singularity, we are explicitly telling it to mount <code>$SLURM_TMPDIR</code> on the cluster's node in the folder <code>/dataset</code> inside the container with the option <code>-B</code> such that each dataset downloaded by PyTorch in <code>/dataset</code> will be available in <code>$SLURM_TMPDIR</code>.</p> <p>This will allow us to have code and scripts that are invariant to the cluster environment. The option <code>-H</code> specify what will be the container's home. For example, if you have your code in <code>$HOME/Project12345/Version35/</code> you can specify <code>-H $HOME/Project12345/Version35:/home</code>, thus the container will only have access to the code inside <code>Version35</code>.</p> <p>If you want to run multiple commands inside the container you can use:</p> <pre><code>singularity exec --nv -H $HOME:/home -B $SLURM_TMPDIR:/dataset/ \\\n  -B $SLURM_TMPDIR:/tmp_log/ -B $SCRATCH:/final_log/ \\\n  $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; bash -c 'pwd &amp;&amp; ls &amp;&amp; python &lt;YOUR_CODE&gt;'\n</code></pre>"},{"location":"Userguide_singularity/#example-interactive-case-srunsalloc","title":"Example: Interactive case (srun/salloc)","text":"<p>Once you get an interactive session with SLURM, copy <code>&lt;YOUR_CONTAINER&gt;</code> and <code>&lt;YOUR_DATASET&gt;</code> to <code>$SLURM_TMPDIR</code></p> <pre><code># 0. Get an interactive session\n$ srun --gres=gpu:1\n# 1. Copy your container on the compute node\n$ rsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n# 2. Copy your dataset on the compute node\n$ rsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n</code></pre> <p>Then use <code>singularity shell</code> to get a shell inside the container</p> <pre><code># 3. Get a shell in your environment\n$ singularity shell --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;\n\n# 4. Execute your code\n&lt;Singularity_container&gt;$ python &lt;YOUR_CODE&gt;\n</code></pre> <p>or use <code>singularity exec</code> to execute <code>&lt;YOUR_CODE&gt;</code>.</p> <pre><code># 3. Execute your code\n$ singularity exec --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n          python &lt;YOUR_CODE&gt;\n</code></pre> <p>You can create also the following alias to make your life easier. <pre><code>alias my_env='singularity exec --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;'\n</code></pre></p> <p>This will allow you to run any code with: <pre><code>my_env python &lt;YOUR_CODE&gt;\n</code></pre></p>"},{"location":"Userguide_singularity/#example-sbatch-case","title":"Example: sbatch case","text":"<p>You can also create a <code>sbatch</code> script:</p> <pre><code>#!/bin/bash\n#SBATCH --cpus-per-task=6         # Ask for 6 CPUs\n#SBATCH --gres=gpu:1              # Ask for 1 GPU\n#SBATCH --mem=10G                 # Ask for 10 GB of RAM\n#SBATCH --time=0:10:00            # The job will run for 10 minutes\n\n# 1. Copy your container on the compute node\nrsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n# 2. Copy your dataset on the compute node\nrsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n# 3. Executing your code with singularity\nsingularity exec --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n          python \"&lt;YOUR_CODE&gt;\"\n# 4. Copy whatever you want to save on $SCRATCH\nrsync -avz $SLURM_TMPDIR/&lt;to_save&gt; $SCRATCH\n</code></pre>"},{"location":"Userguide_singularity/#issue-with-pybullet-and-opengl-libraries","title":"Issue with PyBullet and OpenGL libraries","text":"<p>If you are running certain gym environments that require <code>pyglet</code>, you may encounter a problem when running your singularity instance with the Nvidia drivers using the <code>--nv</code> flag. This happens because the <code>--nv</code> flag also provides the OpenGL libraries:</p> <pre><code>libGL.so.1 =&gt; /.singularity.d/libs/libGL.so.1\nlibGLX.so.0 =&gt; /.singularity.d/libs/libGLX.so.0\n</code></pre> <p>If you don't experience those problems with <code>pyglet</code>, you probably don't need to address this. Otherwise, you can resolve those problems by <code>apt-get install -y libosmesa6-dev mesa-utils mesa-utils-extra libgl1-mesa-glx</code>, and then making sure that your <code>LD_LIBRARY_PATH</code> points to those libraries before the ones in <code>/.singularity.d/libs</code>.</p> <pre><code>%environment\n     # ...\n     export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/mesa:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"Userguide_singularity/#mila-cluster","title":"Mila cluster","text":"<p>On the Mila cluster <code>$SCRATCH</code> is not yet defined, you should add the experiment results you want to keep in <code>/network/scratch/&lt;u&gt;/&lt;username&gt;/</code>. In order to use the sbatch script above and to match other cluster environment's names, you can define <code>$SCRATCH</code> as an alias for <code>/network/scratch/&lt;u&gt;/&lt;username&gt;</code> with:</p> <pre><code>echo \"export SCRATCH=/network/scratch/${USER:0:1}/$USER\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Then, you can follow the general procedure explained above.</p>"},{"location":"Userguide_singularity/#digital-research-alliance-of-canada","title":"Digital Research Alliance of Canada","text":"<p>Using singularity on Digital Research Alliance of Canada is similar except that you need to add Yoshua's account name and load singularity. Here is an example of a <code>sbatch</code> script using singularity on compute Canada cluster:</p> <p>Warning</p> <p>You should use singularity/2.6 or singularity/3.4. There is a bug in singularity/3.2 which makes gpu unusable.</p> <pre><code>#!/bin/bash\n#SBATCH --account=rpp-bengioy     # Yoshua pays for your job\n#SBATCH --cpus-per-task=6         # Ask for 6 CPUs\n#SBATCH --gres=gpu:1              # Ask for 1 GPU\n#SBATCH --mem=32G                 # Ask for 32 GB of RAM\n#SBATCH --time=0:10:00            # The job will run for 10 minutes\n#SBATCH --output=\"/scratch/&lt;user&gt;/slurm-%j.out\" # Modify the output of sbatch\n\n# 1. You have to load singularity\nmodule load singularity\n# 2. Then you copy the container to the local disk\nrsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n# 3. Copy your dataset on the compute node\nrsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n# 4. Executing your code with singularity\nsingularity exec --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n          python \"&lt;YOUR_CODE&gt;\"\n# 5. Copy whatever you want to save on $SCRATCH\nrsync -avz $SLURM_TMPDIR/&lt;to_save&gt; $SCRATCH\n</code></pre>"},{"location":"Userguide_singularity_building/","title":"Userguide singularity building","text":""},{"location":"Userguide_singularity_building/#building-the-containers","title":"Building the containers","text":"<p>Building a container is like creating a new environment except that containers are much more powerful since they are self-contained systems.  With singularity, there are two ways to build containers.</p> <p>The first one is by yourself, it's like when you got a new Linux laptop and you don't really know what you need, if you see that something is missing, you install it. Here you can get a vanilla container with Ubuntu called a sandbox, you log in and you install each packages by yourself.  This procedure can take time but will allow you to understand how things work and what you need. This is recommended if you need to figure out how things will be compiled or if you want to install packages on the fly. We'll refer to this procedure as singularity sandboxes.</p> <p>The second way is more like you know what you want, so you write a list of everything you need, you send it to singularity and it will install everything for you. Those lists are called singularity recipes.</p>"},{"location":"Userguide_singularity_building/#first-way-build-and-use-a-sandbox","title":"First way: Build and use a sandbox","text":"<p>You might ask yourself: On which machine should I build a container?</p> <p>First of all, you need to choose where you'll build your container. This operation requires memory and high cpu usage.</p> <p>Do NOT build containers on any login nodes!</p> <ul> <li> <p>(Recommended for beginner) If you need to use apt-get, you should build   the container on your laptop with sudo privileges. You'll only need to   install singularity on your laptop. Windows/Mac users can look <code>there</code>_ and   Ubuntu/Debian users can use directly:   <pre><code>sudo apt-get install singularity-container\n</code></pre></p> </li> <li> <p>If you can't install singularity on your laptop and you don't need     apt-get, you can reserve a cpu node on the Mila cluster to build your     container.</p> <p>In this case, in order to avoid too much I/O over the network, you should define the singularity cache locally:</p> <pre><code>export SINGULARITY_CACHEDIR=$SLURM_TMPDIR\n</code></pre> </li> <li> <p>If you can't install singularity on your laptop and you want to use   apt-get, you can use <code>singularity-hub</code> to build your containers and read   Recipe_section.</p> </li> </ul>"},{"location":"Userguide_singularity_building/#download-containers-from-the-web","title":"Download containers from the web","text":"<p>Hopefully, you may not need to create containers from scratch as many have been already built for the most common deep learning software. You can find most of them on dockerhub.</p> <p>Go on dockerhub and select the container you want to pull.</p> <p>For example, if you want to get the latest PyTorch version with GPU support (Replace runtime by devel if you need the full Cuda toolkit):</p> <pre><code>singularity pull docker://pytorch/pytorch:1.0.1-cuda10.0-cudnn7-runtime\n</code></pre> <p>Or the latest TensorFlow:</p> <pre><code>singularity pull docker://tensorflow/tensorflow:latest-gpu-py3\n</code></pre> <p>Currently the pulled image <code>pytorch.simg</code> or <code>tensorflow.simg</code> is read-only meaning that you won't be able to install anything on it.  Starting now, PyTorch will be taken as example. If you use TensorFlow, simply replace every pytorch occurrences by tensorflow.</p>"},{"location":"Userguide_singularity_building/#how-to-add-or-install-stuff-in-a-container","title":"How to add or install stuff in a container","text":"<p>The first step is to transform your read only container <code>pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</code> in a writable version that will allow you to add packages.</p> <p>Warning</p> <p>Depending on the version of singularity you are using, singularity will build a container with the extension .simg or .sif. If you're using .sif files, replace every occurences of .simg by .sif.</p> <p>Tip</p> <p>If you want to use apt-get you have to put sudo ahead of the  following commands</p> <p>This command will create a writable image in the folder <code>pytorch</code>. <pre><code>singularity build --sandbox pytorch pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg\n</code></pre> Then you'll need the following command to log inside the container. <pre><code>singularity shell --writable -H $HOME:/home pytorch\n</code></pre> Once you get into the container, you can use pip and install anything you need (Or with <code>apt-get</code> if you built the container with sudo).</p> <p>Warning</p> <p>Singularity mounts your home folder, so if you install things into the <code>$HOME</code> of your container, they will be installed in your real <code>$HOME</code>!</p> <p>You should install your stuff in /usr/local instead.</p>"},{"location":"Userguide_singularity_building/#creating-useful-directories","title":"Creating useful directories","text":"<p>One of the benefits of containers is that you'll be able to use them across different clusters. However for each cluster the datasets and experiments folder location can be different. In order to be invariant to those locations, we will create some useful mount points inside the container:</p> <pre><code>&lt;Singularity_container&gt;$ mkdir /dataset\n&lt;Singularity_container&gt;$ mkdir /tmp_log\n&lt;Singularity_container&gt;$ mkdir /final_log\n</code></pre> <p>From now, you won't need to worry anymore when you write your code to specify where to pick up your dataset. Your dataset will always be in <code>/dataset</code> independently of the cluster you are using.</p>"},{"location":"Userguide_singularity_building/#testing","title":"Testing","text":"<p>If you have some code that you want to test before finalizing your container, you have two choices.  You can either log into your container and run Python code inside it with: <pre><code>singularity shell --nv pytorch\n</code></pre> Or you can execute your command directly with <pre><code>singularity exec --nv pytorch Python YOUR_CODE.py\n</code></pre></p> <p>Tip</p> <p>---nv allows the container to use gpus. You don't need this if you don't plan to use a gpu.</p> <p>Warning</p> <p>Don't forget to clear the cache of the packages you installed in the containers.</p>"},{"location":"Userguide_singularity_building/#creating-a-new-image-from-the-sandbox","title":"Creating a new image from the sandbox","text":"<p>Once everything you need is installed inside the container, you need to convert it back to a read-only singularity image with:</p> <pre><code>singularity build pytorch_final.simg pytorch\n</code></pre>"},{"location":"Userguide_singularity_building/#second-way-use-recipes","title":"Second way: Use recipes","text":"<p>A singularity recipe is a file including specifics about installation software, environment variables, files to add, and container metadata.  It is a starting point for designing any custom container. Instead of pulling a container and installing your packages manually, you can specify in this file the packages you want and then build your container from this file.</p> <p>Here is a toy example of a singularity recipe installing some stuff:</p> <pre><code>################# Header: Define the base system you want to use ################\n# Reference of the kind of base you want to use (e.g., docker, debootstrap, shub).\nBootstrap: docker\n# Select the docker image you want to use (Here we choose tensorflow)\nFrom: tensorflow/tensorflow:latest-gpu-py3\n\n################# Section: Defining the system #################################\n# Commands in the %post section are executed within the container.\n%post\n        echo \"Installing Tools with apt-get\"\n        apt-get update\n        apt-get install -y cmake libcupti-dev libyaml-dev wget unzip\n        apt-get clean\n        echo \"Installing things with pip\"\n        pip install tqdm\n        echo \"Creating mount points\"\n        mkdir /dataset\n        mkdir /tmp_log\n        mkdir /final_log\n\n# Environment variables that should be sourced at runtime.\n%environment\n        # use bash as default shell\n        SHELL=/bin/bash\n        export SHELL\n</code></pre> <p>A recipe file contains two parts: the <code>header</code> and <code>sections</code>. In the <code>header</code> you specify which base system you want to use, it can be any docker or singularity container. In <code>sections</code>, you can list the things you want to install in the subsection <code>post</code> or list the environment's variable you need to source at each runtime in the subsection <code>environment</code>. For a more detailed description, please look at the singularity documentation.</p> <p>In order to build a singularity container from a singularity recipe file, you should use: <pre><code>sudo singularity build &lt;NAME_CONTAINER&gt; &lt;YOUR_RECIPE_FILES&gt;\n</code></pre></p> <p>Warning</p> <p>You always need to use sudo when you build a container from a    recipe. As there is no access to sudo on the cluster, a personal computer or    the use singularity hub is needed to build a container</p>"},{"location":"Userguide_singularity_building/#build-recipe-on-singularity-hub","title":"Build recipe on singularity hub","text":"<p>Singularity hub allows users to build containers from recipes directly on singularity-hub's cloud meaning that you don't need to build containers by yourself.  You need to register on singularity-hub and link your singularity-hub account to your GitHub account, then:</p> <ol> <li>Create a new github repository.</li> <li>Add a collection on singularity-hub and select the github repository your created.</li> <li> <p>Clone the github repository on your computer.     <pre><code>$ git clone &lt;url&gt;\n</code></pre></p> </li> <li> <p>Write the singularity recipe and save it as a file named Singularity.</p> </li> <li> <p>Git add Singularity, commit and push on the master branch</p> <pre><code>$ git add Singularity\n$ git commit\n$ git push origin master\n</code></pre> </li> </ol> <p>At this point, robots from singularity-hub will build the container for you, you will be able to download your container from the website or directly with: <pre><code>singularity pull shub://&lt;github_username&gt;/&lt;repository_name&gt;\n</code></pre></p>"},{"location":"Userguide_singularity_building/#example-recipe-with-openai-gym-mujoco-and-miniworld","title":"Example: Recipe with OpenAI gym, MuJoCo and Miniworld","text":"<p>Here is an example on how you can use a singularity recipe to install complex environment such as OpenAI gym, MuJoCo and Miniworld on a PyTorch based container. In order to use MuJoCo, you'll need to copy the key stored on the Mila cluster in <code>/ai/apps/mujoco/license/mjkey.txt</code> to your current directory.</p> <pre><code>#This is a dockerfile that sets up a full Gym install with test dependencies\nBootstrap: docker\n\n# Here we ll build our container upon the pytorch container\nFrom: pytorch/pytorch:1.0-cuda10.0-cudnn7-runtime\n\n# Now we'll copy the mjkey file located in the current directory inside the container's root\n# directory\n%files\n        mjkey.txt\n\n# Then we put everything we need to install\n%post\n        export PATH=$PATH:/opt/conda/bin\n        apt -y update &amp;&amp; \\\n        apt install -y keyboard-configuration &amp;&amp; \\\n        apt install -y \\\n        python3-dev \\\n        python-pyglet \\\n        python3-opengl \\\n        libhdf5-dev \\\n        libjpeg-dev \\\n        libboost-all-dev \\\n        libsdl2-dev \\\n        libosmesa6-dev \\\n        patchelf \\\n        ffmpeg \\\n        xvfb \\\n        libhdf5-dev \\\n        openjdk-8-jdk \\\n        wget \\\n        git \\\n        unzip &amp;&amp; \\\n        apt clean &amp;&amp; \\\n        rm -rf /var/lib/apt/lists/*\n        pip install h5py\n\n        # Download Gym and MuJoCo\n        mkdir /Gym &amp;&amp; cd /Gym\n        git clone https://github.com/openai/gym.git || true &amp;&amp; \\\n        mkdir /Gym/.mujoco &amp;&amp; cd /Gym/.mujoco\n        wget https://www.roboti.us/download/mjpro150_linux.zip  &amp;&amp; \\\n        unzip mjpro150_linux.zip &amp;&amp; \\\n        wget https://www.roboti.us/download/mujoco200_linux.zip &amp;&amp; \\\n        unzip mujoco200_linux.zip &amp;&amp; \\\n        mv mujoco200_linux mujoco200\n\n        # Export global environment variables\n        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n        cp /mjkey.txt /Gym/.mujoco/mjkey.txt\n        # Install Python dependencies\n        wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt\n        pip install -r requirements.txt\n        # Install Gym and MuJoCo\n        cd /Gym/gym\n        pip install -e '.[all]'\n        # Change permission to use mujoco_py as non sudoer user\n        chmod -R 777 /opt/conda/lib/python3.6/site-packages/mujoco_py/\n        pip install --upgrade minerl\n\n# Export global environment variables\n%environment\n        export SHELL=/bin/sh\n        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n        export PATH=/Gym/gym/.tox/py3/bin:$PATH\n\n%runscript\n        exec /bin/sh \"$@\"\n</code></pre> <p>Here is the same recipe but written for TensorFlow:</p> <pre><code>#This is a dockerfile that sets up a full Gym install with test dependencies\nBootstrap: docker\n\n# Here we ll build our container upon the tensorflow container\nFrom: tensorflow/tensorflow:latest-gpu-py3\n\n# Now we'll copy the mjkey file located in the current directory inside the container's root\n# directory\n%files\n        mjkey.txt\n\n# Then we put everything we need to install\n%post\n        apt -y update &amp;&amp; \\\n        apt install -y keyboard-configuration &amp;&amp; \\\n        apt install -y \\\n        python3-setuptools \\\n        python3-dev \\\n        python-pyglet \\\n        python3-opengl \\\n        libjpeg-dev \\\n        libboost-all-dev \\\n        libsdl2-dev \\\n        libosmesa6-dev \\\n        patchelf \\\n        ffmpeg \\\n        xvfb \\\n        wget \\\n        git \\\n        unzip &amp;&amp; \\\n        apt clean &amp;&amp; \\\n        rm -rf /var/lib/apt/lists/*\n\n        # Download Gym and MuJoCo\n        mkdir /Gym &amp;&amp; cd /Gym\n        git clone https://github.com/openai/gym.git || true &amp;&amp; \\\n        mkdir /Gym/.mujoco &amp;&amp; cd /Gym/.mujoco\n        wget https://www.roboti.us/download/mjpro150_linux.zip  &amp;&amp; \\\n        unzip mjpro150_linux.zip &amp;&amp; \\\n        wget https://www.roboti.us/download/mujoco200_linux.zip &amp;&amp; \\\n        unzip mujoco200_linux.zip &amp;&amp; \\\n        mv mujoco200_linux mujoco200\n\n        # Export global environment variables\n        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n        cp /mjkey.txt /Gym/.mujoco/mjkey.txt\n\n        # Install Python dependencies\n        wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt\n        pip install -r requirements.txt\n        # Install Gym and MuJoCo\n        cd /Gym/gym\n        pip install -e '.[all]'\n        # Change permission to use mujoco_py as non sudoer user\n        chmod -R 777 /usr/local/lib/python3.5/dist-packages/mujoco_py/\n\n        # Then install miniworld\n        cd /usr/local/\n        git clone https://github.com/maximecb/gym-miniworld.git\n        cd gym-miniworld\n        pip install -e .\n\n# Export global environment variables\n%environment\n        export SHELL=/bin/bash\n        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n        export PATH=/Gym/gym/.tox/py3/bin:$PATH\n\n%runscript\n        exec /bin/bash \"$@\"\n</code></pre> <p>Keep in mind that those environment variables are sourced at runtime and not at build time. This is why, you should also define them in the <code>%post</code> section since they are required to install MuJoCo.</p>"},{"location":"Userguide_singularity_on_clusters/","title":"Userguide singularity on clusters","text":""},{"location":"Userguide_singularity_on_clusters/#using-containers-on-clusters","title":"Using containers on clusters","text":""},{"location":"Userguide_singularity_on_clusters/#how-to-use-containers-on-clusters","title":"How to use containers on clusters","text":"<p>On every cluster with Slurm, datasets and intermediate results should go in <code>$SLURM_TMPDIR</code> while the final experiment results should go in <code>$SCRATCH</code>. In order to use the container you built, you need to copy it on the cluster you want to use.</p> <p>You should always store your container in $SCRATCH!</p> <p>Then reserve a node with srun/sbatch, copy the container and your dataset on the node given by SLURM (i.e in <code>$SLURM_TMPDIR</code>) and execute the code <code>&lt;YOUR_CODE&gt;</code> within the container <code>&lt;YOUR_CONTAINER&gt;</code> with:</p> <pre><code>singularity exec --nv -H $HOME:/home -B $SLURM_TMPDIR:/dataset/ \\\n  -B $SLURM_TMPDIR:/tmp_log/ -B $SCRATCH:/final_log/ \\\n  $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; python &lt;YOUR_CODE&gt;\n</code></pre> <p>Remember that <code>/dataset</code>, <code>/tmp_log</code> and <code>/final_log</code> were created in the previous section. Now each time, we'll use singularity, we are explicitly telling it to mount <code>$SLURM_TMPDIR</code> on the cluster's node in the folder <code>/dataset</code> inside the container with the option <code>-B</code> such that each dataset downloaded by PyTorch in <code>/dataset</code> will be available in <code>$SLURM_TMPDIR</code>.</p> <p>This will allow us to have code and scripts that are invariant to the cluster environment. The option <code>-H</code> specify what will be the container's home. For example, if you have your code in <code>$HOME/Project12345/Version35/</code> you can specify <code>-H $HOME/Project12345/Version35:/home</code>, thus the container will only have access to the code inside <code>Version35</code>.</p> <p>If you want to run multiple commands inside the container you can use:</p> <pre><code>singularity exec --nv -H $HOME:/home -B $SLURM_TMPDIR:/dataset/ \\\n  -B $SLURM_TMPDIR:/tmp_log/ -B $SCRATCH:/final_log/ \\\n  $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; bash -c 'pwd &amp;&amp; ls &amp;&amp; python &lt;YOUR_CODE&gt;'\n</code></pre>"},{"location":"Userguide_singularity_on_clusters/#example-interactive-case-srunsalloc","title":"Example: Interactive case (srun/salloc)","text":"<p>Once you get an interactive session with SLURM, copy <code>&lt;YOUR_CONTAINER&gt;</code> and <code>&lt;YOUR_DATASET&gt;</code> to <code>$SLURM_TMPDIR</code></p> <pre><code># 0. Get an interactive session\n$ srun --gres=gpu:1\n# 1. Copy your container on the compute node\n$ rsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n# 2. Copy your dataset on the compute node\n$ rsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n</code></pre> <p>Then use <code>singularity shell</code> to get a shell inside the container</p> <pre><code># 3. Get a shell in your environment\n$ singularity shell --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;\n\n# 4. Execute your code\n&lt;Singularity_container&gt;$ python &lt;YOUR_CODE&gt;\n</code></pre> <p>or use <code>singularity exec</code> to execute <code>&lt;YOUR_CODE&gt;</code>.</p> <pre><code># 3. Execute your code\n$ singularity exec --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n          python &lt;YOUR_CODE&gt;\n</code></pre> <p>You can create also the following alias to make your life easier. <pre><code>alias my_env='singularity exec --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;'\n</code></pre></p> <p>This will allow you to run any code with: <pre><code>my_env python &lt;YOUR_CODE&gt;\n</code></pre></p>"},{"location":"Userguide_singularity_on_clusters/#example-sbatch-case","title":"Example: sbatch case","text":"<p>You can also create a <code>sbatch</code> script:</p> <pre><code>#!/bin/bash\n#SBATCH --cpus-per-task=6         # Ask for 6 CPUs\n#SBATCH --gres=gpu:1              # Ask for 1 GPU\n#SBATCH --mem=10G                 # Ask for 10 GB of RAM\n#SBATCH --time=0:10:00            # The job will run for 10 minutes\n\n# 1. Copy your container on the compute node\nrsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n# 2. Copy your dataset on the compute node\nrsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n# 3. Executing your code with singularity\nsingularity exec --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n          python \"&lt;YOUR_CODE&gt;\"\n# 4. Copy whatever you want to save on $SCRATCH\nrsync -avz $SLURM_TMPDIR/&lt;to_save&gt; $SCRATCH\n</code></pre>"},{"location":"Userguide_singularity_on_clusters/#issue-with-pybullet-and-opengl-libraries","title":"Issue with PyBullet and OpenGL libraries","text":"<p>If you are running certain gym environments that require <code>pyglet</code>, you may encounter a problem when running your singularity instance with the Nvidia drivers using the <code>--nv</code> flag. This happens because the <code>--nv</code> flag also provides the OpenGL libraries:</p> <pre><code>libGL.so.1 =&gt; /.singularity.d/libs/libGL.so.1\nlibGLX.so.0 =&gt; /.singularity.d/libs/libGLX.so.0\n</code></pre> <p>If you don't experience those problems with <code>pyglet</code>, you probably don't need to address this. Otherwise, you can resolve those problems by <code>apt-get install -y libosmesa6-dev mesa-utils mesa-utils-extra libgl1-mesa-glx</code>, and then making sure that your <code>LD_LIBRARY_PATH</code> points to those libraries before the ones in <code>/.singularity.d/libs</code>.</p> <pre><code>%environment\n     # ...\n     export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/mesa:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"Userguide_singularity_on_clusters/#mila-cluster","title":"Mila cluster","text":"<p>On the Mila cluster <code>$SCRATCH</code> is not yet defined, you should add the experiment results you want to keep in <code>/network/scratch/&lt;u&gt;/&lt;username&gt;/</code>. In order to use the sbatch script above and to match other cluster environment's names, you can define <code>$SCRATCH</code> as an alias for <code>/network/scratch/&lt;u&gt;/&lt;username&gt;</code> with:</p> <pre><code>echo \"export SCRATCH=/network/scratch/${USER:0:1}/$USER\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Then, you can follow the general procedure explained above.</p>"},{"location":"Userguide_singularity_on_clusters/#digital-research-alliance-of-canada","title":"Digital Research Alliance of Canada","text":"<p>Using singularity on Digital Research Alliance of Canada is similar except that you need to add Yoshua's account name and load singularity. Here is an example of a <code>sbatch</code> script using singularity on compute Canada cluster:</p> <p>Warning</p> <p>You should use singularity/2.6 or singularity/3.4. There is a bug in singularity/3.2 which makes gpu unusable.</p> <pre><code>#!/bin/bash\n#SBATCH --account=rpp-bengioy     # Yoshua pays for your job\n#SBATCH --cpus-per-task=6         # Ask for 6 CPUs\n#SBATCH --gres=gpu:1              # Ask for 1 GPU\n#SBATCH --mem=32G                 # Ask for 32 GB of RAM\n#SBATCH --time=0:10:00            # The job will run for 10 minutes\n#SBATCH --output=\"/scratch/&lt;user&gt;/slurm-%j.out\" # Modify the output of sbatch\n\n# 1. You have to load singularity\nmodule load singularity\n# 2. Then you copy the container to the local disk\nrsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n# 3. Copy your dataset on the compute node\nrsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n# 4. Executing your code with singularity\nsingularity exec --nv \\\n          -H $HOME:/home \\\n          -B $SLURM_TMPDIR:/dataset/ \\\n          -B $SLURM_TMPDIR:/tmp_log/ \\\n          -B $SCRATCH:/final_log/ \\\n          $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n          python \"&lt;YOUR_CODE&gt;\"\n# 5. Copy whatever you want to save on $SCRATCH\nrsync -avz $SLURM_TMPDIR/&lt;to_save&gt; $SCRATCH\n</code></pre>"},{"location":"Userguide_singularity_overview/","title":"Userguide singularity overview","text":""},{"location":"Userguide_singularity_overview/#overview","title":"Overview","text":""},{"location":"Userguide_singularity_overview/#what-is-singularity","title":"What is Singularity?","text":"<p>Running Docker on SLURM is a security problem (e.g. running as root, being able to mount any directory).  The alternative is to use Singularity, which is a popular solution in the world of HPC.</p> <p>There is a good level of compatibility between Docker and Singularity, and we can find many exaggerated claims about able to convert containers from Docker to Singularity without any friction. Oftentimes, Docker images from DockerHub are 100% compatible with Singularity, and they can indeed be used without friction, but things get messy when we try to convert our own Docker build files to Singularity recipes.</p>"},{"location":"Userguide_singularity_overview/#links-to-official-documentation","title":"Links to official documentation","text":"<ul> <li>official Singularity user guide   (this is the one you will use most often)</li> <li>official Singularity admin guide</li> </ul>"},{"location":"Userguide_singularity_overview/#overview-of-the-steps-used-in-practice","title":"Overview of the steps used in practice","text":"<p>Most often, the process to create and use a Singularity container is:</p> <ul> <li> <p>on your Linux computer (at home or work)</p> </li> <li> <p>select a Docker image from DockerHub (e.g. pytorch/pytorch)</p> </li> <li>make a recipe file for Singularity that starts with that DockerHub image</li> <li>build the recipe file, thus creating the image file (e.g. <code>my-pytorch-image.sif</code>)</li> <li>test your singularity container before send it over to the cluster</li> <li> <p><code>rsync -av my-pytorch-image.sif &lt;login-node&gt;:Documents/my-singularity-images</code></p> </li> <li> <p>on the login node for that cluster</p> </li> <li> <p>queue your jobs with <code>sbatch ...</code></p> </li> <li>(note that your jobs will copy over the <code>my-pytorch-image.sif</code> to $SLURM_TMPDIR     and will then launch Singularity with that image)</li> <li>do something else while you wait for them to finish</li> <li>queue more jobs with the same <code>my-pytorch-image.sif</code>,     reusing it many times over</li> </ul> <p>In the following sections you will find specific examples or tips to accomplish in practice the steps highlighted above.</p>"},{"location":"Userguide_singularity_overview/#nope-not-on-macos","title":"Nope, not on MacOS","text":"<p>Singularity does not work on MacOS, as of the time of this writing in 2021. Docker does not actually run on MacOS, but there Docker silently installs a virtual machine running Linux, which makes it a pleasant experience, and the user does not need to care about the details of how Docker does it.</p> <p>Given its origins in HPC, Singularity does not provide that kind of seamless experience on MacOS, even though it's technically possible to run it inside a Linux virtual machine on MacOS.</p>"},{"location":"Userguide_singularity_overview/#where-to-build-images","title":"Where to build images","text":"<p>Building Singularity images is a rather heavy task, which can take 20 minutes if you have a lot of steps in your recipe. This makes it a bad task to run on the login nodes of our clusters, especially if it needs to be run regularly.</p> <p>On the Mila cluster, we are lucky to have unrestricted internet access on the compute nodes, which means that anyone can request an interactive CPU node (no need for GPU) and build their images there without problem.</p> <p>Warning</p> <p>Do not build Singularity images from scratch every time your run a job in a large batch.  This will be a colossal waste of GPU time as well as internet bandwidth. If you setup your workflow properly (e.g. using bind paths for your code and data), you can spend months reusing the same Singularity image <code>my-pytorch-image.sif</code>.</p>"},{"location":"Userguide_wandb/","title":"Weight and Biases (WandB)","text":"<p>Students supervised by core professors are elligible to the Mila organization on wandb. To request access, write to it-support@mila.quebec. Then please follow the guidelines below to get your account created or linked to Mila's organization.</p>"},{"location":"Userguide_wandb/#logging-in-for-the-first-time","title":"Logging in for the first time","text":""},{"location":"Userguide_wandb/#for-those-who-already-have-a-wandb-account","title":"For those who already have a WandB account","text":"<p>Note</p> <p>If you already have an account and want to have access to mila-org with it, first add your email @mila.quebec to your account and make it your primary email. See documentation here on how to do so. Then log out so that you can make your first connection with single sign-on. Make sure to do this before following the next steps otherwise you will end up with 2 separate accounts</p> <ul> <li> <p>Go to https://wandb.ai and click sign in.</p> </li> <li> <p>Enter your email @mila.quebec at the bottom.</p> </li> <li> <p>The password box should disappear when you are done typing your email. Then click log in,   and you will be redirected to a single sign-on page.</p> </li> <li> <p>Select your account mila.quebec. If you already have an account, wandb will offer to link   it, otherwise, you will be invited to create a new account.</p> <ul> <li>For new accounts, select Professional.</li> </ul> </li> </ul>"},{"location":"VSCode/","title":"Visual Studio Code","text":"<p>One editor of choice for many researchers is VSCode. One feature of VSCode is remote editing through SSH. This allows you to edit files on the cluster as if they were local. You can also debug your programs using VSCode's debugger, open terminal sessions, etc.</p>"},{"location":"VSCode/#connecting-to-the-cluster","title":"Connecting to the cluster","text":"<p>VSCode cannot be used to edit code on the login nodes, because it is a heavy enough process (a <code>node</code> process, plus the language server, linter, and possibly other plugins depending on your configured environment) that there is a risk of overloading the login nodes if too many researchers did it at the same time.</p> <p>Therefore, to use VSCode on the cluster, you first need to allocate a compute node, then connect to that node.</p> <p>The simplest way to do so is to install milatools on your local machine and run the <code>mila init</code> command. This will add a <code>mila-cpu</code> entry to your SSH configuration that automatically allocates a CPU on a compute node for you. Then you can simply choose <code>mila-cpu</code> in the dropdown menu for remote SSH connection.</p> <p>The milatools package also provides the mila code command which gives you greater flexibility by letting you allocate more cores, RAM and/or GPUs.</p>"},{"location":"VSCode/#activating-an-environment","title":"Activating an environment","text":"<p>Reference</p> <p>To activate a conda or pip environment, you can open the command palette with Ctrl+Shift+P and type \"Python: Select interpreter\". This will prompt you for the path to the Python executable for your environment.</p> <p>Tip</p> <p>If you already have the environment activated in a terminal session, you can  run the command <code>which python</code> to get the path for this environment. This  path can be pasted into the interpreter selection prompt in VSCode to use  that same environment.</p>"},{"location":"VSCode/#troubleshooting","title":"Troubleshooting","text":""},{"location":"VSCode/#cannot-reconnect","title":"\"Cannot reconnect\"","text":"<p>When connecting to multiple compute nodes (and/or from multiple computers), some instances may crash with that message because of conflicts in the lock files VSCode installs in <code>~/.vscode-server</code> (which is shared on all compute nodes). To fix this issue, you can change this setting in your <code>settings.json</code> file:</p> <pre><code>{ \"remote.SSH.lockfilesInTmp\": true }\n</code></pre> <p>This will store the necessary lockfiles in <code>/tmp</code> on the compute nodes (which are local to the node).</p>"},{"location":"VSCode/#debugger-timeouts","title":"Debugger timeouts","text":"<p>Sometimes, slowness on the compute node or the networked filesystem might cause the VSCode debugger to timeout when starting a remote debug process. As a quick fix, you can add this to your <code>~/.bashrc</code> or <code>~/.profile</code> or equivalent resource file for your preferred shell, to increase the timeout delay to 500 seconds:</p> <pre><code>export DEBUGPY_PROCESS_SPAWN_TIMEOUT=500\n</code></pre>"},{"location":"examples/header/","title":"Header","text":"About these examples <p>This section contains some minimal examples of how to run jobs on the Mila cluster. Each example is self-contained and can be run as-is directly on the cluster without error.  Each example has the following structure:</p> <ul> <li><code>job.sh</code>: SLURM <code>sbatch</code> script. Can be launched with <code>sbatch job.sh</code>.</li> <li><code>main.py</code>: Example python script.</li> </ul> <p>Some examples are displayed as a difference with respect to a \"base\" example. For instance, the multi-gpu example is shown as a difference with respect to the single-gpu example.</p>"},{"location":"examples/advanced/","title":"Advanced Examples","text":"About these examples <p>This section contains some minimal examples of how to run jobs on the Mila cluster. Each example is self-contained and can be run as-is directly on the cluster without error.  Each example has the following structure:</p> <ul> <li><code>job.sh</code>: SLURM <code>sbatch</code> script. Can be launched with <code>sbatch job.sh</code>.</li> <li><code>main.py</code>: Example python script.</li> </ul> <p>Some examples are displayed as a difference with respect to a \"base\" example. For instance, the multi-gpu example is shown as a difference with respect to the single-gpu example.</p> <ul> <li>Multi-Node / Multi-GPU ImageNet Training</li> </ul>"},{"location":"examples/advanced/imagenet/","title":"Multi-Node / Multi-GPU ImageNet Training","text":"<p>Prerequisites:</p> <ul> <li>PyTorch Setup</li> <li>Single GPU</li> <li>Multi-GPU</li> <li>Multi-node</li> <li>Checkpointing</li> <li>Launch many jobs</li> </ul> <p>Other interesting resources:</p> <ul> <li>https://sebarnold.net/dist_blog/</li> <li>Multi-node PyTorch distributed training guide</li> </ul> <p>Click here to see the source code for this example</p> <p>This is an advanced and quite lengthy example. We recommend viewing the files directly on GitHub to get the best experience.</p> <p>pyproject.toml</p> <p>This is the configuration file for UV, which manages the dependencies for this project.</p> <pre><code>[project]\nname = \"distributed-imagenet-example\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11,&lt;3.14\"\ndependencies = [\n    \"debugpy&gt;=1.8.16\",\n    \"scipy&gt;=1.16.2\",\n    \"torch&gt;=2.7.1\",\n    \"torch-tb-profiler&gt;=0.4.3\",\n    \"torchvision&gt;=0.22.1\",\n    \"tqdm&gt;=4.67.1\",\n    \"rich&gt;=14.1.0\",\n    \"simple-parsing&gt;=0.1.7\",\n    \"scikit-learn&gt;=1.7.2\",\n    \"wandb&gt;=0.21.4\",\n]\n\n#ruff: increase max line length\n[tool.ruff]\nline-length = 100\n</code></pre> <p>safe_sbatch</p> <p>This job script uses the <code>safe_sbatch</code> submission script to submit a job at the current git state. This practice is recommended to ensure reproducibility, and to prevent changes in the python files between when the job is submitted and when it starts to affect the results.</p> <p>Unlike the script passed to sbatch, which is copied and saved with the job in SLURM (and reused when resuming a job), the python files are not saved.</p> <pre><code>#!/bin/bash\nset -eof pipefail\ngit_status=`git status --porcelain`\n# idea: Could add command-line arguments to control whether to add all changes and commit before sbatch.\nif [[ ! -z $git_status ]]; then\n    echo \"Your working directory is dirty! Please add and commit changes before continuing.\"\n    exit 1\nfi;\n# This environment variable will be available in the job script.\n# It should be used to checkout the repo at this commit (in a different directory than the original).\n# For example:\n# ```\n# git clone \"$repo\" \"$dest\"\n# echo \"Checking out commit $GIT_COMMIT\"\n# cd \"$dest\"\n# git checkout $GIT_COMMIT\n# ```\nexport GIT_COMMIT=`git rev-parse HEAD`\nexec sbatch \"$@\"\n</code></pre> <p>job.sh</p> <p>This file uses a <code>code_checkpointing.sh</code> utility script. For now, to keep this already very heavy example a bit lighter, we do not include it here, but you can find it in the GitHub repository here</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --gpus-per-task=l40s:1\n#SBATCH --mem-per-gpu=16G\n#SBATCH --tmp=200G  # We need 200GB of storage on the local disk of each node.\n#SBATCH --time=02:00:00\n#SBATCH --output=checkpoints/%j/out.txt\n\nset -e  # exit on error.\necho \"Date:     $(date)\"\necho \"Hostname: $(hostname)\"\necho \"Attempt #${SLURM_RESTART_COUNT:-0}\"\n\n# Make sure to use UV_OFFLINE=1 on DRAC clusters where compute nodes don't have internet access,\n# or use `module load httpproxy/1.0` if it works.\n# Note: You will either have to warm up the uv cache before submitting your job so  or use the drac wheelhouse as a source.\n# export UV_OFFLINE=1\n\n## Code checkpointing with git to avoid unexpected bugs ##\nUV_DIR=$(./code_checkpointing.sh)\necho \"Git commit used for this job: ${GIT_COMMIT:-not set - code checkpointing is not enabled}\"\necho \"Running uv commands in directory: $UV_DIR\"\n\n# Stage dataset into $SLURM_TMPDIR\n# Prepare the dataset on each node's local storage using all the CPUs (and memory) of each node.\nmkdir -p $SLURM_TMPDIR/data\nsrun --ntasks-per-node=1 --ntasks=${SLURM_JOB_NUM_NODES:-1} bash -c \\\n    \"uv run --directory=$UV_DIR python prepare_data.py --dest \\$SLURM_TMPDIR/data\"\n\n\n# These environment variables are used by torch.distributed and should ideally be set\n# before running the python script, or at the very beginning of the python script.\n# Master address is the hostname of the first node in the job.\nexport MASTER_ADDR=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n# Get a unique port for this job based on the job ID\nexport MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOB_ID | tail -c 4))\nexport WORLD_SIZE=$SLURM_NTASKS\n\n# srun is always used to launch the tasks.\n# Whether there is one 'task' per GPU or one task per node can vary based on your setup.\n# In the latter case, you would typically use torchrun or accelerate to launch one processes\n# per GPU within each task.\n# See the commented examples below for different ways to launch the training script.\n\n# Important note: In all cases, some variables (for example RANK, LOCAL_RANK, or machine_rank\n# in accelerate) vary between tasks, so we need to escape env variables such as $SLURM_PROCID,\n# $SLURM_TMPDIR and $SLURM_NODEID so they are evaluated within each task, not just once here\n# on the first node.\n\n## Pure Slurm version ##\n# They can either be set here or as early as possible in the Python script.\n# Use `uv run --offline` on clusters without internet access on compute nodes.\n# Using `srun` executes the command once per task, once per GPU in our case.\n# --gres-flags=allow-task-sharing is required to allow tasks on the same node to\n# access GPUs allocated to other tasks on that node. Without this flag,\n# --gpus-per-task=1 would isolate each task to only see its own GPU, which\n# causes a a mysterious NCCL error in\n# nn.parallel.DistributedDataParallel:\n# ncclUnhandledCudaError: Call to CUDA function failed.\n# when NCCL tries to communicate to local GPUs via shared memory but fails due\n# to cgroups isolation. See https://slurm.schedmd.com/srun.html#OPT_gres-flags\n# and https://support.schedmd.com/show_bug.cgi?id=17875 for details.\nsrun --gres-flags=allow-task-sharing bash -c \\\n    \"RANK=\\$SLURM_PROCID LOCAL_RANK=\\$SLURM_LOCALID \\\n    uv run --directory=$UV_DIR \\\n    python main.py --dataset_path=\\$SLURM_TMPDIR/data $@\"\n\n## srun + torchrun version ##\n# srun --ntasks-per-node=1 bash -c \"\\\n#     uv run torchrun --node-rank=\\$SLURM_NODEID --nnodes=\\$SLURM_STEP_NUM_NODES \\\n#     --master-addr=$MASTER_ADDR --master-port=$MASTER_PORT --nproc-per-node=gpu \\\n#     main.py $@\"\n\n## srun + accelerate version ##\n## NOTE: This particular example doesn't use accelerate, this is just here to illustrate.\n# srun --ntasks-per-node=1 bash -c \"\\\n#     uv run --directory=$UV_DIR \\\n#     accelerate launch \\\n#     --machine_rank \\$SLURM_NODEID \\\n#     --main_process_ip $MASTER_ADDR --main_process_port $MASTER_PORT \\\n#     --num_machines  $SLURM_NNODES --num_processes $SLURM_NTASKS \\\n#     main.py $@\"\n</code></pre> <p>prepare_data.py</p> <p>This script downloads and prepares the ImageNet dataset. You need to run it once before running the main training script.</p> <pre><code>\"\"\"Dataset preprocessing script.\n\nRun this with `srun --ntasks-per-node=1 --pty uv run python prepare_data.py`\n\"\"\"\n\nimport argparse\nimport datetime\nimport os\nfrom typing import Literal\nfrom torchvision.datasets import ImageNet\nfrom pathlib import Path\n\nSLURM_TMPDIR = Path(os.environ[\"SLURM_TMPDIR\"])\nNETWORK_IMAGENET_DIR = Path(\"/network/datasets/imagenet\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        \"--dest\",\n        type=Path,\n        default=SLURM_TMPDIR / \"data\",\n        help=\"Where to prepare the dataset.\",\n    )\n    parser.add_argument(\n        \"--network-imagenet-dir\",\n        type=Path,\n        default=NETWORK_IMAGENET_DIR,\n        help=\"The path to the folder containing the ILSVRC2012 train and val archives and devkit.\",\n    )\n    dest = parser.parse_args().dest\n    assert isinstance(dest, Path)\n    # to see it as soon as it happens in logs.\n    # `srun` can keep output in a buffer for quite a while otherwise.\n    print(f\"Preparing ImageNet dataset in {dest}\", flush=True)\n    _, _ = prepare_imagenet(dest)\n    print(f\"Done preparing ImageNet dataset in {dest}\")\n\n\ndef prepare_imagenet(output_directory: Path, network_imagenet_dir: Path = NETWORK_IMAGENET_DIR):\n    devkit_archive = network_imagenet_dir / \"ILSVRC2012_devkit_t12.tar.gz\"\n    train_archive = network_imagenet_dir / \"ILSVRC2012_img_train.tar\"\n    val_archive = network_imagenet_dir / \"ILSVRC2012_img_val.tar\"\n    checksums_file = network_imagenet_dir / \"md5sums\"\n    if any(\n        not p.exists()\n        for p in (network_imagenet_dir, devkit_archive, train_archive, val_archive, checksums_file)\n    ):\n        raise FileNotFoundError(\n            f\"Could not find the ImageNet dataset archives at {network_imagenet_dir}! \"\n            \"Adjust the location with the argument as needed. \"\n        )\n    output_directory.mkdir(parents=True, exist_ok=True)\n\n    _make_symlink_in_dest(devkit_archive, output_directory)\n    _make_symlink_in_dest(train_archive, output_directory)\n    _make_symlink_in_dest(val_archive, output_directory)\n    _make_symlink_in_dest(checksums_file, output_directory)\n\n    train_dataset = _make_split(output_directory, \"train\")\n    test_dataset = _make_split(output_directory, \"val\")\n    return train_dataset, test_dataset\n\n\ndef _make_symlink_in_dest(file: Path, dest_dir: Path):\n    if not (symlink_to_file := (dest_dir / file.name)).exists():\n        symlink_to_file.symlink_to(file)\n    return symlink_to_file\n\n\ndef _make_split(root: Path, split: Literal[\"train\", \"val\"]):\n    \"\"\"Use the torchvision.datasets.ImageNet class constructor to prepare the data.\n\n    There are faster ways of doing this with the `tarfile` package or fancy bash\n    commands but this is simplest.\n    \"\"\"\n    print(f\"Preparing ImageNet {split} split in {root}\", flush=True)\n    t = datetime.datetime.now()\n    d = ImageNet(root=str(root), split=split)\n    print(f\"Preparing ImageNet {split} split took {datetime.datetime.now() - t}\")\n    return d\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>main.py</p> <pre><code>\"\"\"ImageNet Distributed training script.\n\n# Features:\n- Multi-GPU / Multi-node training with DDP\n- Wandb logging\n- Checkpointing\n- Profiling with the PyTorch profiler and tensorboard\n- Good sanity checks\n- Automatic mixed precision (AMP) support\n\n# Potential Improvements - to be added as an exercise! \ud83d\ude09\n- Use a larger model from HuggingFace or change the dataset from ImageNet to a language dataset from HuggingFace\n- Use FSDP to train a larger model that doesn't fit inside a single GPU\n\n\nExample:\n\nsrun --ntasks=2 --pty uv run python main.py --epochs=1 --limit_train_samples=50_000 \\\n    --limit_val_samples=2000 --batch_size=512 --use_amp --compile=default \\\n    --run_name=1024_amp_compile_default\n\"\"\"\n\nimport contextlib\nimport dataclasses\nimport datetime\nimport logging\nimport os\nimport random\nimport subprocess\nimport sys\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, Iterable, TypeVar\n\nimport numpy as np\nimport rich.logging\nimport rich.pretty\nimport simple_parsing\nimport sklearn\nimport sklearn.model_selection\nimport torch\nimport torchvision\nimport tqdm\nimport tqdm.rich\nimport wandb\nfrom torch import Tensor, nn\nfrom torch.distributed import ReduceOp\nfrom torch.nn import functional as F\nfrom torch.profiler import profile, tensorboard_trace_handler\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torchvision.datasets import ImageNet\nfrom torchvision.transforms import v2 as transforms\n\nJOB_ID = os.environ[\"SLURM_JOB_ID\"]  # you absolutely need to be within a slurm job!\nSCRATCH = Path(os.environ[\"SCRATCH\"])\nSLURM_TMPDIR = Path(os.environ.get(\"SLURM_TMPDIR\", \"/tmp\"))\nassert SLURM_TMPDIR.exists(), f\"SLURM_TMPDIR (assumed {SLURM_TMPDIR}) should exist!\"\n\n# Set any missing environment variables so that `torch.distributed.init_process_group`\n# works properly, namely RANK, WORLD_SIZE, MASTER_ADDR, MASTER_PORT, (LOCAL_RANK).\n#\n# The accompanying sbatch script already does this in bash, which is preferable, since\n# you need to make sure that these environment variables are set before any torch operations\n# are executed. (Some modules might inadvertently initialize cuda when imported which is a problem).\n#\n# Also doing this here just in case you're using a different sbatch script or running this from\n# the vscode terminal or with the vscode debugger.\n# Using the Vscode debugger to debug multi-gpu jobs is very convenient.\n#\n# Note: here by using .setdefault we don't overwrite env variables that are already set,\n# so you could in principle use this in a workflow based on srun + torchrun or\n# srun + 'accelerate launch'.\n\nif \"SLURM_PROCID\" not in os.environ and \"RANK\" not in os.environ:\n    # If neither the SLURM nor the torch distributed env vars are set, raise an error.\n    raise RuntimeError(\n        \"Both the SLURM and the torch distributed env vars are not set! \"\n        \"This indicates that you might be running this script in something like the \"\n        \"vscode terminal with `python main.py&gt;`.\\n\"\n        f\"Consider relaunching the same command with srun instead, like so: \\n\"\n        f\"\u27a1\ufe0f    srun --pty python main.py {' '.join(sys.argv)}\\n\"\n        \"See https://slurm.schedmd.com/srun.html for more info.\"\n    )\n\n# This will raise an error if both are unset. This is expected (see above).\nRANK = int(os.environ.setdefault(\"RANK\", os.environ.get(\"SLURM_PROCID\", \"\")))\nLOCAL_RANK = int(os.environ.setdefault(\"LOCAL_RANK\", os.environ.get(\"SLURM_LOCALID\", \"\")))\nWORLD_SIZE = int(os.environ.setdefault(\"WORLD_SIZE\", os.environ.get(\"SLURM_NTASKS\", \"\")))\nMASTER_PORT = int(os.environ.setdefault(\"MASTER_PORT\", str(10000 + int(JOB_ID) % 10000)))\nif \"SLURM_JOB_NODELIST\" in os.environ:\n    # Get the hostname of the first node, for example: \"cn-l[084-085]\" --&gt; cn-l084\n    _first_node = subprocess.check_output(\n        f\"scontrol show hostnames {os.environ['SLURM_JOB_NODELIST']}\", text=True, shell=True\n    ).split()[0]\n    MASTER_ADDR = os.environ.setdefault(\"MASTER_ADDR\", _first_node)\nelse:\n    MASTER_ADDR = os.environ.setdefault(\"MASTER_ADDR\", \"127.0.0.1\")\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=f\"[{RANK + 1}/{WORLD_SIZE}] - %(message)s \",\n    handlers=[rich.logging.RichHandler(markup=True)],\n    force=True,\n)\nlogger = logging.getLogger(__name__)\n\n\nclass DummyModel(nn.Module):\n    \"\"\"Dummy model used while debugging - uses almost no compute or memory.\n\n    Examples of when this is useful:\n    -   to check if data loading is the bottleneck, we can pull samples from the dataloader\n        as fast as possible and compare that throughput (in samples/second) to the same\n        during training. If the two are similar, then the dataloader is the bottleneck.\n        Using a dummy model like this makes it so we don't have to modify our training loop\n        to do this kind of sanity check.\n    \"\"\"\n\n    def __init__(self, num_classes: int, **_kwargs):\n        super().__init__()\n        self.num_classes = num_classes\n        # A dummy weight..\n        self.linear = nn.Linear(1, num_classes)\n\n    def forward(self, x: Tensor) -&gt; Tensor:\n        mean_of_each_xi = x.flatten(1).mean(1, keepdim=True)  # [batch_size, 1]\n        return self.linear(mean_of_each_xi)  # [batch_size, num_classes]\n\n\nmodels: dict[str, Callable[..., nn.Module]] = {\n    \"dummy\": DummyModel,\n    \"resnet18\": torchvision.models.resnet18,\n    \"resnet34\": torchvision.models.resnet34,\n    \"resnet50\": torchvision.models.resnet50,\n    \"resnet101\": torchvision.models.resnet101,\n    \"resnet152\": torchvision.models.resnet152,\n    \"vit_b_16\": torchvision.models.vit_b_16,\n    \"vit_b_32\": torchvision.models.vit_b_32,  # default model\n    \"vit_l_16\": torchvision.models.vit_l_16,\n    \"vit_l_32\": torchvision.models.vit_l_32,\n}\n\n\n@dataclass\nclass Args:\n    \"\"\"Dataclass that contains the command-line arguments for this script.\"\"\"\n\n    epochs: int = 10\n    learning_rate: float = 3e-4\n    weight_decay: float = 1e-4\n    batch_size: int = 512\n\n    pretrained: bool = False\n    \"\"\"Whether to use a pretrained model or start from a random initialization.\"\"\"\n\n    checkpoint_dir: Path | None = None\n    \"\"\"Where checkpoints are stored.\"\"\"\n\n    checkpoint_interval_epochs: int = 1\n    \"\"\"Interval (in epochs) between saving checkpoints.\"\"\"\n\n    dataset_path: Path = SLURM_TMPDIR / \"data\"\n    \"\"\"Where to look for the dataset.\"\"\"\n\n    use_fake_data: bool = False\n    \"\"\"If true, use torchvision.datasets.FakeData instead of ImageNet.\n\n    Useful for debugging.\n    \"\"\"\n\n    limit_train_samples: int = 0\n    \"\"\" If &gt; 0, limit the number of training samples to this value.\n\n    This can be very useful to debug the training loop, checkpointing, and validation, or to check that\n    the model can overfit on a small number of samples.\n    \"\"\"\n\n    limit_val_samples: int = 0\n    \"\"\" If &gt; 0, limit the number of validation samples to this value.\"\"\"\n\n    num_workers: int = int(os.environ.get(\"SLURM_CPUS_PER_TASK\", len(os.sched_getaffinity(0))))\n    \"\"\"Number of dataloader workers.\"\"\"\n\n    seed: int = 42\n    \"\"\"Base random seed for everything except the train/validation split.\"\"\"\n\n    val_seed: int = 0\n    \"\"\"Random seed used to create the train/validation split.\"\"\"\n\n    model_name: str = simple_parsing.choice(*models.keys(), default=\"vit_b_32\")\n    \"\"\"Which model function to use.\"\"\"\n\n    compile: str = \"\"\n    \"\"\"If set, use torch.compile to compile the model with the given string as the \"mode\" argument.\"\"\"\n\n    verbose: int = simple_parsing.field(alias=\"-v\", action=\"count\", default=0)\n    \"\"\"Increase logging verbosity (can be specified multiple times).\"\"\"\n\n    # IDEA: Can we instead use a logging interval in seconds?\n    # One problem is that this would make it hard to compare metric values at the same step.\n    logging_interval: int = 20\n    \"\"\"Interval (in batches) between logging of training metrics to wandb or to the output file.\"\"\"\n\n    use_amp: bool = False\n    \"\"\"If True, use automatic mixed precision (AMP) for training.\"\"\"\n\n    run_name: str | None = JOB_ID + (\n        f\"_step{_step}\" if (_step := int(os.environ.get(\"SLURM_STEP_ID\", \"0\"))) &gt; 0 else \"\"\n    )\n    \"\"\"Name for the run (in wandb and in tensorboard).\"\"\"\n\n    wandb_run_id: str = JOB_ID + (\n        f\"_step{_step}\" if (_step := int(os.environ.get(\"SLURM_STEP_ID\", \"0\"))) &gt; 0 else \"\"\n    )\n    \"\"\"Unique ID for the Weights &amp; Biases run.\n\n    Used to resume a run if the job is restarted.\n    \"\"\"\n\n    wandb_group: str | None = None\n\n    wandb_project: str = \"codingtips_profiling_example\"\n\n    no_wandb: bool = False\n    \"\"\"When set, disables wandb logging.\"\"\"\n\n\ndef main():\n    # Use an argument parser so we can pass hyperparameters from the command line.\n    # You can use plain argparse if you like. Simple-parsing is an extension of argparse for dataclasses.\n    args: Args = simple_parsing.parse(Args)\n\n    # Create a checkpoints directory in $SCRATCH and symlink it so it appears in the current directory.\n    if not (_checkpoints_dir := Path(\"checkpoints\")).exists():\n        _checkpoints_dir_in_scratch = SCRATCH / \"checkpoints\"\n        _checkpoints_dir_in_scratch.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"Creating a symlink from {_checkpoints_dir} --&gt; {_checkpoints_dir_in_scratch}\")\n        _checkpoints_dir.symlink_to(_checkpoints_dir_in_scratch)\n\n    if args.checkpoint_dir is None:\n        # Use the run name or run_id as the checkpoint folder by default if unset.\n        # This makes it so the names in wandb and the names in tensorboard line up nicely.\n        args.checkpoint_dir = (\n            SCRATCH / \"checkpoints\" / (args.run_name or args.wandb_run_id or JOB_ID)\n        )\n\n    assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n    assert torch.distributed.is_available()\n    # https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html#constructing-the-process-group\n    # Default timeout is 30 minutes. Reducing the timeout here, so the job fails quicker if there's\n    # a communication problem between nodes.\n    # NOTE: Since preparing imagenet on each node can take about 12-15 minutes on the Mila cluster,\n    # we set the timeout to 20 minutes here.\n    torch.cuda.set_device(LOCAL_RANK)\n    torch.distributed.init_process_group(\n        backend=\"nccl\",\n        rank=RANK,\n        world_size=WORLD_SIZE,\n        timeout=datetime.timedelta(minutes=20),\n    )\n    is_master = RANK == 0\n\n    device = torch.device(\"cuda\", LOCAL_RANK)\n\n    print(f\"Using random seed: {args.seed}\")\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    logger.setLevel(\n        logging.WARNING\n        if args.verbose == 0\n        else logging.INFO\n        if args.verbose == 1\n        else logging.DEBUG\n    )\n    logger.info(f\"World size: {WORLD_SIZE}, global rank: {RANK}, local rank: {LOCAL_RANK}\")\n    if is_master:\n        print(\"Args:\")\n        rich.pretty.pprint(dataclasses.asdict(args))\n\n    # Create a model and move it to the GPU.\n    kwargs = {} if not args.pretrained else {\"weights\": \"DEFAULT\"}\n    with device:\n        model = models[args.model_name](num_classes=1000, **kwargs)\n        model = model.to(device=device)\n    # https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html#multi-gpu-training-with-ddp\n    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n    if args.compile:\n        # TODO: Try different torch.compile modes, see how this affects performance!\n        torch.set_float32_matmul_precision(\"high\")  # Use TensorFloat32 tensor cores.\n        model = torch.compile(model, mode=args.compile)\n    # Wrap the model with DistributedDataParallel\n    # (See https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)\n    model = nn.parallel.DistributedDataParallel(\n        model, device_ids=[LOCAL_RANK], output_device=LOCAL_RANK\n    )\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay\n    )\n    # https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n    scaler = None\n    if args.use_amp:\n        scaler = torch.amp.grad_scaler.GradScaler(enabled=True)\n        torch.set_float32_matmul_precision(\"high\")\n        logger.info(\"Using automatic mixed precision (AMP) with bfloat16\")\n\n    # Setup the dataset.\n    train_dataset, valid_dataset, test_dataset = make_datasets(\n        args.dataset_path,\n        val_split_seed=args.val_seed,\n        use_fake_data=args.use_fake_data,\n    )\n    # IDEA: Use a smaller subset of the dataset for faster debugging of the checkpointing / validation loop or\n    # to test if the model can overfit on a small number of samples.\n    if args.limit_train_samples:\n        train_dataset = torch.utils.data.Subset(\n            train_dataset, list(range(args.limit_train_samples))\n        )\n    if args.limit_val_samples:\n        valid_dataset = torch.utils.data.Subset(valid_dataset, list(range(args.limit_val_samples)))\n\n    # Restricts data loading to a subset of the dataset exclusive to the current process\n    train_sampler = DistributedSampler(\n        dataset=train_dataset, shuffle=True, num_replicas=WORLD_SIZE, rank=RANK, seed=args.seed\n    )\n    valid_sampler = DistributedSampler(\n        dataset=valid_dataset, shuffle=False, num_replicas=WORLD_SIZE, rank=RANK\n    )\n    test_sampler = DistributedSampler(\n        dataset=test_dataset, shuffle=False, num_replicas=WORLD_SIZE, rank=RANK\n    )\n    # TODO: make sure that the dataloader workers random state is restored properly.\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        sampler=train_sampler,\n        pin_memory=True,\n    )\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        sampler=valid_sampler,\n        pin_memory=True,\n    )\n    _test_dataloader = DataLoader(  # Not used in this example.\n        test_dataset,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        sampler=test_sampler,\n        pin_memory=True,\n    )\n    effective_batch_size = args.batch_size * WORLD_SIZE\n    logger.info(f\"Effective (global) batch size: {effective_batch_size}\")\n\n    # Load the latest checkpoint if it exists.\n    if previous_checkpoints := list(args.checkpoint_dir.glob(\"*.pt\")):\n        # Checkpoints are named like `epoch_0.pt`, `epoch_1.pt`. Find the latest.\n        # Note: epoch_0 in this case is the initial checkpoint before any training.\n        # epoch_1 is after one epoch of training, etc.\n        latest_checkpoint = max(previous_checkpoints, key=lambda p: int(p.stem.split(\"_\")[-1]))\n        _num_epochs_done, step, num_samples = load_checkpoint(\n            latest_checkpoint, model=model, optimizer=optimizer, device=device\n        )\n        starting_epoch = _num_epochs_done\n        total_updates = step\n        total_samples = num_samples\n        logger.info(\n            f\"Resuming training from epoch {starting_epoch} (step {step}, {total_samples} total samples)\"\n        )\n    else:\n        starting_epoch = 0\n        total_updates = 0\n        total_samples = 0\n        args.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        logger.info(\"Starting training from scratch\")\n\n    # Initialize wandb logging.\n    setup_wandb(\n        args,\n        effective_batch_size=effective_batch_size,\n        previous_checkpoints=previous_checkpoints,\n        total_updates=total_updates,\n    )\n\n    # Save an initial checkpoint (epoch 0) before training to make sure we can easily get the exact same initial weights.\n    # Since code is supposed to be correctly seeded and reproducible, this is just an additional precaution.\n    # Doing this here also makes it so if there is a checkpoint, there is also a wandb run, so we can resume the wandb run\n    # more correctly than with just `resume=\"allow\"`.\n    if not previous_checkpoints and RANK == 0:\n        save_checkpoint(\n            checkpoint_path=args.checkpoint_dir / \"epoch_0.pt\",\n            model=model,\n            optimizer=optimizer,\n            device=device,\n            epoch=0,\n            step=0,\n            num_samples=0,\n        )\n\n    # Create the PyTorch profiler with a schedule that will output some traces that can be inspected with tensorboard.\n    # https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#using-profiler-to-analyze-long-running-jobs\n    # To view the traces, run `uvx tensorboard --with=torch_tb_profiler --logdir checkpoints`\n    profiler = profile(\n        schedule=torch.profiler.schedule(wait=2, warmup=2, active=2, repeat=1),\n        on_trace_ready=tensorboard_trace_handler(\n            str(args.checkpoint_dir), worker_name=f\"rank_{RANK}\"\n        ),\n        record_shapes=True,\n        profile_memory=True,\n        # Warning: This can be a bit too verbose while debugging. Only enable this if you really need it.\n        # with_stack=True if \"debugpy\" not in sys.modules else True,\n        with_stack=True if args.verbose &gt;= 3 else False,\n        with_flops=True,\n        with_modules=True,\n    )\n\n    ###################\n    ## Training loop ##\n    ###################\n\n    # Used at the end to display overall samples per second.\n    t0 = time.time()\n    starting_num_samples = total_samples\n\n    for epoch in range(starting_epoch, args.epochs):\n        logger.debug(f\"Starting epoch {epoch}/{args.epochs}\")\n        # Important so each epoch uses a different ordering for the training samples.\n        train_sampler.set_epoch(epoch)\n        model.train()\n\n        # Using a progress bar when in an interactive terminal. It also shows the throughput in samples/second.\n        # If we're going to enable verbose logging within an epoch (for example to help identify issues),\n        # it makes sense to use the progress bar from rich so that the logs are displayed nicely.\n        # However, it doesn't support the `unit_scale` and `unit` arguments atm so we disable those arguments.\n        pbar_type = tqdm.rich.tqdm_rich if args.verbose &gt;= 2 else tqdm.tqdm\n        assert isinstance(train_dataloader.batch_size, int)\n        progress_bar = pbar_type(\n            train_dataloader,\n            desc=f\"Train epoch {epoch + 1}/{args.epochs}\",\n            # Don't use a progress bar if outputting to a slurm output file or when not in task 0\n            disable=(not sys.stdout.isatty() or not is_master),\n            unit_scale=False if pbar_type is tqdm.rich.tqdm_rich else effective_batch_size,\n            unit=\"batches\" if pbar_type is tqdm.rich.tqdm_rich else \"samples\",\n            dynamic_ncols=True,  # allow window resizing\n        )\n        data_transfer_cuda_stream = torch.cuda.Stream(device=device)\n        epoch_loss = 0.0\n        t = time.perf_counter()\n        for batch_index, batch in enumerate(\n            # We only create the profiling traces in the first two epochs.\n            profile_loop(progress_bar, profiler) if epoch &lt;= 1 else progress_bar\n        ):\n            # This allows the GPU to keep working on the previous step while the data is copied from CPU to GPU!\n            # https://docs.pytorch.org/tutorials/intermediate/pinmem_nonblock.html\n            with torch.cuda.stream(data_transfer_cuda_stream):\n                # Move the batch to the GPU before we pass it to the model\n                batch = tuple(item.to(device, non_blocking=True) for item in batch)\n                x, y = batch\n\n            loss, accuracy, n_samples = training_step(\n                model,\n                x,\n                y,\n                optimizer,\n                scaler=scaler,\n                batch_index=batch_index,\n            )\n\n            epoch_loss += loss\n            total_updates += 1\n            total_samples += n_samples\n\n            # Simple training speed calculation in samples/sec using the effective batch size.\n            new_t = time.perf_counter()\n            dt = new_t - t\n            samples_per_sec = n_samples / dt\n            t = new_t\n\n            # Move the tensors to CPU so we can log them in the progress bar and to wandb.\n            # Perform some logging, but only on the first task, and only every `logging_interval` batches.\n            # Also only do this if wandb logging is enabled or if the progress bar is enabled.\n            if (\n                is_master\n                and ((wandb.run and not wandb.run.disabled) or (not progress_bar.disable))\n                and (batch_index == 0 or ((batch_index + 1) % args.logging_interval) == 0)\n            ):\n                # TODO: if --limit_train_samples=100_000, the logs in wandb have their last logged metrics\n                # at samples=89_600 (7(updates) * 50(log interval) * 256(batch_size)). It would be nice to\n                # also log metrics at the last batch (when we reach the limit_num_steps) even if batch_index\n                # isnt a multiple of logging interval.\n\n                _loss = loss.item()\n                _accuracy = accuracy.item()\n                progress_bar.set_postfix(loss=f\"{_loss:.3f}\", accuracy=f\"{_accuracy:.2%}\")\n                wandb.log(\n                    {\n                        \"train/loss\": _loss,\n                        \"train/accuracy\": _accuracy,\n                        \"train/samples_per_sec\": samples_per_sec,\n                        \"epoch\": epoch,\n                        \"updates\": total_updates,\n                        \"samples\": total_samples,\n                    }\n                )\n        progress_bar.close()\n\n        t = time.perf_counter()\n        val_loss, val_accuracy, val_samples = validation_loop(model, valid_dataloader, device)\n        dt = time.perf_counter() - t\n        val_sps = val_samples / dt\n        if RANK == 0:\n            rich.print(\n                f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%} samples/sec: {val_sps:.1f}\"\n            )\n        wandb.log(\n            {\n                \"val/loss\": val_loss,\n                \"val/accuracy\": val_accuracy,\n                \"val/samples_per_sec\": val_sps,\n                \"epoch\": epoch,\n            }\n        )\n\n        # Only save the checkpoint from the master process.\n        # Make sure this doesn't cause a torch.distributed.timeout if it takes too long.\n        if is_master and (epoch % args.checkpoint_interval_epochs) == 0:\n            # save as epoch_1 after having done 1 epoch of training.\n            save_checkpoint(\n                checkpoint_path=args.checkpoint_dir / f\"epoch_{epoch + 1}.pt\",\n                model=model,\n                optimizer=optimizer,\n                device=device,\n                epoch=epoch,\n                step=total_updates,\n                num_samples=int(total_samples),\n            )\n\n    torch.distributed.destroy_process_group()\n    total_time = time.time() - t0\n    overall_samples = int(total_samples) - starting_num_samples\n    overall_sps = overall_samples / total_time\n    if wandb.run:\n        wandb.run.summary[\"overall_train_samples_per_sec\"] = overall_sps\n        wandb.run.finish()\n    print(f\"Done in {total_time:.1f} seconds, with {overall_sps:.1f} images/second\")\n\n\ndef training_step(\n    model: nn.Module,\n    x: Tensor,\n    y: Tensor,\n    optimizer: torch.optim.Optimizer,\n    scaler: torch.amp.grad_scaler.GradScaler | None = None,\n    batch_index: int | None = None,\n):\n    with torch.autocast(\n        device_type=\"cuda\", dtype=torch.bfloat16, enabled=scaler is not None and scaler.is_enabled()\n    ):\n        # Forward pass\n        logits: Tensor = model(x)\n\n        local_loss = F.cross_entropy(logits, y, reduction=\"mean\")\n\n    if scaler is not None:\n        # https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision\n        scaler.scale(local_loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n    else:\n        # nn.DistributedDataParallel automatically averages the gradients across devices.\n        local_loss.backward()\n        optimizer.step()\n    optimizer.zero_grad()\n\n    # Calculate some metrics\n\n    # local metrics calculated with the tensors on the current GPU.\n    local_n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n    local_n_samples = logits.shape[0]\n    local_accuracy = local_n_correct_predictions / local_n_samples\n\n    # Global metrics calculated with the results from all workers\n    # Creating new tensors to hold the \"global\" values, but this isn't required.\n    # Reduce the local metrics across all workers, sending the result to rank 0.\n    # Summing n_correct and n_samples to get accuracy is resilient to\n    # workers having different number of samples.\n    # This could happen if the number of batches is not divisible by the number of batches\n    # and if the distributed sampler is not set to drop the last incomplete batch.\n    n_correct_predictions = local_n_correct_predictions.clone()\n    n_samples = local_n_samples * torch.ones(1, device=local_loss.device, dtype=torch.int32)\n    loss = local_loss.clone().detach()\n\n    torch.distributed.reduce(loss, dst=0, op=ReduceOp.AVG)\n    torch.distributed.reduce(n_correct_predictions, dst=0, op=ReduceOp.SUM)\n    torch.distributed.reduce(n_samples, dst=0, op=ReduceOp.SUM)\n    accuracy = n_correct_predictions / n_samples\n\n    # Using lazy formatting so these tensors are only moved to cpu when necessary.\n    if RANK == 0:\n        logger.debug(\"(local) Loss: %.2f Accuracy: %.2f\", local_loss, local_accuracy)\n        logger.debug(\"Average Loss: %.2f Accuracy: %.2%\", loss, accuracy)\n    return loss, accuracy, n_samples\n\n\n@torch.no_grad()\ndef validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n    model.eval()\n\n    epoch_loss = torch.zeros(1, device=device)\n    num_samples = torch.zeros(1, device=device, dtype=torch.int32)\n    correct_predictions = torch.zeros(1, device=device, dtype=torch.int32)\n    assert isinstance(dataloader.batch_size, int)\n\n    progress_bar = tqdm.tqdm(\n        dataloader,\n        desc=\"Validation\",\n        unit_scale=dataloader.batch_size * WORLD_SIZE,\n        unit=\"samples\",\n        # Don't use a progress bar if outputting to a slurm output file or when not in task 0\n        disable=(not sys.stdout.isatty() or RANK != 0),\n    )\n    # NOTE: Because of DDP and distributed sampler, the last batch might have repeated samples,\n    # leading to slightly imprecise metrics.\n    for batch in progress_bar:\n        batch = tuple(item.to(device) for item in batch)\n        x, y = batch\n\n        logits: Tensor = model(x)\n        loss = F.cross_entropy(logits, y, reduction=\"sum\")\n\n        batch_n_samples = x.shape[0]\n        batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n\n        epoch_loss += loss\n        num_samples += batch_n_samples\n        correct_predictions += batch_correct_predictions\n    # Here we only need to reduce metrics once, after iterating over the entire dataset.\n    torch.distributed.reduce(epoch_loss, dst=0, op=ReduceOp.SUM)\n    torch.distributed.reduce(num_samples, dst=0, op=ReduceOp.SUM)\n    torch.distributed.reduce(correct_predictions, dst=0, op=ReduceOp.SUM)\n    epoch_average_loss = epoch_loss / num_samples\n    accuracy = correct_predictions / num_samples\n    return epoch_average_loss.item(), accuracy.item(), num_samples.item()\n\n\ndef setup_wandb(\n    args: Args, effective_batch_size: int, previous_checkpoints: list[Path], total_updates: int\n):\n    \"\"\"Calls `wandb.init` with the appropriate arguments.\"\"\"\n    # Normally you would only do this in the first task (rank 0), but here we do it in all tasks\n    # using the new \"shared\" feature of wandb. This makes it much easier to track the GPU util of\n    # all gpus on all nodes in the job.\n    # See this link for more info:\n    # - https://docs.wandb.ai/guides/track/log/distributed-training/#track-all-processes-to-a-single-run\n    is_master = RANK == 0\n    with goes_first(is_master):\n        run = wandb.init(\n            project=args.wandb_project,\n            # if None, wandb will use a random name.\n            name=args.run_name if args.run_name else None,\n            id=args.wandb_run_id,\n            # It's a good idea to log the SLURM environment variables to wandb.\n            config=(\n                dataclasses.asdict(args)\n                | {k: v for k, v in os.environ.items() if k.startswith(\"SLURM_\")}\n                | dict(\n                    effective_batch_size=effective_batch_size,\n                    WORLD_SIZE=WORLD_SIZE,\n                    MASTER_ADDR=MASTER_ADDR,\n                    MASTER_PORT=MASTER_PORT,\n                )\n            ),\n            group=args.wandb_group,\n            # Use the new \"shared\" mode to log system utilization metrics from all tasks in the job:\n            settings=wandb.Settings(\n                mode=\"disabled\" if args.no_wandb else os.environ.get(\"WANDB_MODE\", \"shared\"),  # type: ignore\n                x_primary=is_master,\n                x_label=f\"task_{RANK}\",\n                x_stats_gpu_device_ids=[LOCAL_RANK],\n                x_update_finish_state=not is_master,\n            ),\n            # Resume an existing run with the same ID if the job is restarting after being preempted.\n            # It would be *really* nice to use this resume feature, but this is new\n            # at the time of writing (2025-09) and needs to be enabled for your project\n            # by contacting wandb support.\n            # resume_from=(\n            #     f\"{args.wandb_run_id}?_step={total_updates}\"\n            #     if previous_checkpoints and args.wandb_run_id\n            #     else None\n            # ),\n            # resume=None if previous_checkpoints and args.wandb_run_id else \"allow\",\n            # Use this for the time being instead:\n            resume=\"allow\",\n        )\n        # Wait a bit to make sure the run is created properly in wandb by the first task before other workers try to\n        # also create it. Otherwise we can get a 409 error from the wandb server.\n        time.sleep(5)\n\n    # Specify the step metric (x-axis) and the metric to log against it (y-axis)\n    run.define_metric(\"train/*\", step_metric=\"updates\")\n    run.define_metric(\"val/*\", step_metric=\"epoch\")\n    # https://docs.wandb.ai/guides/track/log/log-summary/#customize-summary-metrics\n    run.define_metric(\"train/samples_per_sec\", summary=\"max\")\n    run.define_metric(\"train/samples_per_sec\", summary=\"mean\")\n    run.define_metric(\"train/samples_per_sec\", summary=\"min\")\n    run.define_metric(\"val/samples_per_sec\", summary=\"max\")\n    run.define_metric(\"val/samples_per_sec\", summary=\"mean\")\n    run.define_metric(\"val/samples_per_sec\", summary=\"min\")\n\n\nT = TypeVar(\"T\")\n\n\ndef profile_loop(dataloader: Iterable[T], profiler: torch.profiler.profile) -&gt; Iterable[T]:\n    \"\"\"Wraps the dataloader (or progress bar) and calls .step after each batch.\n\n    This is used to save one level of indentation (with profiler block) and to call prof.step() at each step.\n\n    Note, this doesn't need to be done at every epoch. It creates files used by tensorboard.\n    \"\"\"\n    with profiler as prof:\n        for batch in dataloader:\n            yield batch\n            prof.step()\n\n\ndef make_datasets(\n    path: Path,\n    val_split: float = 0.1,\n    val_split_seed: int = 42,\n    use_fake_data: bool = False,\n):\n    \"\"\"Returns the training, validation, and test splits.\"\"\"\n    if use_fake_data:\n        train_dataset = torchvision.datasets.FakeData(\n            size=1_281_167,\n            image_size=(3, 224, 224),\n            num_classes=1000,\n            transform=transforms.ToTensor(),\n        )\n        valid_dataset = torchvision.datasets.FakeData(\n            size=20_000,\n            image_size=(3, 224, 224),\n            num_classes=1000,\n            transform=transforms.Compose(\n                [transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]\n            ),\n        )\n        test_dataset = torchvision.datasets.FakeData(\n            size=50_000,\n            image_size=(3, 224, 224),\n            num_classes=1000,\n            transform=transforms.Compose(\n                [transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)]\n            ),\n        )\n        return train_dataset, valid_dataset, test_dataset\n    # todo: torchvision transforms can apparently moved to the GPU now? Would that speed up the training?\n    train_transforms = torch.nn.Sequential(\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToImage(),\n        transforms.ToDtype(torch.float32, scale=True),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    )\n    test_transforms = torch.nn.Sequential(\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToImage(),\n        transforms.ToDtype(torch.float32, scale=True),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    )\n    # This takes ~12-15 minutes on the Mila cluster. The timeout value for the distributed process group\n    # needs to be higher than this to avoid a timeout.\n    # TODO: Could we setup a new process group just for this operation, with a high enough timeout,\n    # that way the default process group can keep a short timeout value to waste less time in case of errors.\n    with goes_first(LOCAL_RANK == 0):\n        from prepare_data import prepare_imagenet\n\n        logging.info(f\"Preparing the ImageNet dataset in {path}\")\n        prepare_imagenet(path)\n        logging.info(f\"Done preparing the ImageNet dataset in {path}\")\n\n    train_dataset = ImageNet(root=path, transform=train_transforms, split=\"train\")\n    valid_dataset = ImageNet(root=path, transform=test_transforms, split=\"train\")\n    test_dataset = ImageNet(root=path, transform=test_transforms, split=\"val\")\n\n    # Split the training dataset into a training and validation set, based on a stratified split.\n    # This is important to have a balanced distribution of classes in both sets.\n    # See the sklearn.model_selection.train_test_split documentation for more info.\n    n_samples = len(train_dataset)\n    n_valid = int(val_split * n_samples)\n    n_train = n_samples - n_valid\n    train_indices, val_indices = sklearn.model_selection.train_test_split(\n        np.arange(n_samples),\n        train_size=n_train,\n        test_size=n_valid,\n        random_state=np.random.RandomState(val_split_seed),\n        shuffle=True,\n        stratify=train_dataset.targets,\n    )\n    train_dataset = torch.utils.data.Subset(train_dataset, train_indices)\n    valid_dataset = torch.utils.data.Subset(valid_dataset, val_indices)\n\n    return train_dataset, valid_dataset, test_dataset\n\n\ndef load_checkpoint(\n    checkpoint_path: Path,\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n) -&gt; tuple[int, int, int]:\n    logger.info(f\"Loading checkpoint {checkpoint_path}\")\n    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n    model.load_state_dict(checkpoint[\"model\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n    epoch = checkpoint[\"epoch\"]\n    step = checkpoint[\"step\"]\n    nsamples = checkpoint[\"num_samples\"]\n    random.setstate(checkpoint[\"python_rng_state\"])\n    np.random.set_state(checkpoint[\"numpy_rng_state\"])\n    cpu_rng_state = checkpoint[\"torch_rng_state_cpu\"]\n    torch.random.set_rng_state(cpu_rng_state.cpu())\n    torch.cuda.random.set_rng_state_all([t.cpu() for t in checkpoint[\"torch_rng_state_gpu\"]])\n    return epoch, step, nsamples\n\n\ndef save_checkpoint(\n    checkpoint_path: Path,\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n    step: int,\n    num_samples: int,\n):\n    logger.info(f\"Saving checkpoint at {checkpoint_path}\")\n    checkpoint = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"epoch\": epoch,\n        \"step\": step,\n        \"num_samples\": num_samples,\n        \"python_rng_state\": random.getstate(),\n        \"numpy_rng_state\": np.random.get_state(),\n        \"torch_rng_state_cpu\": torch.random.get_rng_state(),\n        \"torch_rng_state_gpu\": torch.cuda.random.get_rng_state_all(),\n    }\n    tmp_checkpoint_path = checkpoint_path.with_suffix(\".temp\")\n    torch.save(checkpoint, tmp_checkpoint_path)\n    tmp_checkpoint_path.rename(checkpoint_path)\n\n\n@contextlib.contextmanager\ndef goes_first(condition: bool, group: torch.distributed.ProcessGroup | None = None):\n    if condition:\n        yield\n        torch.distributed.barrier(group=group, device_ids=[LOCAL_RANK])\n    else:\n        torch.distributed.barrier(group=group, device_ids=[LOCAL_RANK])\n        yield\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/advanced/imagenet/#running-this-example","title":"Running this example","text":"<p>You can submit this as a batch job with sbatch, or you can run it in an interactive job with <code>srun</code>:</p> <pre><code>$ sbatch job.sh\n</code></pre> <p>or, for example in an interactive job:</p> <pre><code>$ ssh mila 'git clone https://github.com/mila-iqia/mila-docs'\n\n# Get an interactive job. You can use as many nodes or gpus, in whatever configuration you wish.\n# Here we choose to use between 1 and 2 nodes, with 4 GPUs distributed in any between the two nodes. (could be 4-0, 3-1, 2-2, etc.)\n$ ssh -tt mila salloc --nodes=1-2 --ntasks=4 --gpus-per-task=l40s:1 --cpus-per-task=4 --mem=32G --tmp=200G --time=02:59:00 --partition=short-unkillable\nsalloc: Granted job allocation 7782523\nsalloc: Waiting for resource configuration\nsalloc: Nodes cn-l[023,054] are ready for job\n\n# Run the dataset preparation on each node:\n$ cd mila-docs/docs/examples/advanced/imagenet\n$ srun --ntasks-per-node=1 uv run python prepare_data.py\n\n# Run the training script on each gpu on each node\n# NOTE: this only works in an interactive terminal with salloc! For the VsCode integrated terminal, see below.\n$ srun uv run python prepare_data.py\n</code></pre> <p>To open this example with VsCode:</p> <pre><code>$ mila code mila-docs/docs/examples/advanced/imagenet --alloc --ntasks=4 --gpus-per-task=l40s:1 --mem=32G --tmp=200G --time=02:59:00 --partition=short-unkillable\n# Or, if you are already in a terminal in an interactive job:\n$ mila code mila-docs/docs/examples/advanced/imagenet --job 7782523\n</code></pre> <p>Then, in the vscode terminal, you will have to explicitly list out the number of nodes and tasks to use, since those can't be inferred from the SLURM environment variables (which are not present, since you are SSH-ing into the compute node).</p> <pre><code># If your job has 2 nodes, for example:\n$ srun --ntasks-per-node=1 --nodes=2 uv run python prepare_data.py\n# Launch the training script on each gpu on each node\n$ srun --ntasks=4 --nodes=2 uv run python main.py\n</code></pre>"},{"location":"examples/distributed/","title":"Distributed Training","text":"About these examples <p>This section contains some minimal examples of how to run jobs on the Mila cluster. Each example is self-contained and can be run as-is directly on the cluster without error.  Each example has the following structure:</p> <ul> <li><code>job.sh</code>: SLURM <code>sbatch</code> script. Can be launched with <code>sbatch job.sh</code>.</li> <li><code>main.py</code>: Example python script.</li> </ul> <p>Some examples are displayed as a difference with respect to a \"base\" example. For instance, the multi-gpu example is shown as a difference with respect to the single-gpu example.</p> <ul> <li>Single GPU Job</li> <li>Multi-GPU Job</li> <li>Multi-node Job</li> </ul>"},{"location":"examples/distributed/multi_gpu/","title":"Multi-GPU Job","text":"<p>Prerequisites:</p> <ul> <li>PyTorch Setup</li> <li>Single-GPU Job</li> </ul> <p>Other interesting resources:</p> <ul> <li>sebarnold.net dist blog</li> <li>Multi-node PyTorch distributed training guide (Lambda Labs)</li> </ul> <p>Click here to see the code for this example</p> <p>job.sh</p> <pre><code> # distributed/single_gpu/job.sh -&gt; distributed/multi_gpu/job.sh\n #!/bin/bash\n-#SBATCH --ntasks=1\n-#SBATCH --ntasks-per-node=1\n+#SBATCH --ntasks=2\n+#SBATCH --ntasks-per-node=2\n #SBATCH --cpus-per-task=4\n #SBATCH --gpus-per-task=l40s:1\n #SBATCH --mem-per-gpu=16G\n #SBATCH --time=00:15:00\n\n # Exit on error\n set -e\n\n # Echo time and hostname into log\n echo \"Date:     $(date)\"\n echo \"Hostname: $(hostname)\"\n\n # To make your code as much reproducible as possible with\n # `torch.use_deterministic_algorithms(True)`, uncomment the following block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # export CUBLAS_WORKSPACE_CONFIG=:4096:8\n ## === Reproducibility (END) ===\n\n # Stage dataset into $SLURM_TMPDIR\n mkdir -p $SLURM_TMPDIR/data\n cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n # General-purpose alternatives combining copy and unpack:\n #     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n #     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n\n-# Execute Python script\n+# Get a unique port for this job based on the job ID\n+export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))\n+export MASTER_ADDR=\"127.0.0.1\"\n+\n+# Execute Python script in each task (one per GPU)\n # Use the `--offline` option of `uv run` on clusters without internet access on compute nodes.\n # Using the `--locked` option can help make your experiments easier to reproduce (it forces\n # your uv.lock file to be up to date with the dependencies declared in pyproject.toml).\n-srun uv run python main.py\n+# --gres-flags=allow-task-sharing is required to allow tasks on the same node to\n+# access GPUs allocated to other tasks on that node. Without this flag,\n+# --gpus-per-task=1 would isolate each task to only see its own GPU, which\n+# causes a a mysterious NCCL error in\n+# nn.parallel.DistributedDataParallel:\n+# ncclUnhandledCudaError: Call to CUDA function failed.\n+# when NCCL tries to communicate to local GPUs via shared memory but fails due\n+# to cgroups isolation. See https://slurm.schedmd.com/srun.html#OPT_gres-flags\n+# and https://support.schedmd.com/show_bug.cgi?id=17875 for details.\n+srun --gres-flags=allow-task-sharing uv run python main.py\n</code></pre> <p>pyproject.toml</p> <pre><code>[project]\nname = \"multi-gpu-example\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11,&lt;3.14\"\ndependencies = [\n    \"rich&gt;=14.0.0\",\n    \"torch&gt;=2.7.1\",\n    \"torchvision&gt;=0.22.1\",\n    \"tqdm&gt;=4.67.1\",\n]\n</code></pre> <p>main.py</p> <pre><code> # distributed/single_gpu/main.py -&gt; distributed/multi_gpu/main.py\n-\"\"\"Single-GPU training example.\"\"\"\n+\"\"\"Multi-GPU Training example.\"\"\"\n\n import argparse\n import logging\n import os\n import random\n import sys\n from pathlib import Path\n\n import numpy as np\n import rich.logging\n import torch\n+import torch.distributed\n from torch import Tensor, nn\n+from torch.distributed import ReduceOp\n from torch.nn import functional as F\n from torch.utils.data import DataLoader, random_split\n+from torch.utils.data.distributed import DistributedSampler\n from torchvision import transforms\n from torchvision.datasets import CIFAR10\n from torchvision.models import resnet18\n from tqdm import tqdm\n\n\n # To make your code as much reproducible as possible, uncomment the following\n # block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # torch.use_deterministic_algorithms(True)\n ## === Reproducibility (END) ===\n\n\n def main():\n     # Use an argument parser so we can pass hyperparameters from the command line.\n     parser = argparse.ArgumentParser(description=__doc__)\n     parser.add_argument(\"--epochs\", type=int, default=10)\n     parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n     parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n     parser.add_argument(\"--batch-size\", type=int, default=128)\n     parser.add_argument(\"--seed\", type=int, default=42)\n     args = parser.parse_args()\n\n     epochs: int = args.epochs\n     learning_rate: float = args.learning_rate\n     weight_decay: float = args.weight_decay\n+    # NOTE: This is the \"local\" batch size, per-GPU.\n     batch_size: int = args.batch_size\n     seed: int = args.seed\n\n     # Seed the random number generators as early as possible for reproducibility\n     random.seed(seed)\n     np.random.seed(seed)\n     torch.random.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n\n     # Check that the GPU is available\n     assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n+    rank, world_size = setup()\n+    is_master = rank == 0\n+    # When using --gpus-per-task=1, SLURM sets CUDA_VISIBLE_DEVICES so each process\n+    # only sees one GPU (index 0). Use device 0 directly.\n     device = torch.device(\"cuda\", 0)\n\n     # Setup logging (optional, but much better than using print statements)\n     # Uses the `rich` package to make logs pretty.\n     logging.basicConfig(\n         level=logging.INFO,\n-        format=\"%(message)s\",\n+        format=f\"[{rank}/{world_size}] %(name)s - %(message)s \",\n         handlers=[\n             rich.logging.RichHandler(\n                 markup=True,\n                 console=rich.console.Console(\n                     # Allower wider log lines in sbatch output files than on the terminal.\n                     width=120 if not sys.stdout.isatty() else None\n                 ),\n             )\n         ],\n     )\n\n     logger = logging.getLogger(__name__)\n+    logger.info(f\"World size: {world_size}, global rank: {rank}\")\n\n     # Create a model and move it to the GPU.\n     model = resnet18(num_classes=10)\n     model.to(device=device)\n\n+    # Wrap the model with DistributedDataParallel\n+    # (See https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)\n+    # When using --gpus-per-task=1, each process only sees one GPU (index 0).\n+    model = nn.parallel.DistributedDataParallel(\n+        model, device_ids=[0], output_device=0\n+    )\n+\n     optimizer = torch.optim.AdamW(\n         model.parameters(), lr=learning_rate, weight_decay=weight_decay\n     )\n\n     # Setup CIFAR10\n     num_workers = get_num_workers()\n     dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n-    train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))\n+    train_dataset, valid_dataset, test_dataset = make_datasets(\n+        str(dataset_path), is_master=is_master\n+    )\n+\n+    # Restricts data loading to a subset of the dataset exclusive to the current process\n+    train_sampler = DistributedSampler(dataset=train_dataset, shuffle=True)\n+    valid_sampler = DistributedSampler(dataset=valid_dataset, shuffle=False)\n+    test_sampler = DistributedSampler(dataset=test_dataset, shuffle=False)\n+\n+    # NOTE: Here `batch_size` is still the \"local\" (per-gpu) batch size.\n+    # This way, the effective batch size scales directly with number of GPUs, no need to specify it\n+    # in advance. You might want to adjust the learning rate and other hyper-parameters though.\n+    if is_master:\n+        logger.info(f\"Effective batch size: {batch_size * world_size}\")\n     train_dataloader = DataLoader(\n         train_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n-        shuffle=True,\n+        shuffle=False,  # shuffling is now done in the sampler, not the dataloader.\n+        sampler=train_sampler,\n     )\n     valid_dataloader = DataLoader(\n         valid_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n+        sampler=valid_sampler,\n     )\n     _test_dataloader = DataLoader(  # NOTE: Not used in this example.\n         test_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n+        sampler=test_sampler,\n     )\n\n     # Checkout the \"checkpointing and preemption\" example for more info!\n     logger.debug(\"Starting training from scratch.\")\n\n     for epoch in range(epochs):\n         logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n+        # NOTE: Here we need to call `set_epoch` so the ordering is able to change at each epoch.\n+        train_sampler.set_epoch(epoch)\n+\n         # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)\n         model.train()\n\n         # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n         progress_bar = tqdm(\n             total=len(train_dataloader),\n             desc=f\"Train epoch {epoch}\",\n-            disable=not sys.stdout.isatty(),  # Disable progress bar in non-interactive environments.\n+            # Disable progress bar in non-interactive environments.\n+            disable=not (sys.stdout.isatty() and is_master),\n         )\n\n         # Training loop\n         for batch in train_dataloader:\n             # Move the batch to the GPU before we pass it to the model\n             batch = tuple(item.to(device) for item in batch)\n             x, y = batch\n\n             # Forward pass\n             logits: Tensor = model(x)\n\n-            loss = F.cross_entropy(logits, y)\n+            local_loss = F.cross_entropy(logits, y)\n\n             optimizer.zero_grad()\n-            loss.backward()\n+            local_loss.backward()\n+            # NOTE: nn.DistributedDataParallel automatically averages the gradients across devices.\n             optimizer.step()\n\n             # Calculate some metrics:\n-            n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n-            n_samples = y.shape[0]\n+            # local metrics\n+            local_n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n+            local_n_samples = logits.shape[0]\n+            local_accuracy = local_n_correct_predictions / local_n_samples\n+\n+            # \"global\" metrics: calculated with the results from all workers\n+            # NOTE: Creating new tensors to hold the \"global\" values, but this isn't required.\n+            n_correct_predictions = local_n_correct_predictions.clone()\n+            # Reduce the local metrics across all workers, sending the result to rank 0.\n+            torch.distributed.reduce(n_correct_predictions, dst=0, op=ReduceOp.SUM)\n+            # Actual (global) batch size for this step.\n+            n_samples = torch.as_tensor(local_n_samples, device=device)\n+            torch.distributed.reduce(n_samples, dst=0, op=ReduceOp.SUM)\n+            # Will store the average loss across all workers.\n+            loss = local_loss.clone()\n+            torch.distributed.reduce(loss, dst=0, op=ReduceOp.SUM)\n+            loss.div_(world_size)  # Report the average loss across all workers.\n+\n             accuracy = n_correct_predictions / n_samples\n\n-            logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n-            logger.debug(f\"Average Loss: {loss.item()}\")\n+            logger.debug(f\"(local) Accuracy: {local_accuracy:.2%}\")\n+            logger.debug(f\"(local) Loss: {local_loss.item()}\")\n+            # NOTE: This would log the same values in all workers. Only logging on master:\n+            if is_master:\n+                logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n+                logger.debug(f\"Average Loss: {loss.item()}\")\n\n             # Advance the progress bar one step and update the progress bar text.\n             progress_bar.update(1)\n             progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n         progress_bar.close()\n\n         val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)\n-        logger.info(\n-            f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n-        )\n+        # NOTE: This would log the same values in all workers. Only logging on master:\n+        if is_master:\n+            logger.info(\n+                f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n+            )\n\n     print(\"Done!\")\n\n\n @torch.no_grad()\n def validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n     model.eval()\n\n-    total_loss = 0.0\n-    n_samples = 0\n-    correct_predictions = 0\n+    total_loss = torch.as_tensor(0.0, device=device)\n+    n_samples = torch.as_tensor(0, device=device)\n+    correct_predictions = torch.as_tensor(0, device=device)\n\n     for batch in dataloader:\n         batch = tuple(item.to(device) for item in batch)\n         x, y = batch\n\n         logits: Tensor = model(x)\n         loss = F.cross_entropy(logits, y)\n\n         batch_n_samples = x.shape[0]\n         batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n\n-        total_loss += loss.item()\n+        total_loss += loss\n         n_samples += batch_n_samples\n         correct_predictions += batch_correct_predictions\n\n+    # Sum up the metrics we gathered on each worker before returning the overall val metrics.\n+    torch.distributed.all_reduce(total_loss, op=torch.distributed.ReduceOp.SUM)\n+    torch.distributed.all_reduce(correct_predictions, op=torch.distributed.ReduceOp.SUM)\n+    torch.distributed.all_reduce(n_samples, op=torch.distributed.ReduceOp.SUM)\n+\n     accuracy = correct_predictions / n_samples\n     return total_loss, accuracy\n\n\n+def setup():\n+    assert torch.distributed.is_available()\n+    print(\"PyTorch Distributed available.\")\n+    print(\"  Backends:\")\n+    print(f\"    Gloo: {torch.distributed.is_gloo_available()}\")\n+    print(f\"    NCCL: {torch.distributed.is_nccl_available()}\")\n+    print(f\"    MPI:  {torch.distributed.is_mpi_available()}\")\n+\n+    # DDP Job is being run via `srun` on a slurm cluster.\n+    rank = int(os.environ[\"SLURM_PROCID\"])\n+    world_size = int(os.environ[\"SLURM_NTASKS\"])\n+\n+    # SLURM var -&gt; torch.distributed vars in case needed\n+    # NOTE: Setting these values isn't exactly necessary, but some code might assume it's\n+    # being run via torchrun or torch.distributed.launch, so setting these can be a good idea.\n+    os.environ[\"RANK\"] = str(rank)\n+    os.environ[\"WORLD_SIZE\"] = str(world_size)\n+\n+    torch.distributed.init_process_group(\n+        backend=\"nccl\",\n+        init_method=\"env://\",\n+        world_size=world_size,\n+        rank=rank,\n+    )\n+    return rank, world_size\n+\n+\n def make_datasets(\n     dataset_path: str,\n+    is_master: bool,\n     val_split: float = 0.1,\n     val_split_seed: int = 42,\n ):\n     \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n     NOTE: We don't use image transforms here for simplicity.\n     Having different transformations for train and validation would complicate things a bit.\n     Later examples will show how to do the train/val/test split properly when using transforms.\n+\n+    NOTE: Only the master process (rank-0) downloads the dataset if necessary.\n     \"\"\"\n+    # - Master: Download (if necessary) THEN Barrier\n+    # - others: Barrier THEN *NO* Download\n+    if not is_master:\n+        # Wait for the master process to finish downloading (reach the barrier below)\n+        torch.distributed.barrier()\n     train_dataset = CIFAR10(\n-        root=dataset_path, transform=transforms.ToTensor(), download=True, train=True\n+        root=dataset_path,\n+        transform=transforms.ToTensor(),\n+        download=is_master,\n+        train=True,\n     )\n     test_dataset = CIFAR10(\n-        root=dataset_path, transform=transforms.ToTensor(), download=True, train=False\n+        root=dataset_path,\n+        transform=transforms.ToTensor(),\n+        download=is_master,\n+        train=False,\n     )\n+    if is_master:\n+        # Join the workers waiting in the barrier above. They can now load the datasets from disk.\n+        torch.distributed.barrier()\n     # Split the training dataset into a training and validation set.\n     n_samples = len(train_dataset)\n     n_valid = int(val_split * n_samples)\n     n_train = n_samples - n_valid\n     train_dataset, valid_dataset = random_split(\n         train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n     )\n     return train_dataset, valid_dataset, test_dataset\n\n\n def get_num_workers() -&gt; int:\n     \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n     if \"SLURM_CPUS_PER_TASK\" in os.environ:\n         return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n     if hasattr(os, \"sched_getaffinity\"):\n         return len(os.sched_getaffinity(0))\n     return torch.multiprocessing.cpu_count()\n\n\n if __name__ == \"__main__\":\n     main()\n</code></pre>"},{"location":"examples/distributed/multi_gpu/#running-this-example","title":"Running this example","text":"<pre><code>sbatch job.sh\n</code></pre>"},{"location":"examples/distributed/multi_node/","title":"Multi-Node (DDP) Job","text":"<p>Prerequisites:</p> <ul> <li>PyTorch Setup</li> <li>Single GPU Job</li> <li>Multi-GPU Job</li> </ul> <p>Other interesting resources:</p> <ul> <li>sebarnold.net dist blog</li> <li>Lambda Labs multi-node PyTorch guide</li> </ul> <p>Click here to see the source code for this example</p> <p>job.sh</p> <pre><code> # distributed/multi_gpu/job.sh -&gt; distributed/multi_node/job.sh\n #!/bin/bash\n-#SBATCH --ntasks=2\n+#SBATCH --ntasks=4\n #SBATCH --ntasks-per-node=2\n #SBATCH --cpus-per-task=4\n #SBATCH --gpus-per-task=l40s:1\n #SBATCH --mem-per-gpu=16G\n #SBATCH --time=00:15:00\n\n # Exit on error\n set -e\n\n # Echo time and hostname into log\n echo \"Date:     $(date)\"\n echo \"Hostname: $(hostname)\"\n\n # To make your code as much reproducible as possible with\n # `torch.use_deterministic_algorithms(True)`, uncomment the following block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # export CUBLAS_WORKSPACE_CONFIG=:4096:8\n ## === Reproducibility (END) ===\n\n-# Stage dataset into $SLURM_TMPDIR\n-mkdir -p $SLURM_TMPDIR/data\n-cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n-# General-purpose alternatives combining copy and unpack:\n-#     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n-#     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n+# Stage dataset into $SLURM_TMPDIR (only on the first worker of each node)\n+srun --ntasks=$SLURM_JOB_NUM_NODES --ntasks-per-node=1 bash -c \\\n+   'mkdir -p $SLURM_TMPDIR/data &amp;&amp; cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/'\n\n # Get a unique port for this job based on the job ID\n export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))\n-export MASTER_ADDR=\"127.0.0.1\"\n+export MASTER_ADDR=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\n\n # Execute Python script in each task (one per GPU)\n # Use the `--offline` option of `uv run` on clusters without internet access on compute nodes.\n # Using the `--locked` option can help make your experiments easier to reproduce (it forces\n # your uv.lock file to be up to date with the dependencies declared in pyproject.toml).\n # --gres-flags=allow-task-sharing is required to allow tasks on the same node to\n # access GPUs allocated to other tasks on that node. Without this flag,\n # --gpus-per-task=1 would isolate each task to only see its own GPU, which\n # causes a a mysterious NCCL error in\n # nn.parallel.DistributedDataParallel:\n # ncclUnhandledCudaError: Call to CUDA function failed.\n # when NCCL tries to communicate to local GPUs via shared memory but fails due\n # to cgroups isolation. See https://slurm.schedmd.com/srun.html#OPT_gres-flags\n # and https://support.schedmd.com/show_bug.cgi?id=17875 for details.\n srun --gres-flags=allow-task-sharing uv run python main.py\n</code></pre> <p>pyproject.toml</p> <pre><code>[project]\nname = \"multi-node-example\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11,&lt;3.14\"\ndependencies = [\n    \"rich&gt;=14.0.0\",\n    \"torch&gt;=2.7.1\",\n    \"torchvision&gt;=0.22.1\",\n    \"tqdm&gt;=4.67.1\",\n]\n</code></pre> <p>main.py</p> <pre><code> # distributed/multi_gpu/main.py -&gt; distributed/multi_node/main.py\n \"\"\"Multi-GPU Training example.\"\"\"\n\n import argparse\n import logging\n import os\n import random\n import sys\n+from datetime import timedelta\n from pathlib import Path\n\n import numpy as np\n import rich.logging\n import torch\n import torch.distributed\n from torch import Tensor, nn\n from torch.distributed import ReduceOp\n from torch.nn import functional as F\n from torch.utils.data import DataLoader, random_split\n from torch.utils.data.distributed import DistributedSampler\n from torchvision import transforms\n from torchvision.datasets import CIFAR10\n from torchvision.models import resnet18\n from tqdm import tqdm\n\n\n # To make your code as much reproducible as possible, uncomment the following\n # block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # torch.use_deterministic_algorithms(True)\n ## === Reproducibility (END) ===\n\n\n def main():\n     # Use an argument parser so we can pass hyperparameters from the command line.\n     parser = argparse.ArgumentParser(description=__doc__)\n     parser.add_argument(\"--epochs\", type=int, default=10)\n     parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n     parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n     parser.add_argument(\"--batch-size\", type=int, default=128)\n     parser.add_argument(\"--seed\", type=int, default=42)\n     args = parser.parse_args()\n\n     epochs: int = args.epochs\n     learning_rate: float = args.learning_rate\n     weight_decay: float = args.weight_decay\n     # NOTE: This is the \"local\" batch size, per-GPU.\n     batch_size: int = args.batch_size\n     seed: int = args.seed\n\n     # Seed the random number generators as early as possible for reproducibility\n     random.seed(seed)\n     np.random.seed(seed)\n     torch.random.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n\n     # Check that the GPU is available\n     assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n-    rank, world_size = setup()\n+    rank, world_size, local_rank = setup()\n     is_master = rank == 0\n+    is_local_master = local_rank == 0\n     # When using --gpus-per-task=1, SLURM sets CUDA_VISIBLE_DEVICES so each process\n     # only sees one GPU (index 0). Use device 0 directly.\n     device = torch.device(\"cuda\", 0)\n\n     # Setup logging (optional, but much better than using print statements)\n     # Uses the `rich` package to make logs pretty.\n     logging.basicConfig(\n         level=logging.INFO,\n         format=f\"[{rank}/{world_size}] %(name)s - %(message)s \",\n         handlers=[\n             rich.logging.RichHandler(\n                 markup=True,\n                 console=rich.console.Console(\n                     # Allower wider log lines in sbatch output files than on the terminal.\n                     width=120 if not sys.stdout.isatty() else None\n                 ),\n             )\n         ],\n     )\n\n     logger = logging.getLogger(__name__)\n-    logger.info(f\"World size: {world_size}, global rank: {rank}\")\n+    logger.info(\n+        f\"World size: {world_size}, global rank: {rank}, local rank: {local_rank}\"\n+    )\n\n     # Create a model and move it to the GPU.\n     model = resnet18(num_classes=10)\n     model.to(device=device)\n\n     # Wrap the model with DistributedDataParallel\n     # (See https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel)\n     # When using --gpus-per-task=1, each process only sees one GPU (index 0).\n     model = nn.parallel.DistributedDataParallel(\n         model, device_ids=[0], output_device=0\n     )\n\n     optimizer = torch.optim.AdamW(\n         model.parameters(), lr=learning_rate, weight_decay=weight_decay\n     )\n\n     # Setup CIFAR10\n     num_workers = get_num_workers()\n+\n     dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n     train_dataset, valid_dataset, test_dataset = make_datasets(\n-        str(dataset_path), is_master=is_master\n+        str(dataset_path), is_master=is_local_master\n     )\n\n     # Restricts data loading to a subset of the dataset exclusive to the current process\n     train_sampler = DistributedSampler(dataset=train_dataset, shuffle=True)\n     valid_sampler = DistributedSampler(dataset=valid_dataset, shuffle=False)\n     test_sampler = DistributedSampler(dataset=test_dataset, shuffle=False)\n\n     # NOTE: Here `batch_size` is still the \"local\" (per-gpu) batch size.\n     # This way, the effective batch size scales directly with number of GPUs, no need to specify it\n     # in advance. You might want to adjust the learning rate and other hyper-parameters though.\n     if is_master:\n         logger.info(f\"Effective batch size: {batch_size * world_size}\")\n     train_dataloader = DataLoader(\n         train_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,  # shuffling is now done in the sampler, not the dataloader.\n         sampler=train_sampler,\n     )\n     valid_dataloader = DataLoader(\n         valid_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n         sampler=valid_sampler,\n     )\n     _test_dataloader = DataLoader(  # NOTE: Not used in this example.\n         test_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n         sampler=test_sampler,\n     )\n\n     # Checkout the \"checkpointing and preemption\" example for more info!\n     logger.debug(\"Starting training from scratch.\")\n\n     for epoch in range(epochs):\n         logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n         # NOTE: Here we need to call `set_epoch` so the ordering is able to change at each epoch.\n         train_sampler.set_epoch(epoch)\n\n         # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)\n         model.train()\n\n         # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n         progress_bar = tqdm(\n             total=len(train_dataloader),\n             desc=f\"Train epoch {epoch}\",\n             # Disable progress bar in non-interactive environments.\n             disable=not (sys.stdout.isatty() and is_master),\n         )\n\n         # Training loop\n         for batch in train_dataloader:\n             # Move the batch to the GPU before we pass it to the model\n             batch = tuple(item.to(device) for item in batch)\n             x, y = batch\n\n             # Forward pass\n             logits: Tensor = model(x)\n\n             local_loss = F.cross_entropy(logits, y)\n\n             optimizer.zero_grad()\n             local_loss.backward()\n             # NOTE: nn.DistributedDataParallel automatically averages the gradients across devices.\n             optimizer.step()\n\n             # Calculate some metrics:\n             # local metrics\n             local_n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n             local_n_samples = logits.shape[0]\n             local_accuracy = local_n_correct_predictions / local_n_samples\n\n             # \"global\" metrics: calculated with the results from all workers\n             # NOTE: Creating new tensors to hold the \"global\" values, but this isn't required.\n             n_correct_predictions = local_n_correct_predictions.clone()\n             # Reduce the local metrics across all workers, sending the result to rank 0.\n             torch.distributed.reduce(n_correct_predictions, dst=0, op=ReduceOp.SUM)\n             # Actual (global) batch size for this step.\n             n_samples = torch.as_tensor(local_n_samples, device=device)\n             torch.distributed.reduce(n_samples, dst=0, op=ReduceOp.SUM)\n             # Will store the average loss across all workers.\n             loss = local_loss.clone()\n             torch.distributed.reduce(loss, dst=0, op=ReduceOp.SUM)\n             loss.div_(world_size)  # Report the average loss across all workers.\n\n             accuracy = n_correct_predictions / n_samples\n\n             logger.debug(f\"(local) Accuracy: {local_accuracy:.2%}\")\n             logger.debug(f\"(local) Loss: {local_loss.item()}\")\n             # NOTE: This would log the same values in all workers. Only logging on master:\n             if is_master:\n                 logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n                 logger.debug(f\"Average Loss: {loss.item()}\")\n\n             # Advance the progress bar one step and update the progress bar text.\n             progress_bar.update(1)\n             progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n         progress_bar.close()\n\n         val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)\n         # NOTE: This would log the same values in all workers. Only logging on master:\n         if is_master:\n             logger.info(\n                 f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n             )\n\n     print(\"Done!\")\n\n\n @torch.no_grad()\n def validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n     model.eval()\n\n     total_loss = torch.as_tensor(0.0, device=device)\n     n_samples = torch.as_tensor(0, device=device)\n     correct_predictions = torch.as_tensor(0, device=device)\n\n     for batch in dataloader:\n         batch = tuple(item.to(device) for item in batch)\n         x, y = batch\n\n         logits: Tensor = model(x)\n         loss = F.cross_entropy(logits, y)\n\n         batch_n_samples = x.shape[0]\n         batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n\n         total_loss += loss\n         n_samples += batch_n_samples\n         correct_predictions += batch_correct_predictions\n\n     # Sum up the metrics we gathered on each worker before returning the overall val metrics.\n     torch.distributed.all_reduce(total_loss, op=torch.distributed.ReduceOp.SUM)\n     torch.distributed.all_reduce(correct_predictions, op=torch.distributed.ReduceOp.SUM)\n     torch.distributed.all_reduce(n_samples, op=torch.distributed.ReduceOp.SUM)\n\n     accuracy = correct_predictions / n_samples\n     return total_loss, accuracy\n\n\n def setup():\n     assert torch.distributed.is_available()\n     print(\"PyTorch Distributed available.\")\n     print(\"  Backends:\")\n     print(f\"    Gloo: {torch.distributed.is_gloo_available()}\")\n     print(f\"    NCCL: {torch.distributed.is_nccl_available()}\")\n     print(f\"    MPI:  {torch.distributed.is_mpi_available()}\")\n\n+    # NOTE: the env:// init method uses FileLocks, which sometimes causes deadlocks due to the\n+    # distributed filesystem configuration on the Mila cluster.\n+    # For multi-node jobs, use the TCP init method instead.\n+    master_addr = os.environ[\"MASTER_ADDR\"]\n+    master_port = os.environ[\"MASTER_PORT\"]\n+\n+    # Default timeout is 30 minutes. Reducing the timeout here, so the job fails quicker if there's\n+    # a communication problem between nodes.\n+    timeout = timedelta(seconds=60)\n+\n     # DDP Job is being run via `srun` on a slurm cluster.\n     rank = int(os.environ[\"SLURM_PROCID\"])\n+    local_rank = int(os.environ[\"SLURM_LOCALID\"])\n     world_size = int(os.environ[\"SLURM_NTASKS\"])\n\n     # SLURM var -&gt; torch.distributed vars in case needed\n     # NOTE: Setting these values isn't exactly necessary, but some code might assume it's\n     # being run via torchrun or torch.distributed.launch, so setting these can be a good idea.\n     os.environ[\"RANK\"] = str(rank)\n+    os.environ[\"LOCAL_RANK\"] = str(local_rank)\n     os.environ[\"WORLD_SIZE\"] = str(world_size)\n\n     torch.distributed.init_process_group(\n         backend=\"nccl\",\n-        init_method=\"env://\",\n+        init_method=f\"tcp://{master_addr}:{master_port}\",\n+        timeout=timeout,\n         world_size=world_size,\n         rank=rank,\n     )\n-    return rank, world_size\n+    return rank, world_size, local_rank\n\n\n def make_datasets(\n     dataset_path: str,\n     is_master: bool,\n     val_split: float = 0.1,\n     val_split_seed: int = 42,\n ):\n     \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n     NOTE: We don't use image transforms here for simplicity.\n     Having different transformations for train and validation would complicate things a bit.\n     Later examples will show how to do the train/val/test split properly when using transforms.\n\n     NOTE: Only the master process (rank-0) downloads the dataset if necessary.\n     \"\"\"\n     # - Master: Download (if necessary) THEN Barrier\n     # - others: Barrier THEN *NO* Download\n     if not is_master:\n         # Wait for the master process to finish downloading (reach the barrier below)\n         torch.distributed.barrier()\n     train_dataset = CIFAR10(\n         root=dataset_path,\n         transform=transforms.ToTensor(),\n         download=is_master,\n         train=True,\n     )\n     test_dataset = CIFAR10(\n         root=dataset_path,\n         transform=transforms.ToTensor(),\n         download=is_master,\n         train=False,\n     )\n     if is_master:\n         # Join the workers waiting in the barrier above. They can now load the datasets from disk.\n         torch.distributed.barrier()\n     # Split the training dataset into a training and validation set.\n     n_samples = len(train_dataset)\n     n_valid = int(val_split * n_samples)\n     n_train = n_samples - n_valid\n     train_dataset, valid_dataset = random_split(\n         train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n     )\n     return train_dataset, valid_dataset, test_dataset\n\n\n def get_num_workers() -&gt; int:\n     \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n     if \"SLURM_CPUS_PER_TASK\" in os.environ:\n         return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n     if hasattr(os, \"sched_getaffinity\"):\n         return len(os.sched_getaffinity(0))\n     return torch.multiprocessing.cpu_count()\n\n\n if __name__ == \"__main__\":\n     main()\n</code></pre>"},{"location":"examples/distributed/multi_node/#running-this-example","title":"Running this example","text":"<pre><code> $ sbatch job.sh\n</code></pre>"},{"location":"examples/distributed/single_gpu/","title":"Single GPU Job","text":"<p>Prerequisites Make sure to read the following sections of the documentation before using this example:</p> <ul> <li>PyTorch setup</li> </ul> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <p>job.sh</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --gpus-per-task=l40s:1\n#SBATCH --mem-per-gpu=16G\n#SBATCH --time=00:15:00\n\n# Exit on error\nset -e\n\n# Echo time and hostname into log\necho \"Date:     $(date)\"\necho \"Hostname: $(hostname)\"\n\n# To make your code as much reproducible as possible with\n# `torch.use_deterministic_algorithms(True)`, uncomment the following block:\n## === Reproducibility ===\n## Be warned that this can make your code slower. See\n## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n## for more details.\n# export CUBLAS_WORKSPACE_CONFIG=:4096:8\n## === Reproducibility (END) ===\n\n# Stage dataset into $SLURM_TMPDIR\nmkdir -p $SLURM_TMPDIR/data\ncp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n# General-purpose alternatives combining copy and unpack:\n#     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n#     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n\n# Execute Python script\n# Use the `--offline` option of `uv run` on clusters without internet access on compute nodes.\n# Using the `--locked` option can help make your experiments easier to reproduce (it forces\n# your uv.lock file to be up to date with the dependencies declared in pyproject.toml).\nsrun uv run python main.py\n</code></pre> <p>pyproject.toml</p> <pre><code>[project]\nname = \"single-gpu-example\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11,&lt;3.14\"\ndependencies = [\n    \"rich&gt;=14.0.0\",\n    \"torch&gt;=2.7.1\",\n    \"torchvision&gt;=0.22.1\",\n    \"tqdm&gt;=4.67.1\",\n]\n</code></pre> <p>main.py</p> <pre><code>\"\"\"Single-GPU training example.\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport random\nimport sys\nfrom pathlib import Path\n\nimport numpy as np\nimport rich.logging\nimport torch\nfrom torch import Tensor, nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.models import resnet18\nfrom tqdm import tqdm\n\n\n# To make your code as much reproducible as possible, uncomment the following\n# block:\n## === Reproducibility ===\n## Be warned that this can make your code slower. See\n## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n## for more details.\n# torch.use_deterministic_algorithms(True)\n## === Reproducibility (END) ===\n\n\ndef main():\n    # Use an argument parser so we can pass hyperparameters from the command line.\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--epochs\", type=int, default=10)\n    parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n    parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n    parser.add_argument(\"--batch-size\", type=int, default=128)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    args = parser.parse_args()\n\n    epochs: int = args.epochs\n    learning_rate: float = args.learning_rate\n    weight_decay: float = args.weight_decay\n    batch_size: int = args.batch_size\n    seed: int = args.seed\n\n    # Seed the random number generators as early as possible for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.random.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n    # Check that the GPU is available\n    assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n    device = torch.device(\"cuda\", 0)\n\n    # Setup logging (optional, but much better than using print statements)\n    # Uses the `rich` package to make logs pretty.\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(message)s\",\n        handlers=[\n            rich.logging.RichHandler(\n                markup=True,\n                console=rich.console.Console(\n                    # Allower wider log lines in sbatch output files than on the terminal.\n                    width=120 if not sys.stdout.isatty() else None\n                ),\n            )\n        ],\n    )\n\n    logger = logging.getLogger(__name__)\n\n    # Create a model and move it to the GPU.\n    model = resnet18(num_classes=10)\n    model.to(device=device)\n\n    optimizer = torch.optim.AdamW(\n        model.parameters(), lr=learning_rate, weight_decay=weight_decay\n    )\n\n    # Setup CIFAR10\n    num_workers = get_num_workers()\n    dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n    train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=True,\n    )\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n    )\n    _test_dataloader = DataLoader(  # NOTE: Not used in this example.\n        test_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n    )\n\n    # Checkout the \"checkpointing and preemption\" example for more info!\n    logger.debug(\"Starting training from scratch.\")\n\n    for epoch in range(epochs):\n        logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n        # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)\n        model.train()\n\n        # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n        progress_bar = tqdm(\n            total=len(train_dataloader),\n            desc=f\"Train epoch {epoch}\",\n            disable=not sys.stdout.isatty(),  # Disable progress bar in non-interactive environments.\n        )\n\n        # Training loop\n        for batch in train_dataloader:\n            # Move the batch to the GPU before we pass it to the model\n            batch = tuple(item.to(device) for item in batch)\n            x, y = batch\n\n            # Forward pass\n            logits: Tensor = model(x)\n\n            loss = F.cross_entropy(logits, y)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Calculate some metrics:\n            n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n            n_samples = y.shape[0]\n            accuracy = n_correct_predictions / n_samples\n\n            logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n            logger.debug(f\"Average Loss: {loss.item()}\")\n\n            # Advance the progress bar one step and update the progress bar text.\n            progress_bar.update(1)\n            progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n        progress_bar.close()\n\n        val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)\n        logger.info(\n            f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n        )\n\n    print(\"Done!\")\n\n\n@torch.no_grad()\ndef validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n    model.eval()\n\n    total_loss = 0.0\n    n_samples = 0\n    correct_predictions = 0\n\n    for batch in dataloader:\n        batch = tuple(item.to(device) for item in batch)\n        x, y = batch\n\n        logits: Tensor = model(x)\n        loss = F.cross_entropy(logits, y)\n\n        batch_n_samples = x.shape[0]\n        batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n\n        total_loss += loss.item()\n        n_samples += batch_n_samples\n        correct_predictions += batch_correct_predictions\n\n    accuracy = correct_predictions / n_samples\n    return total_loss, accuracy\n\n\ndef make_datasets(\n    dataset_path: str,\n    val_split: float = 0.1,\n    val_split_seed: int = 42,\n):\n    \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n    NOTE: We don't use image transforms here for simplicity.\n    Having different transformations for train and validation would complicate things a bit.\n    Later examples will show how to do the train/val/test split properly when using transforms.\n    \"\"\"\n    train_dataset = CIFAR10(\n        root=dataset_path, transform=transforms.ToTensor(), download=True, train=True\n    )\n    test_dataset = CIFAR10(\n        root=dataset_path, transform=transforms.ToTensor(), download=True, train=False\n    )\n    # Split the training dataset into a training and validation set.\n    n_samples = len(train_dataset)\n    n_valid = int(val_split * n_samples)\n    n_train = n_samples - n_valid\n    train_dataset, valid_dataset = random_split(\n        train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n    )\n    return train_dataset, valid_dataset, test_dataset\n\n\ndef get_num_workers() -&gt; int:\n    \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n    if \"SLURM_CPUS_PER_TASK\" in os.environ:\n        return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n    if hasattr(os, \"sched_getaffinity\"):\n        return len(os.sched_getaffinity(0))\n    return torch.multiprocessing.cpu_count()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/distributed/single_gpu/#running-this-example","title":"Running this example","text":"<pre><code> $ sbatch job.sh\n</code></pre>"},{"location":"examples/frameworks/","title":"Software Frameworks","text":"About these examples <p>This section contains some minimal examples of how to run jobs on the Mila cluster. Each example is self-contained and can be run as-is directly on the cluster without error.  Each example has the following structure:</p> <ul> <li><code>job.sh</code>: SLURM <code>sbatch</code> script. Can be launched with <code>sbatch job.sh</code>.</li> <li><code>main.py</code>: Example python script.</li> </ul> <p>Some examples are displayed as a difference with respect to a \"base\" example. For instance, the multi-gpu example is shown as a difference with respect to the single-gpu example.</p> <p>This section shows how to setup a development environment for various frameworks. These examples use uv to create a virtual environment and manage dependencies.</p> <ul> <li>PyTorch Setup</li> <li>Jax Setup</li> <li> <p>Jax</p> </li> <li> <p>TensorFlow (Coming soon!)</p> </li> </ul>"},{"location":"examples/frameworks/jax/","title":"Jax","text":"<p>Prerequisites</p> <p>Make sure to read the following sections of the documentation before using this example:</p> <ul> <li>JAX setup</li> <li>Single GPU</li> </ul> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <p>job.sh</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --gpus-per-task=l40s:1\n#SBATCH --mem-per-gpu=16G\n#SBATCH --time=00:15:00\n\n# Exit on error\nset -e\n\n# Echo time and hostname into log\necho \"Date:     $(date)\"\necho \"Hostname: $(hostname)\"\n\n# To make your code as much reproducible as possible, uncomment the following\n# block:\n## === Reproducibility ===\n## BROKEN: This seams to block the training and nothing happens.\n## Be warned that this can make your code slower. See\n## https://github.com/jax-ml/jax/issues/13672 for more details.\n## export XLA_FLAGS=--xla_gpu_deterministic_ops=true\n## === Reproducibility (END) ===\n\n# Stage dataset into $SLURM_TMPDIR\nmkdir -p $SLURM_TMPDIR/data\ncp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n# General-purpose alternatives combining copy and unpack:\n#     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n#     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n\n# Execute Python script\n# Use `uv run --offline` on clusters without internet access on compute nodes.\nsrun uv run python main.py\n</code></pre> <p>main.py</p> <pre><code>\"\"\"Single-GPU training example.\n\nThis Jax example is heavily based on the following examples:\n\n* https://juliusruseckas.github.io/ml/flax-cifar10.html\n* https://github.com/fattorib/Flax-ResNets/blob/master/main_flax.py\n\"\"\"\n\nimport argparse\nimport logging\nimport math\nimport os\nfrom pathlib import Path\nimport random\nimport sys\nfrom typing import Any, Sequence\n\nimport PIL.Image\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport optax\nimport rich.logging\nimport torch\n\nfrom flax.training import train_state, common_utils\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import CIFAR10\nfrom tqdm import tqdm\n\nfrom model import ResNet\n\n\nclass TrainState(train_state.TrainState):\n    batch_stats: Any\n\n\nclass ToArray(torch.nn.Module):\n    \"\"\"convert image to float and 0-1 range\"\"\"\n\n    dtype = np.float32\n\n    def __call__(self, x):\n        assert isinstance(x, PIL.Image.Image)\n        x = np.asarray(x, dtype=self.dtype)\n        x /= 255.0\n        return x\n\n\ndef numpy_collate(batch: Sequence):\n    if isinstance(batch[0], np.ndarray):\n        return np.stack(batch)\n    elif isinstance(batch[0], (tuple, list)):\n        transposed = zip(*batch)\n        return [numpy_collate(samples) for samples in transposed]\n    else:\n        return np.array(batch)\n\n\ndef main():\n    # Use an argument parser so we can pass hyperparameters from the command line.\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"--epochs\", type=int, default=10)\n    parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n    parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n    parser.add_argument(\"--batch-size\", type=int, default=128)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    args = parser.parse_args()\n\n    epochs: int = args.epochs\n    learning_rate: float = args.learning_rate\n    weight_decay: float = args.weight_decay\n    # NOTE: This is the \"local\" batch size, per-GPU.\n    batch_size: int = args.batch_size\n    seed: int = args.seed\n\n    # Check that the GPU is available\n    assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n\n    # Seed the random number generators as early as possible for reproducibility\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.random.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    rng = jax.random.PRNGKey(seed)\n\n    # Setup logging (optional, but much better than using print statements)\n    # Uses the `rich` package to make logs pretty.\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(message)s\",\n        handlers=[\n            rich.logging.RichHandler(\n                markup=True,\n                console=rich.console.Console(\n                    # Allower wider log lines in sbatch output files than on the terminal.\n                    width=120 if not sys.stdout.isatty() else None\n                ),\n            )\n        ],\n    )\n\n    logger = logging.getLogger(__name__)\n\n    # Create a model.\n    model = ResNet(\n        10,\n        channel_list=[64, 128, 256, 512],\n        num_blocks_list=[2, 2, 2, 2],\n        strides=[1, 1, 2, 2, 2],\n        head_p_drop=0.3,\n    )\n\n    @jax.jit\n    def initialize(params_rng, image_size=32):\n        init_rngs = {\"params\": params_rng}\n        input_shape = (1, image_size, image_size, 3)\n        variables = model.init(\n            init_rngs, jnp.ones(input_shape, jnp.float32), train=False\n        )\n        return variables\n\n    # Setup CIFAR10\n    num_workers = get_num_workers()\n    dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n    train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=True,\n        collate_fn=numpy_collate,\n    )\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=numpy_collate,\n    )\n    test_dataloader = DataLoader(  # NOTE: Not used in this example.\n        test_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=numpy_collate,\n    )\n\n    train_steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n    num_train_steps = train_steps_per_epoch * epochs\n    shedule_fn = optax.cosine_onecycle_schedule(\n        transition_steps=num_train_steps, peak_value=learning_rate\n    )\n    optimizer = optax.adamw(learning_rate=shedule_fn, weight_decay=weight_decay)\n\n    params_rng, dropout_rng = jax.random.split(rng)\n    variables = initialize(params_rng)\n\n    state = TrainState.create(\n        apply_fn=model.apply,\n        params=variables[\"params\"],\n        batch_stats=variables[\"batch_stats\"],\n        tx=optimizer,\n    )\n\n    # Checkout the \"checkpointing and preemption\" example for more info!\n    logger.debug(\"Starting training from scratch.\")\n\n    for epoch in range(epochs):\n        logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n        # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n        progress_bar = tqdm(\n            total=len(train_dataloader),\n            desc=f\"Train epoch {epoch}\",\n            disable=not sys.stdout.isatty(),  # Disable progress bar in non-interactive environments.\n        )\n\n        # Training loop\n        for input, target in train_dataloader:\n            batch = {\n                \"image\": input,\n                \"label\": target,\n            }\n            state, loss, accuracy = train_step(state, batch, dropout_rng)\n\n            logger.debug(f\"Accuracy: {accuracy:.2%}\")\n            logger.debug(f\"Average Loss: {loss}\")\n\n            # Advance the progress bar one step, and update the \"postfix\" () the progress bar. (nicer than just)\n            progress_bar.update(1)\n            progress_bar.set_postfix(loss=loss, accuracy=accuracy)\n        progress_bar.close()\n\n        val_loss, val_accuracy = validation_loop(state, valid_dataloader)\n        logger.info(\n            f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n        )\n\n    print(\"Done!\")\n\n\ndef cross_entropy_loss(logits, labels, num_classes=10):\n    one_hot_labels = common_utils.onehot(labels, num_classes=num_classes)\n    loss = optax.softmax_cross_entropy(logits=logits, labels=one_hot_labels)\n    loss = jnp.mean(loss)\n    return loss\n\n\n@jax.jit\ndef train_step(state, batch, dropout_rng):\n    dropout_rng = jax.random.fold_in(dropout_rng, state.step)\n\n    def loss_fn(params):\n        variables = {\"params\": params, \"batch_stats\": state.batch_stats}\n        logits, new_model_state = state.apply_fn(\n            variables,\n            batch[\"image\"],\n            train=True,\n            rngs={\"dropout\": dropout_rng},\n            mutable=\"batch_stats\",\n        )\n        loss = cross_entropy_loss(logits, batch[\"label\"])\n        accuracy = jnp.sum(jnp.argmax(logits, -1) == batch[\"label\"])\n        return loss, (accuracy, new_model_state)\n\n    (loss, (accuracy, new_model_state)), grads = jax.value_and_grad(\n        loss_fn, has_aux=True\n    )(state.params)\n    new_state = state.apply_gradients(\n        grads=grads, batch_stats=new_model_state[\"batch_stats\"]\n    )\n    return new_state, loss, accuracy\n\n\n@jax.jit\ndef validation_step(state, batch):\n    variables = {\"params\": state.params, \"batch_stats\": state.batch_stats}\n    logits = state.apply_fn(variables, batch[\"image\"], train=False, mutable=False)\n    loss = cross_entropy_loss(logits, batch[\"label\"])\n    batch_correct_predictions = jnp.sum(jnp.argmax(logits, -1) == batch[\"label\"])\n    return loss, batch_correct_predictions\n\n\n@torch.no_grad()\ndef validation_loop(state, dataloader: DataLoader):\n    losses = []\n    correct_predictions = []\n    for input, target in dataloader:\n        batch = {\n            \"image\": input,\n            \"label\": target,\n        }\n        loss, batch_correct_predictions = validation_step(state, batch)\n        losses.append(loss)\n        correct_predictions.append(batch_correct_predictions)\n\n    total_loss = np.sum(losses)\n    accuracy = np.mean(correct_predictions)\n    return total_loss, accuracy\n\n\ndef make_datasets(\n    dataset_path: str,\n    val_split: float = 0.1,\n    val_split_seed: int = 42,\n):\n    \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n    NOTE: We don't use image transforms here for simplicity.\n    Having different transformations for train and validation would complicate things a bit.\n    Later examples will show how to do the train/val/test split properly when using transforms.\n    \"\"\"\n    train_dataset = CIFAR10(\n        root=dataset_path, transform=ToArray(), download=True, train=True\n    )\n    test_dataset = CIFAR10(\n        root=dataset_path, transform=ToArray(), download=True, train=False\n    )\n    # Split the training dataset into a training and validation set.\n    n_samples = len(train_dataset)\n    n_valid = int(val_split * n_samples)\n    n_train = n_samples - n_valid\n    train_dataset, valid_dataset = random_split(\n        train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n    )\n    return train_dataset, valid_dataset, test_dataset\n\n\ndef get_num_workers() -&gt; int:\n    \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n    if \"SLURM_CPUS_PER_TASK\" in os.environ:\n        return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n    if hasattr(os, \"sched_getaffinity\"):\n        return len(os.sched_getaffinity(0))\n    return torch.multiprocessing.cpu_count()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>model.py</p> <pre><code>from functools import partial\nfrom typing import Any, Sequence\n\nimport jax.numpy as jnp\n\nfrom flax import linen as nn\n\n\nModuleDef = Any\n\n\nclass ConvBlock(nn.Module):\n    channels: int\n    kernel_size: int\n    norm: ModuleDef\n    stride: int = 1\n    act: bool = True\n\n    @nn.compact\n    def __call__(self, x):\n        x = nn.Conv(\n            self.channels,\n            (self.kernel_size, self.kernel_size),\n            strides=self.stride,\n            padding=\"SAME\",\n            use_bias=False,\n            kernel_init=nn.initializers.kaiming_normal(),\n        )(x)\n        x = self.norm()(x)\n        if self.act:\n            x = nn.swish(x)\n        return x\n\n\nclass ResidualBlock(nn.Module):\n    channels: int\n    conv_block: ModuleDef\n\n    @nn.compact\n    def __call__(self, x):\n        channels = self.channels\n        conv_block = self.conv_block\n\n        shortcut = x\n\n        residual = conv_block(channels, 3)(x)\n        residual = conv_block(channels, 3, act=False)(residual)\n\n        if shortcut.shape != residual.shape:\n            shortcut = conv_block(channels, 1, act=False)(shortcut)\n\n        gamma = self.param(\"gamma\", nn.initializers.zeros, 1, jnp.float32)\n        out = shortcut + gamma * residual\n        out = nn.swish(out)\n        return out\n\n\nclass Stage(nn.Module):\n    channels: int\n    num_blocks: int\n    stride: int\n    block: ModuleDef\n\n    @nn.compact\n    def __call__(self, x):\n        stride = self.stride\n        if stride &gt; 1:\n            x = nn.max_pool(x, (stride, stride), strides=(stride, stride))\n        for _ in range(self.num_blocks):\n            x = self.block(self.channels)(x)\n        return x\n\n\nclass Body(nn.Module):\n    channel_list: Sequence[int]\n    num_blocks_list: Sequence[int]\n    strides: Sequence[int]\n    stage: ModuleDef\n\n    @nn.compact\n    def __call__(self, x):\n        for channels, num_blocks, stride in zip(\n            self.channel_list, self.num_blocks_list, self.strides\n        ):\n            x = self.stage(channels, num_blocks, stride)(x)\n        return x\n\n\nclass Stem(nn.Module):\n    channel_list: Sequence[int]\n    stride: int\n    conv_block: ModuleDef\n\n    @nn.compact\n    def __call__(self, x):\n        stride = self.stride\n        for channels in self.channel_list:\n            x = self.conv_block(channels, 3, stride=stride)(x)\n            stride = 1\n        return x\n\n\nclass Head(nn.Module):\n    classes: int\n    dropout: ModuleDef\n\n    @nn.compact\n    def __call__(self, x):\n        x = jnp.mean(x, axis=(1, 2))\n        x = self.dropout()(x)\n        x = nn.Dense(self.classes)(x)\n        return x\n\n\nclass ResNet(nn.Module):\n    classes: int\n    channel_list: Sequence[int]\n    num_blocks_list: Sequence[int]\n    strides: Sequence[int]\n    head_p_drop: float = 0.0\n\n    @nn.compact\n    def __call__(self, x, train=True):\n        norm = partial(nn.BatchNorm, use_running_average=not train)\n        dropout = partial(nn.Dropout, rate=self.head_p_drop, deterministic=not train)\n        conv_block = partial(ConvBlock, norm=norm)\n        residual_block = partial(ResidualBlock, conv_block=conv_block)\n        stage = partial(Stage, block=residual_block)\n\n        x = Stem([32, 32, 64], self.strides[0], conv_block)(x)\n        x = Body(self.channel_list, self.num_blocks_list, self.strides[1:], stage)(x)\n        x = Head(self.classes, dropout)(x)\n        return x\n</code></pre>"},{"location":"examples/frameworks/jax/#running-this-example","title":"Running this example","text":"<pre><code>$ sbatch job.sh\n</code></pre>"},{"location":"examples/frameworks/jax_setup/","title":"Jax Setup","text":"<p>Prerequisites: (Make sure to read the following before using this example!)</p> <ul> <li>Quick Start</li> <li>Running your code</li> <li>uv</li> </ul> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <p>job.sh</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=1\n#SBATCH --gpus-per-task=l40s:1\n#SBATCH --mem-per-gpu=16G\n#SBATCH --time=00:15:00\n\n# Exit on error\nset -e\n\n# Echo time and hostname into log\necho \"Date:     $(date)\"\necho \"Hostname: $(hostname)\"\n\n# Execute Python script\n# Use `uv run --offline` on clusters without internet access on compute nodes.\nuv run python main.py\n</code></pre> <p>pyproject.toml</p> <pre><code>[project]\nname = \"jax-setup\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11,&lt;3.14\"\ndependencies = [\"jax[cuda12]&gt;=0.5.3\"]\n</code></pre> <p>main.py</p> <pre><code>import jax\nimport jax.extend.backend\n\n\ndef main():\n    device_count = len(jax.local_devices(backend=\"gpu\"))\n    print(f\"Jax default backend:         {jax.extend.backend.get_backend().platform}\")\n    print(f\"Jax-detected #GPUs:          {device_count}\")\n\n    if device_count == 0:\n        print(\"    No GPU detected, not printing device names.\")\n    else:\n        for i, device in enumerate(jax.local_devices(backend=\"gpu\")):\n            print(f\"    GPU {i}:      {device.device_kind}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/frameworks/jax_setup/#running-this-example","title":"Running this example","text":"<p>This assumes that you already installed UV on the cluster you are working on.</p> <p>To create this environment, we first request resources for an interactive job. Note that we are requesting a GPU for this job, even though we're only going to install packages. This is because we want PyTorch to be installed with GPU support, and to have all the required libraries.</p> <pre><code> # On the Mila cluster: (on DRAC/PAICE, run `uv sync` on a login node)\n $ salloc --gres=gpu:1 --cpus-per-task=4 --mem=16G --time=00:10:00\n salloc: --------------------------------------------------------------------------------------------------\n salloc: # Using default long partition\n salloc: --------------------------------------------------------------------------------------------------\n salloc: Pending job allocation 2959785\n salloc: job 2959785 queued and waiting for resources\n salloc: job 2959785 has been allocated resources\n salloc: Granted job allocation 2959785\n salloc: Waiting for resource configuration\n salloc: Nodes cn-g022 are ready for job\n $ # Create the virtual environment and install all dependencies\n $ uv sync\n (...)\n $ # Optional: Activate the environment and run the python script:\n $ . .venv/bin/activate\n $ python main.py\n</code></pre> <p>You can exit the interactive job once the environment has been created. Then, you can submit a job to run the example with sbatch:</p> <pre><code> $ sbatch job.sh\n</code></pre>"},{"location":"examples/frameworks/pytorch_setup/","title":"PyTorch Setup","text":"<p>Prerequisites: (Make sure to read the following before using this example!)</p> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <ul> <li>Quick Start</li> <li>Running your code</li> <li>uv</li> </ul> <p>job.sh</p> <pre><code>#!/bin/bash\n#SBATCH --ntasks=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=1\n#SBATCH --gpus-per-task=l40s:1\n#SBATCH --mem-per-gpu=16G\n#SBATCH --time=00:15:00\n\n# Exit on error\nset -e\n\n# Echo time and hostname into log\necho \"Date:     $(date)\"\necho \"Hostname: $(hostname)\"\n\n# Execute Python script\n# Use `uv run --offline` on clusters without internet access on compute nodes.\nuv run python main.py\n</code></pre> <p>pyproject.toml</p> <pre><code>[project]\nname = \"pytorch-setup\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11,&lt;3.14\"\ndependencies = [\"torch&gt;=2.7.1\"]\n</code></pre> <p>main.py</p> <pre><code>import torch\nimport torch.backends.cuda\n\n\ndef main():\n    cuda_built = torch.backends.cuda.is_built()\n    cuda_avail = torch.cuda.is_available()\n    device_count = torch.cuda.device_count()\n\n    print(f\"PyTorch built with CUDA:         {cuda_built}\")\n    print(f\"PyTorch detects CUDA available:  {cuda_avail}\")\n    print(f\"PyTorch-detected #GPUs:          {device_count}\")\n    if device_count == 0:\n        print(\"    No GPU detected, not printing devices' names.\")\n    else:\n        for i in range(device_count):\n            print(f\"    GPU {i}:      {torch.cuda.get_device_name(i)}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/frameworks/pytorch_setup/#running-this-example","title":"Running this example","text":"<p>This assumes that you already installed UV on the cluster you are working on.</p> <p>To create this environment, we first request resources for an interactive job. Note that we are requesting a GPU for this job, even though we're only going to install packages. This is because we want PyTorch to be installed with GPU support, and to have all the required libraries.</p> <pre><code> # On the Mila cluster: (on DRAC/PAICE, run `uv sync` on a login node)\n $ salloc --gres=gpu:1 --cpus-per-task=4 --mem=16G --time=00:10:00\n salloc: --------------------------------------------------------------------------------------------------\n salloc: # Using default long partition\n salloc: --------------------------------------------------------------------------------------------------\n salloc: Pending job allocation 2959785\n salloc: job 2959785 queued and waiting for resources\n salloc: job 2959785 has been allocated resources\n salloc: Granted job allocation 2959785\n salloc: Waiting for resource configuration\n salloc: Nodes cn-g022 are ready for job\n $ # Create the virtual environment and install all dependencies\n $ uv sync\n (...)\n $ # Optional: Activate the environment and run the python script:\n $ . .venv/bin/activate\n $ python main.py\n</code></pre> <p>You can exit the interactive job once the environment has been created. Then, you can submit a job to run the example with sbatch:</p> <pre><code> $ sbatch job.sh\n</code></pre>"},{"location":"examples/good_practices/","title":"Good practices","text":"About these examples <p>This section contains some minimal examples of how to run jobs on the Mila cluster. Each example is self-contained and can be run as-is directly on the cluster without error.  Each example has the following structure:</p> <ul> <li><code>job.sh</code>: SLURM <code>sbatch</code> script. Can be launched with <code>sbatch job.sh</code>.</li> <li><code>main.py</code>: Example python script.</li> </ul> <p>Some examples are displayed as a difference with respect to a \"base\" example. For instance, the multi-gpu example is shown as a difference with respect to the single-gpu example.</p> <p>Work-in-progress: This section will contain minimal examples illustrating various good practices that should be observed when using the Mila cluster.</p> <ul> <li>Checkpointing</li> <li>Weights &amp; Biases (wandb) setup</li> <li>Launch many jobs from same shell script</li> <li>Hyperparameter Optimization with Orion</li> <li>Launch many tasks on the same GPU</li> <li>Launch many jobs using SLURM job arrays</li> </ul>"},{"location":"examples/good_practices/checkpointing/","title":"Checkpointing","text":"<p>Prerequisites</p> <p>Make sure to read the following sections of the documentation before using this example:</p> <ul> <li>PyTorch Setup</li> <li>Single GPU</li> </ul> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <p>job.sh</p> <pre><code> # distributed/single_gpu/job.sh -&gt; good_practices/checkpointing/job.sh\n #!/bin/bash\n #SBATCH --ntasks=1\n #SBATCH --ntasks-per-node=1\n #SBATCH --cpus-per-task=4\n #SBATCH --gpus-per-task=l40s:1\n #SBATCH --mem-per-gpu=16G\n #SBATCH --time=00:15:00\n+#SBATCH --requeue\n+#SBATCH --signal=B:TERM@300 # tells the controller to send SIGTERM to the job 5\n+                            # min before its time ends to give it a chance for\n+                            # better cleanup. If you cancel the job manually,\n+                            # make sure that you specify the signal as TERM like\n+                            # so `scancel --signal=TERM &lt;jobid&gt;`.\n+                            # https://dhruveshp.com/blog/2021/signal-propagation-on-slurm/\n\n # Exit on error\n set -e\n\n # Echo time and hostname into log\n echo \"Date:     $(date)\"\n echo \"Hostname: $(hostname)\"\n+echo \"Job has been preempted $SLURM_RESTART_COUNT times.\"\n\n # To make your code as much reproducible as possible with\n # `torch.use_deterministic_algorithms(True)`, uncomment the following block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # export CUBLAS_WORKSPACE_CONFIG=:4096:8\n ## === Reproducibility (END) ===\n\n # Stage dataset into $SLURM_TMPDIR\n mkdir -p $SLURM_TMPDIR/data\n cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n # General-purpose alternatives combining copy and unpack:\n #     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n #     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n\n # Execute Python script\n # Use the `--offline` option of `uv run` on clusters without internet access on compute nodes.\n # Using the `--locked` option can help make your experiments easier to reproduce (it forces\n # your uv.lock file to be up to date with the dependencies declared in pyproject.toml).\n-srun uv run python main.py\n+# Here we use `exec` to ensure that the signals are received and handled in the Python process.\n+exec srun uv run python main.py\n</code></pre> <p>pyproject.toml</p> <pre><code>[project]\nname = \"checkpointing-example\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11,&lt;3.14\"\ndependencies = [\n    \"rich&gt;=14.0.0\",\n    \"torch&gt;=2.7.1\",\n    \"torchvision&gt;=0.22.1\",\n    \"tqdm&gt;=4.67.1\",\n]\n</code></pre> <p>main.py</p> <pre><code> # distributed/single_gpu/main.py -&gt; good_practices/checkpointing/main.py\n-\"\"\"Single-GPU training example.\"\"\"\n+\"\"\"Checkpointing example.\"\"\"\n+\n+from __future__ import annotations\n\n import argparse\n import logging\n import os\n import random\n+import shutil\n+import signal\n import sys\n+import uuid\n+import warnings\n+from logging import getLogger as get_logger\n from pathlib import Path\n+from types import FrameType\n+from typing import Any, TypedDict\n\n import numpy as np\n import rich.logging\n import torch\n from torch import Tensor, nn\n from torch.nn import functional as F\n from torch.utils.data import DataLoader, random_split\n from torchvision import transforms\n from torchvision.datasets import CIFAR10\n from torchvision.models import resnet18\n from tqdm import tqdm\n\n+SCRATCH = Path(os.environ[\"SCRATCH\"])\n+SLURM_TMPDIR = Path(os.environ[\"SLURM_TMPDIR\"])\n+SLURM_JOBID = os.environ[\"SLURM_JOBID\"]\n+\n+CHECKPOINT_FILE_NAME = \"checkpoint.pth\"\n+\n\n # To make your code as much reproducible as possible, uncomment the following\n # block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # torch.use_deterministic_algorithms(True)\n ## === Reproducibility (END) ===\n\n\n+class RunState(TypedDict):\n+    \"\"\"Typed dictionary containing the state of the training run which is saved at each epoch.\n+\n+    Using type hints helps prevent bugs and makes your code easier to read for both humans and\n+    machines (e.g. Copilot). This leads to less time spent debugging and better code suggestions.\n+    \"\"\"\n+\n+    epoch: int\n+    best_acc: float\n+    model_state: dict[str, Tensor]\n+    optimizer_state: dict[str, Tensor]\n+\n+    random_state: tuple[Any, ...]\n+    numpy_random_state: dict[str, Any]\n+    torch_random_state: Tensor\n+    torch_cuda_random_state: list[Tensor]\n+\n+\n def main():\n     # Use an argument parser so we can pass hyperparameters from the command line.\n     parser = argparse.ArgumentParser(description=__doc__)\n     parser.add_argument(\"--epochs\", type=int, default=10)\n     parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n     parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n     parser.add_argument(\"--batch-size\", type=int, default=128)\n+    parser.add_argument(\n+        \"--run-dir\", type=Path, default=SCRATCH / \"checkpointing_example\" / SLURM_JOBID\n+    )\n     parser.add_argument(\"--seed\", type=int, default=42)\n     args = parser.parse_args()\n\n     epochs: int = args.epochs\n     learning_rate: float = args.learning_rate\n     weight_decay: float = args.weight_decay\n     batch_size: int = args.batch_size\n+    run_dir: Path = args.run_dir\n     seed: int = args.seed\n\n+    checkpoint_dir = run_dir / \"checkpoints\"\n+    start_epoch: int = 0\n+    best_acc: float = 0.0\n+\n     # Seed the random number generators as early as possible for reproducibility\n     random.seed(seed)\n     np.random.seed(seed)\n     torch.random.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n\n     # Check that the GPU is available\n     assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n     device = torch.device(\"cuda\", 0)\n\n     # Setup logging (optional, but much better than using print statements)\n     # Uses the `rich` package to make logs pretty.\n     logging.basicConfig(\n         level=logging.INFO,\n         format=\"%(message)s\",\n         handlers=[\n             rich.logging.RichHandler(\n                 markup=True,\n                 console=rich.console.Console(\n                     # Allower wider log lines in sbatch output files than on the terminal.\n                     width=120 if not sys.stdout.isatty() else None\n                 ),\n             )\n         ],\n     )\n\n     logger = logging.getLogger(__name__)\n\n-    # Create a model and move it to the GPU.\n+    # Create a model.\n     model = resnet18(num_classes=10)\n+\n+    # Move the model to the GPU.\n     model.to(device=device)\n\n     optimizer = torch.optim.AdamW(\n         model.parameters(), lr=learning_rate, weight_decay=weight_decay\n     )\n\n-    # Setup CIFAR10\n+    # Try to resume from a checkpoint, if one exists.\n+    checkpoint: RunState | None = load_checkpoint(checkpoint_dir, map_location=device)\n+    if checkpoint:\n+        start_epoch = checkpoint[\"epoch\"] + 1  # +1 to start at the next epoch.\n+        best_acc = checkpoint[\"best_acc\"]\n+        model.load_state_dict(checkpoint[\"model_state\"])\n+        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n+        random.setstate(checkpoint[\"random_state\"])\n+        np.random.set_state(checkpoint[\"numpy_random_state\"])\n+        # NOTE: Need to move those tensors to CPU before they can be loaded.\n+        torch.random.set_rng_state(checkpoint[\"torch_random_state\"].cpu())\n+        torch.cuda.random.set_rng_state_all(\n+            t.cpu() for t in checkpoint[\"torch_cuda_random_state\"]\n+        )\n+        logger.info(\n+            f\"Resuming training at epoch {start_epoch} (best_acc={best_acc:.2%}).\"\n+        )\n+    else:\n+        logger.info(f\"No checkpoints found in {checkpoint_dir}. Training from scratch.\")\n+\n+    # Setup the dataset\n     num_workers = get_num_workers()\n-    dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n+    dataset_path = (SLURM_TMPDIR or Path(\"..\")) / \"data\"\n+\n     train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))\n     train_dataloader = DataLoader(\n         train_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=True,\n+        # generator=torch.Generator().manual_seed(seed),\n     )\n     valid_dataloader = DataLoader(\n         valid_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n+        # generator=torch.Generator().manual_seed(seed),\n     )\n     _test_dataloader = DataLoader(  # NOTE: Not used in this example.\n         test_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n\n-    # Checkout the \"checkpointing and preemption\" example for more info!\n-    logger.debug(\"Starting training from scratch.\")\n-\n-    for epoch in range(epochs):\n+    def signal_handler(signum: int, frame: FrameType | None):\n+        \"\"\"Called before the job gets pre-empted or reaches the time-limit.\n+\n+        This should run quickly. Performing a full checkpoint here mid-epoch is not recommended.\n+        \"\"\"\n+        signal_enum = signal.Signals(signum)\n+        logger.error(f\"Job received a {signal_enum.name} signal!\")\n+        # Perform quick actions that will help the job resume later.\n+        # If you use Weights &amp; Biases: https://docs.wandb.ai/guides/runs/resuming#preemptible-sweeps\n+        # if wandb.run:\n+        #     wandb.mark_preempting()\n+\n+    signal.signal(\n+        signal.SIGTERM, signal_handler\n+    )  # Before getting pre-empted and requeued.\n+    signal.signal(\n+        signal.SIGUSR1, signal_handler\n+    )  # Before reaching the end of the time limit.\n+\n+    for epoch in range(start_epoch, epochs):\n         logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n-        # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)\n+        # Set the model in training mode (this is important for e.g. BatchNorm and Dropout layers)\n         model.train()\n\n-        # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n+        # NOTE: using a progress bar from tqdm much nicer than using `print`s).\n         progress_bar = tqdm(\n             total=len(train_dataloader),\n             desc=f\"Train epoch {epoch}\",\n             disable=not sys.stdout.isatty(),  # Disable progress bar in non-interactive environments.\n         )\n\n         # Training loop\n+        batch: tuple[Tensor, Tensor]\n         for batch in train_dataloader:\n             # Move the batch to the GPU before we pass it to the model\n             batch = tuple(item.to(device) for item in batch)\n             x, y = batch\n\n             # Forward pass\n             logits: Tensor = model(x)\n\n             loss = F.cross_entropy(logits, y)\n\n             optimizer.zero_grad()\n             loss.backward()\n             optimizer.step()\n\n             # Calculate some metrics:\n             n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n             n_samples = y.shape[0]\n             accuracy = n_correct_predictions / n_samples\n\n             logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n             logger.debug(f\"Average Loss: {loss.item()}\")\n\n-            # Advance the progress bar one step and update the progress bar text.\n+            # Advance the progress bar one step, and update the text displayed in the progress bar.\n             progress_bar.update(1)\n             progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n         progress_bar.close()\n\n         val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)\n         logger.info(\n             f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n         )\n\n+        # remember best accuracy and save the current state.\n+        is_best = val_accuracy &gt; best_acc\n+        best_acc = max(val_accuracy, best_acc)\n+\n+        if checkpoint_dir is not None:\n+            save_checkpoint(\n+                checkpoint_dir,\n+                is_best,\n+                RunState(\n+                    epoch=epoch,\n+                    model_state=model.state_dict(),\n+                    optimizer_state=optimizer.state_dict(),\n+                    random_state=random.getstate(),\n+                    numpy_random_state=np.random.get_state(legacy=False),\n+                    torch_random_state=torch.random.get_rng_state(),\n+                    torch_cuda_random_state=torch.cuda.random.get_rng_state_all(),\n+                    best_acc=best_acc,\n+                ),\n+            )\n+\n     print(\"Done!\")\n\n\n @torch.no_grad()\n def validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n     model.eval()\n\n     total_loss = 0.0\n     n_samples = 0\n     correct_predictions = 0\n\n     for batch in dataloader:\n         batch = tuple(item.to(device) for item in batch)\n         x, y = batch\n\n         logits: Tensor = model(x)\n         loss = F.cross_entropy(logits, y)\n\n         batch_n_samples = x.shape[0]\n-        batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n+        batch_correct_predictions = logits.argmax(-1).eq(y).sum().item()\n\n         total_loss += loss.item()\n         n_samples += batch_n_samples\n-        correct_predictions += batch_correct_predictions\n+        correct_predictions += int(batch_correct_predictions)\n\n     accuracy = correct_predictions / n_samples\n     return total_loss, accuracy\n\n\n def make_datasets(\n     dataset_path: str,\n     val_split: float = 0.1,\n     val_split_seed: int = 42,\n ):\n     \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n     NOTE: We don't use image transforms here for simplicity.\n     Having different transformations for train and validation would complicate things a bit.\n     Later examples will show how to do the train/val/test split properly when using transforms.\n     \"\"\"\n     train_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=True\n     )\n     test_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=False\n     )\n     # Split the training dataset into a training and validation set.\n-    n_samples = len(train_dataset)\n-    n_valid = int(val_split * n_samples)\n-    n_train = n_samples - n_valid\n     train_dataset, valid_dataset = random_split(\n-        train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n+        train_dataset,\n+        ((1 - val_split), val_split),\n+        torch.Generator().manual_seed(val_split_seed),\n     )\n     return train_dataset, valid_dataset, test_dataset\n\n\n def get_num_workers() -&gt; int:\n-    \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n+    \"\"\"Gets the optimal number of DataLoader workers to use in the current job.\"\"\"\n     if \"SLURM_CPUS_PER_TASK\" in os.environ:\n         return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n     if hasattr(os, \"sched_getaffinity\"):\n         return len(os.sched_getaffinity(0))\n     return torch.multiprocessing.cpu_count()\n\n\n+def load_checkpoint(checkpoint_dir: Path, **torch_load_kwargs) -&gt; RunState | None:\n+    \"\"\"Loads the latest checkpoint if possible, otherwise returns `None`.\"\"\"\n+    logger = logging.getLogger(__name__)\n+\n+    checkpoint_file = checkpoint_dir / CHECKPOINT_FILE_NAME\n+    restart_count = int(os.environ.get(\"SLURM_RESTART_COUNT\", 0))\n+    if restart_count:\n+        logger.info(\n+            f\"NOTE: This job has been restarted {restart_count} times by SLURM.\"\n+        )\n+\n+    if not checkpoint_file.exists():\n+        logger.debug(f\"No checkpoint found in checkpoints dir ({checkpoint_dir}).\")\n+        if restart_count:\n+            logger.warning(\n+                RuntimeWarning(\n+                    f\"This job has been restarted {restart_count} times by SLURM, but no \"\n+                    \"checkpoint was found! This either means that your checkpointing code is \"\n+                    \"broken, or that the job did not reach the checkpointing portion of your \"\n+                    \"training loop.\"\n+                )\n+            )\n+        return None\n+\n+    torch_load_kwargs.setdefault(\"weights_only\", False)\n+    checkpoint_state: dict = torch.load(checkpoint_file, **torch_load_kwargs)\n+\n+    missing_keys = set(checkpoint_state.keys()) - RunState.__required_keys__\n+    if missing_keys:\n+        warnings.warn(\n+            RuntimeWarning(\n+                f\"Checkpoint at {checkpoint_file} is missing the following keys: {missing_keys}. \"\n+                f\"Ignoring this checkpoint.\"\n+            )\n+        )\n+        return None\n+\n+    logger.debug(f\"Resuming from the checkpoint file at {checkpoint_file}\")\n+    state: RunState = checkpoint_state  # type: ignore\n+    return state\n+\n+\n+def save_checkpoint(checkpoint_dir: Path, is_best: bool, state: RunState):\n+    \"\"\"Saves a checkpoint with the current state of the run in the checkpoint dir.\n+\n+    The best checkpoint is also updated if `is_best` is `True`.\n+\n+    Parameters\n+    ----------\n+    checkpoint_dir: The checkpoint directory.\n+    is_best: Whether this is the best checkpoint so far.\n+    state: The dictionary containing all the things to save.\n+    \"\"\"\n+    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n+    checkpoint_file = checkpoint_dir / CHECKPOINT_FILE_NAME\n+\n+    # Use a unique ID to avoid any potential collisions.\n+    unique_id = uuid.uuid1()\n+    temp_checkpoint_file = checkpoint_file.with_suffix(f\".temp{unique_id}\")\n+\n+    torch.save(state, temp_checkpoint_file)\n+    os.replace(temp_checkpoint_file, checkpoint_file)\n+\n+    if is_best:\n+        best_checkpoint_file = checkpoint_file.with_name(\"model_best.pth\")\n+        temp_best_checkpoint_file = best_checkpoint_file.with_suffix(\n+            f\".temp{unique_id}\"\n+        )\n+        shutil.copyfile(checkpoint_file, temp_best_checkpoint_file)\n+        os.replace(temp_best_checkpoint_file, best_checkpoint_file)\n+\n+\n if __name__ == \"__main__\":\n     main()\n</code></pre>"},{"location":"examples/good_practices/checkpointing/#running-this-example","title":"Running this example","text":"<pre><code>$ sbatch job.sh\n</code></pre>"},{"location":"examples/good_practices/hpo_with_orion/","title":"Hyperparameter Optimization with Or\u00edon","text":"<p>There are different frameworks that allow you to do hyperparameter optimization, for example wandb, hydra, Ray Tune and Or\u00edon. Here we provide an example for Or\u00edon, the HPO framework developped at Mila.</p> <p>Orion is an asynchronous framework for black-box function optimization developped at Mila.</p> <p>Its purpose is to serve as a meta-optimizer for machine learning models and training, as well as a flexible experimentation platform for large scale asynchronous optimization procedures.</p> <p>Prerequisites</p> <p>Make sure to read the following sections of the documentation before using this example:</p> <ul> <li>PyTorch Setup</li> <li>Single GPU Job</li> </ul> <p>The full documentation for Or\u00edon is available on Or\u00edon's ReadTheDocs page.</p> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <p>Hyperparameter optimization is very easy to parallelize as each trial (unique set of hyperparameters) are independant of each other. The easiest way is to launch as many jobs as possible each trying a different set of hyperparameters and reporting their results back to a synchronized location (database).</p> <p>job.sh</p> <p>The easiest way to run an hyperparameter search on the cluster is simply to use a job array, which will launch the same job n times. Your HPO library will generate different parameters to try for each job.</p> <p>Orion saves all the results of its optimization process in a database, by default it is using a local database on a shared filesystem named <code>pickleddb</code>. You will need to specify its location and the name of your experiment. Optionally you can configure workers which will run in parallel to maximize resource usage.</p> <pre><code> # distributed/single_gpu/job.sh -&gt; good_practices/hpo_with_orion/job.sh\n #!/bin/bash\n #SBATCH --ntasks=1\n #SBATCH --ntasks-per-node=1\n #SBATCH --cpus-per-task=4\n #SBATCH --gpus-per-task=l40s:1\n #SBATCH --mem-per-gpu=16G\n #SBATCH --time=00:15:00\n\n # Exit on error\n set -e\n\n # Echo time and hostname into log\n echo \"Date:     $(date)\"\n echo \"Hostname: $(hostname)\"\n\n # To make your code as much reproducible as possible with\n # `torch.use_deterministic_algorithms(True)`, uncomment the following block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # export CUBLAS_WORKSPACE_CONFIG=:4096:8\n ## === Reproducibility (END) ===\n\n # Stage dataset into $SLURM_TMPDIR\n mkdir -p $SLURM_TMPDIR/data\n cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n+srun --ntasks=${SLURM_JOB_NUM_NODES:-1} uv run python -c \\\n+    'import os, torchvision.datasets; torchvision.datasets.CIFAR10(root=os.environ[\"SLURM_TMPDIR\"] + \"/data\", download=True)'\n # General-purpose alternatives combining copy and unpack:\n #     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n #     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n\n-# Execute Python script\n-# Use the `--offline` option of `uv run` on clusters without internet access on compute nodes.\n-# Using the `--locked` option can help make your experiments easier to reproduce (it forces\n-# your uv.lock file to be up to date with the dependencies declared in pyproject.toml).\n-srun uv run python main.py\n+# =============\n+# Execute Orion\n+# =============\n+\n+# Specify an experiment name with `-n`,\n+# which could be reused to display results (see section \"Running example\" below)\n+\n+# Specify max trials (here 10) to prevent a too-long run.\n+\n+# Then you can specify a search space for each `main.py`'s script parameter\n+# you want to optimize. Here we optimize only the learning rate.\n+\n+\n+# Configure Orion\n+# ===================\n+#\n+#    - use a pickleddb database stored in $SCRATCH\n+#    - worker dies if idle for more than a minute\n+#\n+export ORION_CONFIG=$SLURM_TMPDIR/orion-config.yml\n+cat &gt; $ORION_CONFIG &lt;&lt;- EOM\n+    experiment:\n+        name: orion-example\n+        algorithms:\n+            tpe:\n+                seed: null\n+                n_initial_points: 5\n+        max_broken: 10\n+        max_trials: 10\n+\n+    storage:\n+        database:\n+            host: $SCRATCH/orion.pkl\n+            type: pickleddb\n+EOM\n+\n+srun uv run orion hunt --config $ORION_CONFIG python main.py \\\n+    --learning-rate~'loguniform(1e-5, 1.0)'\n</code></pre> <p>pyproject.toml</p> <p>This doesn't change much, the only difference is that we add the Orion dependency.</p> <pre><code>[project]\nname = \"orion-example\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nreadme = \"README.md\"\nrequires-python = \"&gt;=3.11,&lt;3.12\"\ndependencies = [\n    # Fix orion to be compatible with Python &gt;=3.12\n    \"orion&gt;=0.2.7\",\n    \"rich&gt;=14.0.0\",\n    \"setuptools&gt;=80.9.0\",  # Required for pkg_resources used by orion\n    \"torch&gt;=2.7.1\",\n    \"torchvision&gt;=0.22.1\",\n    \"tqdm&gt;=4.67.1\",\n]\n</code></pre> <p>main.py</p> <p>Here we only really add the reporting of the objective to Orion at the end of the main function.</p> <pre><code> # distributed/single_gpu/main.py -&gt; good_practices/hpo_with_orion/main.py\n-\"\"\"Single-GPU training example.\"\"\"\n+\"\"\"Hyperparameter optimization using Or\u00edon.\"\"\"\n\n import argparse\n+import json\n import logging\n import os\n import random\n import sys\n from pathlib import Path\n\n import numpy as np\n import rich.logging\n import torch\n+from orion.client import report_objective\n from torch import Tensor, nn\n from torch.nn import functional as F\n from torch.utils.data import DataLoader, random_split\n from torchvision import transforms\n from torchvision.datasets import CIFAR10\n from torchvision.models import resnet18\n from tqdm import tqdm\n\n\n # To make your code as much reproducible as possible, uncomment the following\n # block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # torch.use_deterministic_algorithms(True)\n ## === Reproducibility (END) ===\n\n\n def main():\n-    # Use an argument parser so we can pass hyperparameters from the command line.\n+    # Add an argument parser so that we can pass hyperparameters from command line.\n     parser = argparse.ArgumentParser(description=__doc__)\n     parser.add_argument(\"--epochs\", type=int, default=10)\n     parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n     parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n     parser.add_argument(\"--batch-size\", type=int, default=128)\n     parser.add_argument(\"--seed\", type=int, default=42)\n     args = parser.parse_args()\n\n-    epochs: int = args.epochs\n-    learning_rate: float = args.learning_rate\n-    weight_decay: float = args.weight_decay\n-    batch_size: int = args.batch_size\n-    seed: int = args.seed\n+    epochs = args.epochs\n+    learning_rate = args.learning_rate\n+    weight_decay = args.weight_decay\n+    batch_size = args.batch_size\n+    seed = args.seed\n\n     # Seed the random number generators as early as possible for reproducibility\n     random.seed(seed)\n     np.random.seed(seed)\n     torch.random.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n\n     # Check that the GPU is available\n     assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n     device = torch.device(\"cuda\", 0)\n\n     # Setup logging (optional, but much better than using print statements)\n     # Uses the `rich` package to make logs pretty.\n     logging.basicConfig(\n         level=logging.INFO,\n         format=\"%(message)s\",\n         handlers=[\n             rich.logging.RichHandler(\n                 markup=True,\n                 console=rich.console.Console(\n                     # Allower wider log lines in sbatch output files than on the terminal.\n                     width=120 if not sys.stdout.isatty() else None\n                 ),\n             )\n         ],\n     )\n\n     logger = logging.getLogger(__name__)\n+    logger.info(f\"Args: {json.dumps(vars(args), indent=1)}\")\n\n     # Create a model and move it to the GPU.\n     model = resnet18(num_classes=10)\n     model.to(device=device)\n\n     optimizer = torch.optim.AdamW(\n         model.parameters(), lr=learning_rate, weight_decay=weight_decay\n     )\n\n     # Setup CIFAR10\n     num_workers = get_num_workers()\n     dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n     train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))\n     train_dataloader = DataLoader(\n         train_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=True,\n     )\n     valid_dataloader = DataLoader(\n         valid_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n     _test_dataloader = DataLoader(  # NOTE: Not used in this example.\n         test_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n\n     # Checkout the \"checkpointing and preemption\" example for more info!\n     logger.debug(\"Starting training from scratch.\")\n-\n+    val_accuracy = 0.0\n     for epoch in range(epochs):\n         logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n         # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)\n         model.train()\n\n         # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n         progress_bar = tqdm(\n             total=len(train_dataloader),\n             desc=f\"Train epoch {epoch}\",\n             disable=not sys.stdout.isatty(),  # Disable progress bar in non-interactive environments.\n         )\n\n         # Training loop\n         for batch in train_dataloader:\n             # Move the batch to the GPU before we pass it to the model\n             batch = tuple(item.to(device) for item in batch)\n             x, y = batch\n\n             # Forward pass\n             logits: Tensor = model(x)\n\n             loss = F.cross_entropy(logits, y)\n\n             optimizer.zero_grad()\n             loss.backward()\n             optimizer.step()\n\n             # Calculate some metrics:\n             n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n             n_samples = y.shape[0]\n             accuracy = n_correct_predictions / n_samples\n\n             logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n             logger.debug(f\"Average Loss: {loss.item()}\")\n\n             # Advance the progress bar one step and update the progress bar text.\n             progress_bar.update(1)\n             progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n         progress_bar.close()\n\n         val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)\n         logger.info(\n             f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n         )\n+        val_accuracy = float(val_accuracy)\n+\n+    # We report to Orion the objective that we want to minimize.\n+    report_objective(1 - val_accuracy)\n\n     print(\"Done!\")\n\n\n @torch.no_grad()\n def validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n     model.eval()\n\n     total_loss = 0.0\n     n_samples = 0\n     correct_predictions = 0\n\n     for batch in dataloader:\n         batch = tuple(item.to(device) for item in batch)\n         x, y = batch\n\n         logits: Tensor = model(x)\n         loss = F.cross_entropy(logits, y)\n\n         batch_n_samples = x.shape[0]\n         batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n\n         total_loss += loss.item()\n         n_samples += batch_n_samples\n         correct_predictions += batch_correct_predictions\n\n     accuracy = correct_predictions / n_samples\n     return total_loss, accuracy\n\n\n def make_datasets(\n     dataset_path: str,\n     val_split: float = 0.1,\n     val_split_seed: int = 42,\n ):\n     \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n     NOTE: We don't use image transforms here for simplicity.\n     Having different transformations for train and validation would complicate things a bit.\n     Later examples will show how to do the train/val/test split properly when using transforms.\n     \"\"\"\n     train_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=True\n     )\n     test_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=False\n     )\n     # Split the training dataset into a training and validation set.\n     n_samples = len(train_dataset)\n     n_valid = int(val_split * n_samples)\n     n_train = n_samples - n_valid\n     train_dataset, valid_dataset = random_split(\n         train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n     )\n     return train_dataset, valid_dataset, test_dataset\n\n\n def get_num_workers() -&gt; int:\n     \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n     if \"SLURM_CPUS_PER_TASK\" in os.environ:\n         return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n     if hasattr(os, \"sched_getaffinity\"):\n         return len(os.sched_getaffinity(0))\n     return torch.multiprocessing.cpu_count()\n\n\n if __name__ == \"__main__\":\n     main()\n</code></pre>"},{"location":"examples/good_practices/hpo_with_orion/#running-this-example","title":"Running this example","text":"<p>In the example below we use 10 jobs each with 5 CPU cores and one GPU. Each job will run 5 tasks in parallel on the same GPU to maximize its utilization. This means there could be 50 sets of hyperparameters being worked on in parallel across 10 GPUs.</p> <pre><code> $ sbatch --array=1-10 --ntasks-per-gpu=5 --gpus=1 --cpus-per-task=1 job.sh\n</code></pre> <p>To get more information about the optimization run, use <code>orion info</code> with the experiment name:</p> <pre><code> $ uv run orion info -n orion-example\n</code></pre> <p>You can also generate a plot to visualize the optimization run. For example:</p> <pre><code> $ uv run orion plot regret -n orion-example\n</code></pre> <p>For more complex and useful plots, see Or\u00edon documentation.</p>"},{"location":"examples/good_practices/launch_many_jobs/","title":"Launch many jobs from same shell script","text":"<p>Sometimes you may want to run the same job with different arguments. For example, you may want to launch an experiment using a few different learning rates. This example shows an easy way to do this.</p> <p>Prerequisites Make sure to read the following sections of the documentation before using this example:</p> <ul> <li>PyTorch Setup</li> <li>Single GPU</li> </ul> <p>job.sh</p> <p>Compared to the single GPU job example, here we use the <code>$@</code> bash directive to pass command-line arguments down to the Python script.</p> <p>This makes it very easy to submit multiple jobs, each with different values!</p> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <pre><code> # distributed/single_gpu/job.sh -&gt; good_practices/launch_many_jobs/job.sh\n #!/bin/bash\n #SBATCH --ntasks=1\n #SBATCH --ntasks-per-node=1\n #SBATCH --cpus-per-task=4\n #SBATCH --gpus-per-task=l40s:1\n #SBATCH --mem-per-gpu=16G\n #SBATCH --time=00:15:00\n\n # Exit on error\n set -e\n\n # Echo time and hostname into log\n echo \"Date:     $(date)\"\n echo \"Hostname: $(hostname)\"\n\n # To make your code as much reproducible as possible with\n # `torch.use_deterministic_algorithms(True)`, uncomment the following block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # export CUBLAS_WORKSPACE_CONFIG=:4096:8\n ## === Reproducibility (END) ===\n\n # Stage dataset into $SLURM_TMPDIR\n mkdir -p $SLURM_TMPDIR/data\n cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n # General-purpose alternatives combining copy and unpack:\n #     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n #     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n\n # Execute Python script\n # Use the `--offline` option of `uv run` on clusters without internet access on compute nodes.\n # Using the `--locked` option can help make your experiments easier to reproduce (it forces\n # your uv.lock file to be up to date with the dependencies declared in pyproject.toml).\n-srun uv run python main.py\n+srun uv run python main.py \"$@\"\n</code></pre> <p>Running this example</p> <p>You can run this example just like the single GPU job example, but you can now also pass command-line arguments directly when submitting the job with <code>sbatch</code>!</p> <p>For example:</p> <pre><code> $ sbatch job.sh --learning-rate 0.1\n $ sbatch job.sh --learning-rate 0.5\n $ sbatch job.sh --weight-decay 1e-3\n</code></pre>"},{"location":"examples/good_practices/launch_many_jobs/#next-steps","title":"Next steps","text":"<p>These next examples build on top of this one and show how to properly launch lots of jobs for hyper-parameter sweeps: * Using SLURM Job Arrays to launch lots of jobs * Running more effective Hyper-Parameter Sweeps with Orion</p>"},{"location":"examples/good_practices/many_tasks_per_gpu/","title":"Launch many tasks on the same GPU","text":"<p>If you want to use a powerful GPU efficiently, you can run many tasks on same GPU using a combination of <code>sbatch</code> arguments. In your <code>sbatch</code> script:</p> <ul> <li>Specify only 1 GPU to use, e.g. with <code>--gres=gpu:rtx8000:1</code></li> <li>Specify number of tasks to run on the selected GPU with <code>--ntasks-per-gpu=N</code></li> <li>Launch your job using <code>srun main.py</code> instead of just <code>main.py</code>.</li> </ul> <p><code>srun</code> will then launch <code>main.py</code> script <code>N</code> times. Each task will receive specific environment variables, such as <code>SLURM_PROCID</code>, which you can then use to parameterize the script execution.</p> <p>Prerequisites</p> <p>Make sure to read the following sections of the documentation before using this example:</p> <ul> <li>PyTorch setup</li> </ul> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <p>job.sh</p> <pre><code> # distributed/single_gpu/job.sh -&gt; good_practices/many_tasks_per_gpu/job.sh\n #!/bin/bash\n-#SBATCH --ntasks=1\n-#SBATCH --ntasks-per-node=1\n+#SBATCH --ntasks=2\n+#SBATCH --ntasks-per-gpu=2\n #SBATCH --cpus-per-task=4\n-#SBATCH --gpus-per-task=l40s:1\n+#SBATCH --gres=gpu:l40s:1\n #SBATCH --mem-per-gpu=16G\n #SBATCH --time=00:15:00\n\n # Exit on error\n set -e\n\n # Echo time and hostname into log\n echo \"Date:     $(date)\"\n echo \"Hostname: $(hostname)\"\n\n # To make your code as much reproducible as possible with\n # `torch.use_deterministic_algorithms(True)`, uncomment the following block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # export CUBLAS_WORKSPACE_CONFIG=:4096:8\n ## === Reproducibility (END) ===\n\n # Stage dataset into $SLURM_TMPDIR\n mkdir -p $SLURM_TMPDIR/data\n cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n # General-purpose alternatives combining copy and unpack:\n #     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n #     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n\n # Execute Python script\n # Use the `--offline` option of `uv run` on clusters without internet access on compute nodes.\n # Using the `--locked` option can help make your experiments easier to reproduce (it forces\n # your uv.lock file to be up to date with the dependencies declared in pyproject.toml).\n srun uv run python main.py\n</code></pre> <p>main.py</p> <pre><code> # distributed/single_gpu/main.py -&gt; good_practices/many_tasks_per_gpu/main.py\n-\"\"\"Single-GPU training example.\"\"\"\n+\"\"\"Many tasks per GPU (job packing) example.\"\"\"\n\n import argparse\n import logging\n import os\n import random\n import sys\n from pathlib import Path\n\n import numpy as np\n import rich.logging\n import torch\n from torch import Tensor, nn\n from torch.nn import functional as F\n from torch.utils.data import DataLoader, random_split\n from torchvision import transforms\n from torchvision.datasets import CIFAR10\n from torchvision.models import resnet18\n from tqdm import tqdm\n\n\n # To make your code as much reproducible as possible, uncomment the following\n # block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # torch.use_deterministic_algorithms(True)\n ## === Reproducibility (END) ===\n\n\n def main():\n     # Use an argument parser so we can pass hyperparameters from the command line.\n     parser = argparse.ArgumentParser(description=__doc__)\n     parser.add_argument(\"--epochs\", type=int, default=10)\n     parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n     parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n     parser.add_argument(\"--batch-size\", type=int, default=128)\n-    parser.add_argument(\"--seed\", type=int, default=42)\n+    # Get SLURM_PROCID and use it as a random seed for the script.\n+    # This makes it so each task within a job uses a different initialization with the same\n+    # hyper-parameters.\n+    parser.add_argument(\n+        \"--seed\",\n+        type=int,\n+        default=int(os.environ.get(\"SLURM_PROCID\", 0)),\n+        help=\"Random seed used for network initialization and the training loop.\",\n+    )\n     args = parser.parse_args()\n\n     epochs: int = args.epochs\n     learning_rate: float = args.learning_rate\n     weight_decay: float = args.weight_decay\n     batch_size: int = args.batch_size\n     seed: int = args.seed\n\n     # Seed the random number generators as early as possible for reproducibility\n     random.seed(seed)\n     np.random.seed(seed)\n     torch.random.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n\n     # Check that the GPU is available\n     assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n     device = torch.device(\"cuda\", 0)\n\n     # Setup logging (optional, but much better than using print statements)\n     # Uses the `rich` package to make logs pretty.\n     logging.basicConfig(\n         level=logging.INFO,\n         format=\"%(message)s\",\n         handlers=[\n             rich.logging.RichHandler(\n                 markup=True,\n                 console=rich.console.Console(\n                     # Allower wider log lines in sbatch output files than on the terminal.\n                     width=120 if not sys.stdout.isatty() else None\n                 ),\n             )\n         ],\n     )\n\n     logger = logging.getLogger(__name__)\n\n     # Create a model and move it to the GPU.\n     model = resnet18(num_classes=10)\n     model.to(device=device)\n\n     optimizer = torch.optim.AdamW(\n         model.parameters(), lr=learning_rate, weight_decay=weight_decay\n     )\n\n     # Setup CIFAR10\n     num_workers = get_num_workers()\n     dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n     train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))\n     train_dataloader = DataLoader(\n         train_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=True,\n     )\n     valid_dataloader = DataLoader(\n         valid_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n     _test_dataloader = DataLoader(  # NOTE: Not used in this example.\n         test_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n\n     # Checkout the \"checkpointing and preemption\" example for more info!\n     logger.debug(\"Starting training from scratch.\")\n\n     for epoch in range(epochs):\n         logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n         # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)\n         model.train()\n\n         # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n         progress_bar = tqdm(\n             total=len(train_dataloader),\n             desc=f\"Train epoch {epoch}\",\n             disable=not sys.stdout.isatty(),  # Disable progress bar in non-interactive environments.\n         )\n\n         # Training loop\n         for batch in train_dataloader:\n             # Move the batch to the GPU before we pass it to the model\n             batch = tuple(item.to(device) for item in batch)\n             x, y = batch\n\n             # Forward pass\n             logits: Tensor = model(x)\n\n             loss = F.cross_entropy(logits, y)\n\n             optimizer.zero_grad()\n             loss.backward()\n             optimizer.step()\n\n             # Calculate some metrics:\n             n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n             n_samples = y.shape[0]\n             accuracy = n_correct_predictions / n_samples\n\n             logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n             logger.debug(f\"Average Loss: {loss.item()}\")\n\n             # Advance the progress bar one step and update the progress bar text.\n             progress_bar.update(1)\n             progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n         progress_bar.close()\n\n         val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)\n         logger.info(\n             f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n         )\n\n     print(\"Done!\")\n\n\n @torch.no_grad()\n def validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n     model.eval()\n\n     total_loss = 0.0\n     n_samples = 0\n     correct_predictions = 0\n\n     for batch in dataloader:\n         batch = tuple(item.to(device) for item in batch)\n         x, y = batch\n\n         logits: Tensor = model(x)\n         loss = F.cross_entropy(logits, y)\n\n         batch_n_samples = x.shape[0]\n         batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n\n         total_loss += loss.item()\n         n_samples += batch_n_samples\n         correct_predictions += batch_correct_predictions\n\n     accuracy = correct_predictions / n_samples\n     return total_loss, accuracy\n\n\n def make_datasets(\n     dataset_path: str,\n     val_split: float = 0.1,\n     val_split_seed: int = 42,\n ):\n     \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n     NOTE: We don't use image transforms here for simplicity.\n     Having different transformations for train and validation would complicate things a bit.\n     Later examples will show how to do the train/val/test split properly when using transforms.\n     \"\"\"\n     train_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=True\n     )\n     test_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=False\n     )\n     # Split the training dataset into a training and validation set.\n     n_samples = len(train_dataset)\n     n_valid = int(val_split * n_samples)\n     n_train = n_samples - n_valid\n     train_dataset, valid_dataset = random_split(\n         train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n     )\n     return train_dataset, valid_dataset, test_dataset\n\n\n def get_num_workers() -&gt; int:\n     \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n     if \"SLURM_CPUS_PER_TASK\" in os.environ:\n         return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n     if hasattr(os, \"sched_getaffinity\"):\n         return len(os.sched_getaffinity(0))\n     return torch.multiprocessing.cpu_count()\n\n\n if __name__ == \"__main__\":\n     main()\n</code></pre>"},{"location":"examples/good_practices/many_tasks_per_gpu/#running-this-example","title":"Running this example","text":"<p>You can launch this example with sbatch:</p> <pre><code> $ sbatch job.sh\n</code></pre>"},{"location":"examples/good_practices/slurm_job_arrays/","title":"Launch many jobs using SLURM job arrays","text":"<p>Sometimes you may want to run many tasks by changing just a single parameter.</p> <p>One way to do that is to use SLURM job arrays, which consists of launching an array of jobs using the same script. Each job will run with a specific environment variable called <code>SLURM_ARRAY_TASK_ID</code>, containing the job index value inside job array. You can then slightly modify your script to choose appropriate parameter based on this variable.</p> <p>You can find more info about job arrays in the SLURM official documentation page.</p> <p>Prerequisites</p> <p>Make sure to read the following sections of the documentation before using this example:</p> <ul> <li>PyTorch setup</li> <li>Single GPU</li> <li>Checkpointing</li> <li>Many tasks per GPU</li> </ul> <p>The full source code for this example is available on the mila-docs GitHub repository.</p> <p>main.py</p> <pre><code> # distributed/single_gpu/main.py -&gt; good_practices/slurm_job_arrays/main.py\n-\"\"\"Single-GPU training example.\"\"\"\n+\"\"\"Job Arrays example.\"\"\"\n\n import argparse\n import logging\n import os\n import random\n import sys\n from pathlib import Path\n\n import numpy as np\n import rich.logging\n+import rich.pretty\n import torch\n from torch import Tensor, nn\n from torch.nn import functional as F\n from torch.utils.data import DataLoader, random_split\n from torchvision import transforms\n from torchvision.datasets import CIFAR10\n from torchvision.models import resnet18\n from tqdm import tqdm\n\n\n # To make your code as much reproducible as possible, uncomment the following\n # block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # torch.use_deterministic_algorithms(True)\n ## === Reproducibility (END) ===\n\n\n def main():\n+    job_index = int(os.environ.get(\"SLURM_ARRAY_TASK_ID\", \"0\"))\n+    num_jobs = int(os.environ.get(\"SLURM_ARRAY_TASK_COUNT\", \"1\"))\n+    if num_jobs &gt; 1:\n+        # When in a job array, we use SLURM ARRAY TASK ID to seed a random number generator\n+        # so that each job in the job array will have different set of hyper-parameters.\n+        # You can also view this as indexing into a predefined grid of hyper-parameters.\n+        # Here we just change the default values, but you can override a value from the command-line.\n+        gen = np.random.default_rng(seed=job_index)\n+        # Use random number generator to generate the default values of hyper-parameters.\n+        # If a value is passed from the command-line, it will override this and be used instead.\n+        default_learning_rate = gen.uniform(1e-6, 1e-2)\n+        default_weight_decay = gen.uniform(1e-6, 1e-3)\n+        default_batch_size = int(gen.integers(16, 256))\n+    else:\n+        default_learning_rate = 5e-4\n+        default_weight_decay = 1e-4\n+        default_batch_size = 128\n+\n     # Use an argument parser so we can pass hyperparameters from the command line.\n     parser = argparse.ArgumentParser(description=__doc__)\n     parser.add_argument(\"--epochs\", type=int, default=10)\n-    parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n-    parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n-    parser.add_argument(\"--batch-size\", type=int, default=128)\n-    parser.add_argument(\"--seed\", type=int, default=42)\n+    parser.add_argument(\"--learning-rate\", type=float, default=default_learning_rate)\n+    parser.add_argument(\"--weight-decay\", type=float, default=default_weight_decay)\n+    parser.add_argument(\"--batch-size\", type=int, default=default_batch_size)\n+    # Get SLURM_PROCID (the task index) and use it as a random seed for the script.\n+    # This makes it so each task within each job uses a different initialization with the same\n+    # hyper-parameters. This works great in combination with --ntasks-per-gpu &gt; 1 to use the GPU effectively!\n+    parser.add_argument(\n+        \"--seed\",\n+        type=int,\n+        default=int(os.environ.get(\"SLURM_PROCID\", 42)),\n+        help=\"Random seed used for network initialization and the training loop.\",\n+    )\n     args = parser.parse_args()\n\n     epochs: int = args.epochs\n     learning_rate: float = args.learning_rate\n     weight_decay: float = args.weight_decay\n     batch_size: int = args.batch_size\n     seed: int = args.seed\n\n     # Seed the random number generators as early as possible for reproducibility\n     random.seed(seed)\n     np.random.seed(seed)\n     torch.random.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n\n     # Check that the GPU is available\n     assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n     device = torch.device(\"cuda\", 0)\n\n     # Setup logging (optional, but much better than using print statements)\n     # Uses the `rich` package to make logs pretty.\n+    # Add a prefix to all the logged messages to identify the job and task.\n+    task_index = int(os.environ[\"SLURM_PROCID\"])\n+    num_tasks_per_job = int(os.environ[\"SLURM_NTASKS\"])\n     logging.basicConfig(\n         level=logging.INFO,\n-        format=\"%(message)s\",\n+        format=f\"[Job {job_index}/{num_jobs}][Task {task_index + 1}/{num_tasks_per_job}] %(message)s\",\n         handlers=[\n             rich.logging.RichHandler(\n                 markup=True,\n                 console=rich.console.Console(\n                     # Allower wider log lines in sbatch output files than on the terminal.\n                     width=120 if not sys.stdout.isatty() else None\n                 ),\n             )\n         ],\n     )\n\n     logger = logging.getLogger(__name__)\n+    logger.info(f\"Args:\\n{rich.pretty.pretty_repr(vars(args))}\")\n\n     # Create a model and move it to the GPU.\n     model = resnet18(num_classes=10)\n     model.to(device=device)\n\n     optimizer = torch.optim.AdamW(\n         model.parameters(), lr=learning_rate, weight_decay=weight_decay\n     )\n\n     # Setup CIFAR10\n     num_workers = get_num_workers()\n     dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n     train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))\n     train_dataloader = DataLoader(\n         train_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=True,\n     )\n     valid_dataloader = DataLoader(\n         valid_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n     _test_dataloader = DataLoader(  # NOTE: Not used in this example.\n         test_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n\n     # Checkout the \"checkpointing and preemption\" example for more info!\n     logger.debug(\"Starting training from scratch.\")\n\n     for epoch in range(epochs):\n         logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n         # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)\n         model.train()\n\n         # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n         progress_bar = tqdm(\n             total=len(train_dataloader),\n             desc=f\"Train epoch {epoch}\",\n             disable=not sys.stdout.isatty(),  # Disable progress bar in non-interactive environments.\n         )\n\n         # Training loop\n         for batch in train_dataloader:\n             # Move the batch to the GPU before we pass it to the model\n             batch = tuple(item.to(device) for item in batch)\n             x, y = batch\n\n             # Forward pass\n             logits: Tensor = model(x)\n\n             loss = F.cross_entropy(logits, y)\n\n             optimizer.zero_grad()\n             loss.backward()\n             optimizer.step()\n\n             # Calculate some metrics:\n             n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n             n_samples = y.shape[0]\n             accuracy = n_correct_predictions / n_samples\n\n             logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n             logger.debug(f\"Average Loss: {loss.item()}\")\n\n             # Advance the progress bar one step and update the progress bar text.\n             progress_bar.update(1)\n             progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n         progress_bar.close()\n\n         val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)\n         logger.info(\n             f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n         )\n\n     print(\"Done!\")\n\n\n @torch.no_grad()\n def validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n     model.eval()\n\n     total_loss = 0.0\n     n_samples = 0\n     correct_predictions = 0\n\n     for batch in dataloader:\n         batch = tuple(item.to(device) for item in batch)\n         x, y = batch\n\n         logits: Tensor = model(x)\n         loss = F.cross_entropy(logits, y)\n\n         batch_n_samples = x.shape[0]\n         batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n\n         total_loss += loss.item()\n         n_samples += batch_n_samples\n         correct_predictions += batch_correct_predictions\n\n     accuracy = correct_predictions / n_samples\n     return total_loss, accuracy\n\n\n def make_datasets(\n     dataset_path: str,\n     val_split: float = 0.1,\n     val_split_seed: int = 42,\n ):\n     \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n     NOTE: We don't use image transforms here for simplicity.\n     Having different transformations for train and validation would complicate things a bit.\n     Later examples will show how to do the train/val/test split properly when using transforms.\n     \"\"\"\n     train_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=True\n     )\n     test_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=False\n     )\n     # Split the training dataset into a training and validation set.\n     n_samples = len(train_dataset)\n     n_valid = int(val_split * n_samples)\n     n_train = n_samples - n_valid\n     train_dataset, valid_dataset = random_split(\n         train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n     )\n     return train_dataset, valid_dataset, test_dataset\n\n\n def get_num_workers() -&gt; int:\n     \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n     if \"SLURM_CPUS_PER_TASK\" in os.environ:\n         return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n     if hasattr(os, \"sched_getaffinity\"):\n         return len(os.sched_getaffinity(0))\n     return torch.multiprocessing.cpu_count()\n\n\n if __name__ == \"__main__\":\n     main()\n</code></pre>"},{"location":"examples/good_practices/slurm_job_arrays/#running-this-example","title":"Running this example","text":"<p>You can then launch a job array using <code>sbatch</code> with the <code>--array</code> argument, for example:</p> <pre><code> $ sbatch --array=1-5 job.sh\n</code></pre> <p>In this example, 5 jobs will be launched with indices (therefore, values of <code>SLURM_ARRAY_TASK_ID</code>) from 1 to 5.</p> <p>Even better, you can combine job arrays with the many tasks per GPU (job packing) technique explained in Many tasks per GPU! For example, this command will launch 10 jobs (10 sets of hyper-parameters), each using 5 tasks to run 5 different initializations on the same GPU.</p> <pre><code> $ sbatch --array=1-10 --ntasks-per-gpu=5 --gpus=1 --cpus-per-task=1 job.sh\n</code></pre> <p>Note that each task requires at least one CPU, so you may need to adjust the cpu count in your job in order to scale up <code>--ntasks-per-gpu</code>. Here with <code>--cpu-per-task=1</code>, this will scale nicely.</p>"},{"location":"examples/good_practices/wandb_setup/","title":"Wandb Setup","text":"<p>Prerequisites:</p> <ul> <li>PyTorch setup</li> <li>Single GPU</li> </ul> <p>Make sure to create a Wandb account, then you can either:</p> <ul> <li>Set your <code>WANDB_API_KEY</code> environment variable</li> <li>Run <code>wandb login</code> from the command line</li> </ul> <p>Other resources:</p> <ul> <li>Wandb quickstart</li> </ul> <p>Click here to see the source code for this example.</p> <p>job.sh</p> <pre><code> # distributed/single_gpu/job.sh -&gt; good_practices/wandb_setup/job.sh\n #!/bin/bash\n #SBATCH --ntasks=1\n #SBATCH --ntasks-per-node=1\n #SBATCH --cpus-per-task=4\n #SBATCH --gpus-per-task=l40s:1\n #SBATCH --mem-per-gpu=16G\n #SBATCH --time=00:15:00\n\n # Exit on error\n set -e\n\n # Echo time and hostname into log\n echo \"Date:     $(date)\"\n echo \"Hostname: $(hostname)\"\n\n # To make your code as much reproducible as possible with\n # `torch.use_deterministic_algorithms(True)`, uncomment the following block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # export CUBLAS_WORKSPACE_CONFIG=:4096:8\n ## === Reproducibility (END) ===\n\n # Stage dataset into $SLURM_TMPDIR\n mkdir -p $SLURM_TMPDIR/data\n cp /network/datasets/cifar10/cifar-10-python.tar.gz $SLURM_TMPDIR/data/\n # General-purpose alternatives combining copy and unpack:\n #     unzip   /network/datasets/some/file.zip -d $SLURM_TMPDIR/data/\n #     tar -xf /network/datasets/some/file.tar -C $SLURM_TMPDIR/data/\n\n+## On DRAC or PAICE clusters you can load this module to log to wandb:\n+# module load httpproxy\n+## Otherwise you can also do this:\n+# export WANDB_MODE=offline\n+\n # Execute Python script\n # Use the `--offline` option of `uv run` on clusters without internet access on compute nodes.\n # Using the `--locked` option can help make your experiments easier to reproduce (it forces\n # your uv.lock file to be up to date with the dependencies declared in pyproject.toml).\n srun uv run python main.py\n</code></pre> <p>main.py</p> <pre><code> # distributed/single_gpu/main.py -&gt; good_practices/wandb_setup/main.py\n-\"\"\"Single-GPU training example.\"\"\"\n+\"\"\"Example job that uses Weights &amp; Biases (wandb.ai).\"\"\"\n\n import argparse\n import logging\n import os\n import random\n import sys\n from pathlib import Path\n\n import numpy as np\n import rich.logging\n import torch\n+import wandb\n from torch import Tensor, nn\n from torch.nn import functional as F\n from torch.utils.data import DataLoader, random_split\n from torchvision import transforms\n from torchvision.datasets import CIFAR10\n from torchvision.models import resnet18\n from tqdm import tqdm\n\n\n # To make your code as much reproducible as possible, uncomment the following\n # block:\n ## === Reproducibility ===\n ## Be warned that this can make your code slower. See\n ## https://pytorch.org/docs/stable/notes/randomness.html#cublas-and-cudnn-deterministic-operations\n ## for more details.\n # torch.use_deterministic_algorithms(True)\n ## === Reproducibility (END) ===\n\n\n def main():\n     # Use an argument parser so we can pass hyperparameters from the command line.\n     parser = argparse.ArgumentParser(description=__doc__)\n     parser.add_argument(\"--epochs\", type=int, default=10)\n     parser.add_argument(\"--learning-rate\", type=float, default=5e-4)\n     parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n     parser.add_argument(\"--batch-size\", type=int, default=128)\n     parser.add_argument(\"--seed\", type=int, default=42)\n     args = parser.parse_args()\n\n     epochs: int = args.epochs\n     learning_rate: float = args.learning_rate\n     weight_decay: float = args.weight_decay\n     batch_size: int = args.batch_size\n     seed: int = args.seed\n\n     # Seed the random number generators as early as possible for reproducibility\n     random.seed(seed)\n     np.random.seed(seed)\n     torch.random.manual_seed(seed)\n     torch.cuda.manual_seed_all(seed)\n\n     # Check that the GPU is available\n     assert torch.cuda.is_available() and torch.cuda.device_count() &gt; 0\n     device = torch.device(\"cuda\", 0)\n\n     # Setup logging (optional, but much better than using print statements)\n     # Uses the `rich` package to make logs pretty.\n     logging.basicConfig(\n         level=logging.INFO,\n         format=\"%(message)s\",\n         handlers=[\n             rich.logging.RichHandler(\n                 markup=True,\n                 console=rich.console.Console(\n                     # Allower wider log lines in sbatch output files than on the terminal.\n                     width=120 if not sys.stdout.isatty() else None\n                 ),\n             )\n         ],\n     )\n\n     logger = logging.getLogger(__name__)\n\n+    # To resume experiments with Wandb, we need to have code that can properly\n+    # handle checkpointing (see other minimalist example about \"checkpointing\").\n+    # We have to set the `id` of the experiment.\n+    # This specific example here does not do that.\n+\n+    # Setup Wandb\n+    wandb.init(\n+        # Set the project where this run will be logged\n+        project=\"wandb-example\",\n+        name=os.environ.get(\"SLURM_JOB_ID\"),\n+        resume=\"allow\",  # See https://docs.wandb.ai/guides/runs/resuming\n+        # Track hyperparameters and run metadata\n+        config=vars(args),\n+    )\n+\n     # Create a model and move it to the GPU.\n     model = resnet18(num_classes=10)\n     model.to(device=device)\n\n     optimizer = torch.optim.AdamW(\n         model.parameters(), lr=learning_rate, weight_decay=weight_decay\n     )\n\n     # Setup CIFAR10\n     num_workers = get_num_workers()\n     dataset_path = Path(os.environ.get(\"SLURM_TMPDIR\", \".\")) / \"data\"\n     train_dataset, valid_dataset, test_dataset = make_datasets(str(dataset_path))\n     train_dataloader = DataLoader(\n         train_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=True,\n     )\n     valid_dataloader = DataLoader(\n         valid_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n     _test_dataloader = DataLoader(  # NOTE: Not used in this example.\n         test_dataset,\n         batch_size=batch_size,\n         num_workers=num_workers,\n         shuffle=False,\n     )\n\n     # Checkout the \"checkpointing and preemption\" example for more info!\n     logger.debug(\"Starting training from scratch.\")\n\n     for epoch in range(epochs):\n         logger.debug(f\"Starting epoch {epoch}/{epochs}\")\n\n         # Set the model in training mode (important for e.g. BatchNorm and Dropout layers)\n         model.train()\n\n         # NOTE: using a progress bar from tqdm because it's nicer than using `print`.\n         progress_bar = tqdm(\n             total=len(train_dataloader),\n             desc=f\"Train epoch {epoch}\",\n             disable=not sys.stdout.isatty(),  # Disable progress bar in non-interactive environments.\n         )\n\n         # Training loop\n         for batch in train_dataloader:\n             # Move the batch to the GPU before we pass it to the model\n             batch = tuple(item.to(device) for item in batch)\n             x, y = batch\n\n             # Forward pass\n             logits: Tensor = model(x)\n\n             loss = F.cross_entropy(logits, y)\n\n             optimizer.zero_grad()\n             loss.backward()\n             optimizer.step()\n\n             # Calculate some metrics:\n             n_correct_predictions = logits.detach().argmax(-1).eq(y).sum()\n             n_samples = y.shape[0]\n             accuracy = n_correct_predictions / n_samples\n\n             logger.debug(f\"Accuracy: {accuracy.item():.2%}\")\n             logger.debug(f\"Average Loss: {loss.item()}\")\n\n+            # Log metrics with wandb\n+            wandb.log({\"train/accuracy\": accuracy, \"train/loss\": loss})\n+\n             # Advance the progress bar one step and update the progress bar text.\n             progress_bar.update(1)\n             progress_bar.set_postfix(loss=loss.item(), accuracy=accuracy.item())\n         progress_bar.close()\n\n         val_loss, val_accuracy = validation_loop(model, valid_dataloader, device)\n         logger.info(\n             f\"Epoch {epoch}: Val loss: {val_loss:.3f} accuracy: {val_accuracy:.2%}\"\n         )\n\n     print(\"Done!\")\n\n\n @torch.no_grad()\n def validation_loop(model: nn.Module, dataloader: DataLoader, device: torch.device):\n     model.eval()\n\n     total_loss = 0.0\n     n_samples = 0\n     correct_predictions = 0\n\n     for batch in dataloader:\n         batch = tuple(item.to(device) for item in batch)\n         x, y = batch\n\n         logits: Tensor = model(x)\n         loss = F.cross_entropy(logits, y)\n\n         batch_n_samples = x.shape[0]\n         batch_correct_predictions = logits.argmax(-1).eq(y).sum()\n\n         total_loss += loss.item()\n         n_samples += batch_n_samples\n         correct_predictions += batch_correct_predictions\n\n     accuracy = correct_predictions / n_samples\n     return total_loss, accuracy\n\n\n def make_datasets(\n     dataset_path: str,\n     val_split: float = 0.1,\n     val_split_seed: int = 42,\n ):\n     \"\"\"Returns the training, validation, and test splits for CIFAR10.\n\n     NOTE: We don't use image transforms here for simplicity.\n     Having different transformations for train and validation would complicate things a bit.\n     Later examples will show how to do the train/val/test split properly when using transforms.\n     \"\"\"\n     train_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=True\n     )\n     test_dataset = CIFAR10(\n         root=dataset_path, transform=transforms.ToTensor(), download=True, train=False\n     )\n     # Split the training dataset into a training and validation set.\n     n_samples = len(train_dataset)\n     n_valid = int(val_split * n_samples)\n     n_train = n_samples - n_valid\n     train_dataset, valid_dataset = random_split(\n         train_dataset, (n_train, n_valid), torch.Generator().manual_seed(val_split_seed)\n     )\n     return train_dataset, valid_dataset, test_dataset\n\n\n def get_num_workers() -&gt; int:\n     \"\"\"Gets the optimal number of DatLoader workers to use in the current job.\"\"\"\n     if \"SLURM_CPUS_PER_TASK\" in os.environ:\n         return int(os.environ[\"SLURM_CPUS_PER_TASK\"])\n     if hasattr(os, \"sched_getaffinity\"):\n         return len(os.sched_getaffinity(0))\n     return torch.multiprocessing.cpu_count()\n\n\n if __name__ == \"__main__\":\n     main()\n</code></pre>"},{"location":"examples/good_practices/wandb_setup/#running-this-example","title":"Running this example","text":"<p>Note: On DRAC clusters you will need to run <code>wandb off</code> to log your data as offline mode. You will then be able to upload your runs with the command <code>wandb sync --sync-all</code>.</p> <pre><code>$ wandb login\n</code></pre> <pre><code>$ sbatch job.sh\n</code></pre>"},{"location":"singularity/","title":"Index","text":"<p>Singularity is a software container system designed to facilitate portability and reproducibility of high performance computing (HPC) workflows. It performs a function similar to docker, but with HPC in mind. It is compatible with existing docker containers, and provides tools for building new containers from recipe files or ad-hoc commands.</p> <p>Building a container is like creating a new environment except that containers are much more powerful since they are self-contain systems. With singularity, there are two ways to build containers.</p> <p>The first one is by yourself, it's like when you got a new Linux laptop and you don't really know what you need, if you see that something is missing, you install it. Here you can get a vanilla container with Ubuntu called a sandbox, you log in and you install each packages by yourself. This procedure can take time but will allow you to understand how things work and what you need. This is recommended if you need to figure out how things will be compiled or if you want to install packages on the fly. We'll refer to this procedure as singularity sandboxes.</p> <p>The second one way is more like you know what you want, so you write a list of everything you need, you sent it to singularity and it will install everything for you. Those lists are called singularity recipes.</p>"},{"location":"singularity/#first-way-build-and-use-a-sandbox","title":"First way: Build and use a sandbox","text":"<p>You might ask yourself; On which machine should I build a container ?</p> <p>First of all, you need to choose where you'll build your container. This operation requires memory and high cpu usage.</p> <ul> <li> <p>(Recommended for beginner) If you need to use apt-get, you should build the container on your laptop with sudo privileges. You'll only need to install singularity on your laptop. Windows/Mac users can look <code>there</code>_ and Ubuntu/Debian users can use directly:</p> <pre><code>.. _there: https://www.sylabs.io/guides/3.0/user-guide/installation.html#install-on-windows-or-mac\n\n.. prompt:: bash $\n\n        sudo apt-get install singularity-container\n</code></pre> </li> <li> <p>If you can't install singularity on your laptop and you don't need apt-get, you can reserve a cpu node on the mila cluster to build your container.</p> </li> </ul> <p>In this case, in order to avoid too much I/O over the network, you should define the singularity cache locally:</p> <pre><code>    .. prompt:: bash $\n\n            export SINGULARITY_CACHEDIR=$SLURM_TMPDIR\n</code></pre> <ul> <li>If you can't install singularity on your laptop and you want to use apt-get, you can use <code>singularity-hub</code> to build your containers and read Recipe_section.</li> </ul> <p>.. _singularity-hub: https://www.singularity-hub.org/</p>"},{"location":"singularity/#download-containers-from-the-web","title":"Download containers from the web","text":"<p>Hopefully, you may not need to create containers from scratch as many have been already built for the most common deep learning software. You can find most of them on <code>dockerhub</code>_.</p> <p>.. _dockerhub: https://hub.docker.com/</p> <p>tip:         (Optional) You can also pull containers from nvidia cloud see nvidia</p> <p>Go on <code>dockerhub</code>_ and select the container you want to pull.</p> <p>.. _dockerhub: https://hub.docker.com/</p> <p>For example, if you want to get the latest pytorch version with gpu support (Replace runtime by devel if you need the full CUDA toolkit):</p> <pre><code>    singularity pull docker://pytorch/pytorch:1.0.1-cuda10.0-cudnn7-runtime\n</code></pre> <p>or the latest tensorflow:</p> <pre><code>    singularity pull docker://tensorflow/tensorflow:latest-gpu-py3\n</code></pre> <p>Currently the pulled image <code>pytorch.simg</code> or <code>tensorflow.simg</code> is read only meaning that you won't be able to install anything on it. Starting now, pytorch will be taken as example. If you use tensorflow, simply replace every pytorch occurrences by tensorflow.</p>"},{"location":"singularity/#how-to-add-or-install-stuff-in-a-container","title":"How to add or install stuff in a container","text":"<p>The first step is to transform your read only container <code>pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</code> in a writable version that will allow you to add packages.</p> <p>tip:         If you want to use apt-get you have to put sudo ahead of the following commands</p> <p>This command will create a writable image in the folder <code>pytorch</code>.</p> <pre><code>    singularity build --sandbox pytorch pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg\n</code></pre> <p>Then you'll need the following command to log inside the container.</p> <pre><code>    singularity shell --writable -H $HOME:/home pytorch\n</code></pre> <p>Once you get into the container, you can use pip and install anything you need (Or with <code>apt-get</code> if you built the container with sudo).</p> <p>You should install your stuff in /usr/local instead.</p>"},{"location":"singularity/#creating-useful-directory","title":"Creating useful directory","text":"<p>One of the benefit of containers is that you'll be able to use them across different clusters. However for each cluster the dataset and experiment folder location can be different. In order to be invariant to those locations, we will create some useful mount points inside the container:</p> <pre><code>    mkdir /dataset\n    mkdir /tmp_log\n    mkdir /final_log\n</code></pre> <p>From now, you won't need to worry anymore when you write your code to specify where to pick up your dataset. Your dataset will always be in <code>/dataset</code> independently of the cluster you are using.</p>"},{"location":"singularity/#testing","title":"Testing","text":"<p>If you have some code that you want to test before finalizing your container, you have two choices. You can either log into your container and run python code inside it with</p> <pre><code>    singularity shell --nv pytorch\n</code></pre> <p>or you can execute your command directly with</p> <pre><code>    singularity exec --nv pytorch python YOUR_CODE.py\n</code></pre>"},{"location":"singularity/#creating-a-new-image-from-the-sandbox","title":"Creating a new image from the sandbox","text":"<p>Once everything you need is installed inside the container, you need to convert it back to a read-only singularity image with:</p> <pre><code>    singularity build pytorch_final.simg pytorch\n</code></pre>"},{"location":"singularity/#second-way-use-recipes","title":"Second way: Use recipes","text":"<p>A singularity recipe is a file including specifics about installation software, environment variables, files to add, and container metadata. It is a starting point for designing any custom container. Instead of pulling a container and install your packages manually, you can specify in this file the packages you want and then build your container from this file.</p> <p>Here is a toy example of a singularity recipe installing some stuff:</p> <p>```bash</p> <pre><code>    ################# Header: Define the base system you want to use ################\n    # Reference of the kind of base you want to use (e.g., docker, debootstrap, shub).\n    Bootstrap: docker\n    # Select the docker image you want to use (Here we choose tensorflow)\n    From: tensorflow/tensorflow:latest-gpu-py3\n\n    ################# Section: Defining the system #################################\n    # Commands in the %post section are executed within the container.\n    %post\n            echo \"Installing Tools with apt-get\"\n            apt-get update\n            apt-get install -y cmake libcupti-dev libyaml-dev wget unzip\n            apt-get clean\n            echo \"Installing things with pip\"\n            pip install tqdm\n            echo \"Creating mount points\"\n            mkdir /dataset\n            mkdir /tmp_log\n            mkdir /final_log\n\n    # Environment variables that should be sourced at runtime.\n    %environment\n            # use bash as default shell\n            SHELL=/bin/bash\n            export SHELL\n</code></pre> <p>A recipe file contains two parts: the <code>header</code> and <code>sections</code>. In the <code>header</code> you specify which base system you want to use, it can be any docker or singularity container. In <code>sections</code>, you can list the things you want to install in the subsection <code>post</code> or list the environment's variable you need to source at each runtime in the subsection <code>environment</code>. For a more detailed description, please look at the <code>singularity documentation</code>_.</p> <p>.. _singularity documentation: https://www.sylabs.io/guides/2.6/user-guide/container_recipes.html#container-recipes</p> <p>In order to build a singularity container from a singularity recipe file, you should use:</p> <pre><code>    sudo singularity build &lt;NAME_CONTAINER&gt; &lt;YOUR_RECIPE_FILES&gt;\n</code></pre>"},{"location":"singularity/#build-recipe-on-singularity-hub","title":"Build recipe on singularity hub","text":"<p>Singularity hub allows users to build containers from recipes directly on singularity-hub's cloud meaning that you don't need anymore to build containers by yourself. You need to register on <code>singularity-hub</code>_ and link your singularity-hub account to your github account, then</p> <p>.. _singularity-hub: https://www.singularity-hub.org/</p> <pre><code>    1) Create a new github repository.\n    2) Add a collection on `singularity-hub`_ and select the github repository your created.\n    3) Clone the github repository on your computer.\n    4) Write the singularity recipe and save it as a file nammed **Singularity**.\n    5) Git add **Singularity**, commit and push on the master branch.\n</code></pre> <p>At this point, robots from singularity-hub will build the container for you, you will be able to download your container from the website or directly with:</p> <pre><code>    singularity pull shub://&lt;github_username&gt;/&lt;repository_name&gt;\n</code></pre>"},{"location":"singularity/#example-recipe-with-openai-gym-mujoco-and-miniworld","title":"Example: Recipe with openai gym, mujoco and miniworld","text":"<p>Here is an example on how you can use singularity recipe to install complex environment as opanai gym, mujoco and miniworld on a pytorch based container. In order to use mujoco, you'll need to copy the key stored on the mila cluster in <code>/ai/apps/mujoco/license/mjkey.txt</code> to your current directory.</p> <p>```bash</p> <pre><code>    #This is a dockerfile that sets up a full Gym install with test dependencies\n    Bootstrap: docker\n\n    # Here we ll build our container upon the pytorch container\n    From: pytorch/pytorch:1.0-cuda10.0-cudnn7-runtime\n\n    # Now we'll copy the mjkey file located in the current directory inside the container's root\n    # directory\n    %files\n            mjkey.txt\n\n    # Then we put everything we need to install\n    %post\n            export PATH=$PATH:/opt/conda/bin\n            apt -y update &amp;&amp; \\\n            apt install -y keyboard-configuration &amp;&amp; \\\n            apt install -y \\\n            python3-dev \\\n            python-pyglet \\\n            python3-opengl \\\n            libhdf5-dev \\\n            libjpeg-dev \\\n            libboost-all-dev \\\n            libsdl2-dev \\\n            libosmesa6-dev \\\n            patchelf \\\n            ffmpeg \\\n            xvfb \\\n            libhdf5-dev \\\n            openjdk-8-jdk \\\n            wget \\\n            git \\\n            unzip &amp;&amp; \\\n            apt clean &amp;&amp; \\\n            rm -rf /var/lib/apt/lists/*\n            pip install h5py\n\n            # Download Gym and Mujoco\n            mkdir /Gym &amp;&amp; cd /Gym\n            git clone https://github.com/openai/gym.git || true &amp;&amp; \\\n            mkdir /Gym/.mujoco &amp;&amp; cd /Gym/.mujoco\n            wget https://www.roboti.us/download/mjpro150_linux.zip  &amp;&amp; \\\n            unzip mjpro150_linux.zip &amp;&amp; \\\n            wget https://www.roboti.us/download/mujoco200_linux.zip &amp;&amp; \\\n            unzip mujoco200_linux.zip &amp;&amp; \\\n            mv mujoco200_linux mujoco200\n\n            # Export global environment variables\n            export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n            export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n            cp /mjkey.txt /Gym/.mujoco/mjkey.txt\n            # Install python dependencies\n            wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt\n            pip install -r requirements.txt\n            # Install Gym and Mujoco\n            cd /Gym/gym\n            pip install -e '.[all]'\n            # Change permission to use mujoco_py as non sudoer user\n            chmod -R 777 /opt/conda/lib/python3.6/site-packages/mujoco_py/\n            pip install --upgrade minerl\n\n    # Export global environment variables\n    %environment\n            export SHELL=/bin/sh\n            export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n            export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n            export PATH=/Gym/gym/.tox/py3/bin:$PATH\n\n    %runscript\n            exec /bin/sh \"$@\"\n</code></pre> <p>Here is the same recipe but written for TensorFlow.</p> <p>```bash</p> <pre><code>    #This is a dockerfile that sets up a full Gym install with test dependencies\n    Bootstrap: docker\n\n    # Here we ll build our container upon the tensorflow container\n    From: tensorflow/tensorflow:latest-gpu-py3\n\n    # Now we'll copy the mjkey file located in the current directory inside the container's root\n    # directory\n    %files\n            mjkey.txt\n\n    # Then we put everything we need to install\n    %post\n            apt -y update &amp;&amp; \\\n            apt install -y keyboard-configuration &amp;&amp; \\\n            apt install -y \\\n            python3-setuptools \\\n            python3-dev \\\n            python-pyglet \\\n            python3-opengl \\\n            libjpeg-dev \\\n            libboost-all-dev \\\n            libsdl2-dev \\\n            libosmesa6-dev \\\n            patchelf \\\n            ffmpeg \\\n            xvfb \\\n            wget \\\n            git \\\n            unzip &amp;&amp; \\\n            apt clean &amp;&amp; \\\n            rm -rf /var/lib/apt/lists/*\n\n            # Download Gym and Mujoco\n            mkdir /Gym &amp;&amp; cd /Gym\n            git clone https://github.com/openai/gym.git || true &amp;&amp; \\\n            mkdir /Gym/.mujoco &amp;&amp; cd /Gym/.mujoco\n            wget https://www.roboti.us/download/mjpro150_linux.zip  &amp;&amp; \\\n            unzip mjpro150_linux.zip &amp;&amp; \\\n            wget https://www.roboti.us/download/mujoco200_linux.zip &amp;&amp; \\\n            unzip mujoco200_linux.zip &amp;&amp; \\\n            mv mujoco200_linux mujoco200\n\n            # Export global environment variables\n            export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n            export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n            cp /mjkey.txt /Gym/.mujoco/mjkey.txt\n\n            # Install python dependencies\n            wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt\n            pip install -r requirements.txt\n            # Install Gym and Mujoco\n            cd /Gym/gym\n            pip install -e '.[all]'\n            # Change permission to use mujoco_py as non sudoer user\n            chmod -R 777 /usr/local/lib/python3.5/dist-packages/mujoco_py/\n\n            # Then install miniworld\n            cd /usr/local/\n            git clone https://github.com/maximecb/gym-miniworld.git\n            cd gym-miniworld\n            pip install -e .\n\n    # Export global environment variables\n    %environment\n            export SHELL=/bin/bash\n            export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt\n            export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin\n            export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin\n            export PATH=/Gym/gym/.tox/py3/bin:$PATH\n\n    %runscript\n            exec /bin/bash \"$@\"\n</code></pre> <p>Keep in mind that those environment variables are sourced at runtime and not at build time. This is why, you should also define them in the <code>%post</code> section since they are required to install mujuco.</p>"},{"location":"singularity/#using-containers-on-clusters","title":"Using containers on clusters","text":"<p>On every cluster with SLURM, dataset and intermediate results should go in <code>$SLURM_TMPDIR</code> while the final experiments results should go in <code>$SCRATCH</code>. In order to use the container you built, you need to copy it on the cluster you want to use.</p> <p>Then reserve a node with srun/sbatch, copy the container and your dataset on the node given by slurm (i.e in <code>$SLURM_TMPDIR</code>) and execute the code <code>&lt;YOUR_CODE&gt;</code> within the container <code>&lt;YOUR_CONTAINER&gt;</code> with:</p> <pre><code>    singularity exec --nv -H $HOME:/home -B $SLURM_TMPDIR:/dataset/ -B $SLURM_TMPDIR:/tmp_log/ -B $SCRATCH:/final_log/ $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; python &lt;YOUR_CODE&gt;\n</code></pre> <p>Remember that <code>/dataset</code>, <code>/tmp_log</code> and <code>/final_log</code> were created in the previous section. Now each time, we'll use singularity, we are explicitly telling it to mount <code>$SLURM_TMPDIR</code> on the cluster's node in the folder <code>/dataset</code> inside the container with the option <code>-B</code> such that each dataset downloaded by pytorch in <code>/dataset</code> will be available in <code>$SLURM_TMPDIR</code>.</p> <p>This will allow us to have code and scripts that are invariant to the cluster environment. The option <code>-H</code> specify what will be the container's home. For example, if you have your code in <code>$HOME/Project12345/Version35/</code> you can specify <code>-H $HOME/Project12345/Version35:/home</code>, thus the container will only have access to the code inside <code>Version35</code>.</p> <p>If you want to run multiple commands inside the container you can use:</p> <pre><code>    singularity exec --nv -H $HOME:/home -B $SLURM_TMPDIR:/dataset/ -B $SLURM_TMPDIR:/tmp_log/ -B $SCRATCH:/final_log/ $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; bash -c 'pwd &amp;&amp; ls &amp;&amp; python &lt;YOUR_CODE&gt;'\n</code></pre>"},{"location":"singularity/#example-interactive-case-srunsalloc","title":"Example: Interactive case (srun/salloc)","text":"<p>Once you get an interactive session with slurm, copy <code>&lt;YOUR_CONTAINER&gt;</code> and <code>&lt;YOUR_DATASET&gt;</code> to <code>$SLURM_TMPDIR</code></p> <pre><code>    # 0. Get an interactive session\n    $ srun --gres=gpu:1\n    # 1. Copy your container on the compute node\n    $ rsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n    # 2. Copy your dataset on the compute node\n    $ rsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n</code></pre> <p>then use <code>singularity shell</code> to get a shell inside the container</p> <pre><code>    # 3. Get a shell in your environment\n    $ singularity shell --nv \\\n            -H $HOME:/home \\\n            -B $SLURM_TMPDIR:/dataset/ \\\n            -B $SLURM_TMPDIR:/tmp_log/ \\\n            -B $SCRATCH:/final_log/ \\\n            $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;\n\n    # 4. Execute your code\n    &lt;Singularity_container&gt;$ python &lt;YOUR_CODE&gt;\n</code></pre> <p>or use <code>singularity exec</code> to execute <code>&lt;YOUR_CODE&gt;</code>.</p> <pre><code>    # 3. Execute your code\n    $ singularity exec --nv \\\n            -H $HOME:/home \\\n            -B $SLURM_TMPDIR:/dataset/ \\\n            -B $SLURM_TMPDIR:/tmp_log/ \\\n            -B $SCRATCH:/final_log/ \\\n            $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n            python &lt;YOUR_CODE&gt;\n</code></pre> <p>You can create also the following alias to make your life easier.</p> <pre><code>    alias my_env='singularity exec --nv \\\n            -H $HOME:/home \\\n            -B $SLURM_TMPDIR:/dataset/ \\\n            -B $SLURM_TMPDIR:/tmp_log/ \\\n            -B $SCRATCH:/final_log/ \\\n            $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;'\n</code></pre> <p>This will allow you to run any code with:</p> <pre><code>    my_env python &lt;YOUR_CODE&gt;\n</code></pre>"},{"location":"singularity/#example-sbatch-case","title":"Example: sbatch case","text":"<p>You can also create a <code>sbatch</code> script:</p> <p>```bash         :linenos:</p> <pre><code>    #!/bin/bash\n    #SBATCH --cpus-per-task=6         # Ask for 6 CPUs\n    #SBATCH --gres=gpu:1              # Ask for 1 GPU\n    #SBATCH --mem=10G                 # Ask for 10 GB of RAM\n    #SBATCH --time=0:10:00            # The job will run for 10 minutes\n\n    # 1. Copy your container on the compute node\n    rsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n    # 2. Copy your dataset on the compute node\n    rsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n    # 3. Executing your code with singularity\n    singularity exec --nv \\\n            -H $HOME:/home \\\n            -B $SLURM_TMPDIR:/dataset/ \\\n            -B $SLURM_TMPDIR:/tmp_log/ \\\n            -B $SCRATCH:/final_log/ \\\n            $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n            python \"&lt;YOUR_CODE&gt;\"\n    # 4. Copy whatever you want to save on $SCRATCH\n    rsync -avz $SLURM_TMPDIR/&lt;to_save&gt; $SCRATCH\n</code></pre>"},{"location":"singularity/#issue-with-pybullet-and-opengl-libraries","title":"Issue with PyBullet and OpenGL libraries","text":"<p>If you are running certain gym environments that require <code>pyglet</code>, you may encounter a problem when running your singularity instance with the Nvidia drivers using the <code>--nv</code> flag. This happens because the <code>--nv</code> flag also provides the OpenGL libraries:</p> <p>```bash</p> <pre><code>    libGL.so.1 =&gt; /.singularity.d/libs/libGL.so.1\n    libGLX.so.0 =&gt; /.singularity.d/libs/libGLX.so.0\n</code></pre> <p>If you don't experience those problems with <code>pyglet</code>, you probably don't need to address this. Otherwise, you can resolve those problems by <code>apt-get install -y libosmesa6-dev mesa-utils mesa-utils-extra libgl1-mesa-glx</code>, and then making sure that your <code>LD_LIBRARY_PATH</code> points to those libraries before the ones in <code>/.singularity.d/libs</code>.</p> <p>```bash</p> <pre><code>    %environment\n            # ...\n            export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/mesa:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"singularity/#mila-cluster","title":"Mila cluster","text":"<p>On the Mila cluster <code>$SCRATCH</code> is not yet defined, you should add the experiment results you want to keep in <code>/network/scratch/&lt;u&gt;/&lt;username&gt;/</code>. In order to use the sbatch script above and to match other cluster environment's names, you can define <code>$SCRATCH</code> as an alias for <code>/network/scratch/&lt;u&gt;/&lt;username&gt;</code> with:</p> <pre><code>    echo \"export SCRATCH=/network/scratch/${USER:0:1}/$USER\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Then, you can follow the general procedure explained above.</p>"},{"location":"singularity/#digital-research-alliance-of-canada","title":"Digital Research Alliance of Canada","text":"<p>Using singularity on Digital Research Alliance of Canada is similar except that you need to add Yoshua's account name and load singularity.  Here is an example of a <code>sbatch</code> script using singularity on compute Canada cluster:</p> <p>```bash         :linenos:</p> <pre><code>    #!/bin/bash\n    #SBATCH --account=rpp-bengioy     # Yoshua pays for your job\n    #SBATCH --cpus-per-task=6         # Ask for 6 CPUs\n    #SBATCH --gres=gpu:1              # Ask for 1 GPU\n    #SBATCH --mem=32G                 # Ask for 32 GB of RAM\n    #SBATCH --time=0:10:00            # The job will run for 10 minutes\n    #SBATCH --output=\"/scratch/&lt;user&gt;/slurm-%j.out\" # Modify the output of sbatch\n\n    # 1. You have to load singularity\n    module load singularity\n    # 2. Then you copy the container to the local disk\n    rsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR\n    # 3. Copy your dataset on the compute node\n    rsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR\n    # 4. Executing your code with singularity\n    singularity exec --nv \\\n            -H $HOME:/home \\\n            -B $SLURM_TMPDIR:/dataset/ \\\n            -B $SLURM_TMPDIR:/tmp_log/ \\\n            -B $SCRATCH:/final_log/ \\\n            $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; \\\n            python \"&lt;YOUR_CODE&gt;\"\n    # 5. Copy whatever you want to save on $SCRATCH\n    rsync -avz $SLURM_TMPDIR/&lt;to_save&gt; $SCRATCH\n</code></pre>"},{"location":"singularity/nvidia/","title":"Nvidia","text":""},{"location":"singularity/nvidia/#containers-from-nvidia-cloud","title":"Containers from Nvidia cloud","text":"<p>One advantage of pulling containers from nvidia cloud is that nvidia software such as Apex or Dali are already installed on them. In order to get those containers, you need to register on <code>ngc</code>_. After you log in, go on configuration and Generate an API Key.</p> <p>.. image:: ngc-0.png</p> <p>Once you get your API key, you need to add them into your bashrc:</p> <pre><code> echo \"export SINGULARITY_DOCKER_USERNAME='$oauthtoken'\" &gt;&gt; ~/.bashrc\n echo \"export SINGULARITY_DOCKER_PASSWORD=YOUR_API_KEY_HERE\" &gt;&gt; ~/.bashrc\n</code></pre> <p>Then you can pull the container you want from the <code>nvidia page</code>_. For example to get pytorch:</p> <pre><code> singularity pull docker://nvcr.io/nvidia/pytorch:19.02-py3\n</code></pre> <p>or if you want tensorflow:</p> <pre><code> singularity pull docker://nvcr.io/nvidia/tensorflow:19.02-py3\n</code></pre> <p>WARNING</p>"}]}